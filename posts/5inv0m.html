<!DOCTYPE html>
<html lang="en">
<head>
	<link rel="stylesheet" type="text/less" href="/css/post.less">
	
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
	<link rel="shortcut icon" type="image/png" href="/img/cat.jpg"/>
	<script src="https://code.jquery.com/jquery-2.2.4.min.js" integrity="sha256-BbhdlvQf/xTY9gja0Dq3HiwQF8LaCRTXxZKRutelT44=" crossorigin="anonymous"></script>
	<script src="https://cdnjs.cloudflare.com/ajax/libs/less.js/2.5.3/less.min.js"></script>
	<link href="https://fonts.googleapis.com/css?family=Roboto:400,700" rel="stylesheet">
	<link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" rel="stylesheet">

	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
		(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
		m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

		ga('create', 'UA-58440568-4', 'auto');
		ga('send', 'pageview');
	</script>

	<!-- Cookie Consent plugin by Silktide - http://silktide.com/cookieconsent -->
	<script type="text/javascript">
    window.cookieconsent_options = {"message":"This website uses cookies to ensure you get the best experience on our website","dismiss":"Got it!","learnMore":"More info","link":null,"theme":"dark-bottom"};
	</script>
	<script type="text/javascript" src="//cdnjs.cloudflare.com/ajax/libs/cookieconsent2/1.0.10/cookieconsent.min.js"></script>
	<title>If entropy is a measure for the amount of disorder in a system, why is it at its maximum when equillibrum is achieved?</title>
</head>
<body>
	<div id="header">
	<a href="/about" title="About">About</a>
</div>
	<div id="content">
		<div class="home">
			<a href="/">Back to Home</a>
		</div>

		<ul class="posts">
<li class="post" data-handle="5inv0m">
	<div class="overview">
		<a class="source" href="https://www.reddit.com/r/askscience/comments/5inv0m/if_entropy_is_a_measure_for_the_amount_of/" target="_blank" title="Reddit thread where this comes from"><i class="fa fa-external-link" aria-hidden="true"></i></a>
		<h2>
			<span class="tags tag-Physics">Physics</span>
			<a href="/posts/5inv0m" onclick="return false">If entropy is a measure for the amount of disorder in a system, why is it at its maximum when equillibrum is achieved?</a>
		</h2>
		<!--<span class="date">2016-12-19</span>-->
		<span class="is-new">NEW</span>
	</div>

		<div class="question"><span class="qa" title="Question">Q:</span><div class="markdown"><p>If entropy is a measure for the amount of disorder in a system, why is it at its maximum when equillibrum is achieved?</p></div></div>

	<div class="comment-section">
		<div class="answers-placeholder">
			<div class="answers">
	<div class="answer" data-handle="db9lqai">
		<a class="author" href="https://www.reddit.com/user/rantonels" target="_blank">rantonels</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>Entropy is not a measure of disorder but of your lack of information about the state of the system. When a system is in thermal equilibrium, the number of possible actual microstates the system can find itself in is maximum, therefore your knowledge about it is minimum, and entropy is maximum.</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<a class="less-answers upper" href="javascript:void(0)">less answers...</a>
	<div class="answer" data-handle="db9s7hz">
		<a class="author" href="https://www.reddit.com/user/ericGraves" target="_blank">ericGraves</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><blockquote>
<p>We first introduce the concept of <em>entropy</em>, which is a measure of the uncertainty of a random variable. [Thomas Cover and Joy Thomas, <strong>Elements of Information Theory</strong>, 2^nd ed., page 13]</p>
</blockquote>
<p>Anyway, quick answer is that the evolution of a thermodynamic system results in a uniform distribution. And a uniform distribution is the maximum entropy of a discrete system, a <a href="https://www.reddit.com/r/askscience/comments/5i054n/how_are_the_two_main_definitions_of_entropy/db4br89/" target="_blank">proof of which</a> was offered recently by /u/RobusEtCeleritas.</p></div>		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="db9m6xy">
		<a class="author" href="https://www.reddit.com/user/achtungpolizei" target="_blank">achtungpolizei</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>I always like to think of entropy as a measure of how much information is in a given system.
The higher the entropy, the less information you can deduct from the systems state.</p>
<p>Consider a glass of water with warm areas and cold areas (maybe because you poured milk into your coffee).
Now at the beginning since there are patches of cold and hot regions, the amount of information is rather high because you can look at different locations inside the mug and find different states because the milk and the coffee did not form a homogenous distribution inside the mug yet.</p>
<p>Now if you were to wait for some time, the milk and the coffee will order itself in a energetically favorable manner which in this case means that the temperature across the mug is uniform and the liquid is a perfect mix of coffee and milk at any point in the mug.</p>
<p>How much information were you able to deduct from such a mass?
Really not much because if you know the composition of your liquid in one part of your system you basically know how it looks like in the whole mug. This is then called thermal equilibrium because the temperature and consistency of the mass is the same everywhere inside of your mug.</p>
<p>edit: Here you would have to consider there is no interaction of your mug-system with the environment (because in reality you will always have some temperature differences around your mug which would cause your mug to never be in thermal equilibrium)</p>
<p>So: thermal equilibrium means low amount of information means high entropy!</p>
<p>This explanation lacks some technicality I'm sure. This is just my engineers look on the subject of entropy and I hope it helped to somehow visualise the concept!
I think a lot of the times the misunderstandings come from how physicists defined entropy.</p>
<p>When you look at my analogy as a lay-man you would think:&quot;Wait, having one warm patch of coffee in one corner and one cold patch of milk in the other corner of the mug would mean it looks unarranged and disorderly!&quot;</p>
<p>Maybe it's because as a child you interpreted disorder as something being very chaotic and unhomogenous.</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="db9q9pj">
		<a class="author" href="https://www.reddit.com/user/mlmayo" target="_blank">mlmayo</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>So there are many types of &quot;entropy&quot;. They either come from concepts from physics (Boltzmann's entropy) or from Information Theory (e.g., Shannon entropy, Kullbeck entropy).  I think you want to know about Boltzmann's entropy and why, at thermodynamic equilibrium, the entropy is highest there.</p>
<p>I'll try to be brief.  Entropy is <em>not</em> a measure of &quot;disorder&quot;, despite that such examples help give users an intuition about it.  What it <em>is</em>, is the number of possible (micro)states, N, that a thermodynamic system can occupy; that is, how many different values of position and momentum can all the particles in a gas have?  You can imagine that this number is large for many situations, which can be estimated from Boltzmann's model: N=exp(S/K), with K=a constant (see <a href="https://en.wikipedia.org/wiki/Boltzmann's_entropy_formula" target="_blank">here</a> for more explanation).</p>
<p>Now, there are many details which we won't discuss, but you should start to think abstractly about thermodynamic systems in terms of their <em>states</em>, i.e., the set of all possible values of position and momentum.  If you only imagine these states, then the set of all states is sometimes called a <a href="https://en.wikipedia.org/wiki/State_space" target="_blank">state space</a>.  In terms of this state space, a system evolves by &quot;hopping&quot; between these states.</p>
<p>For a system in thermodynamic equilibrium (one that is in chemical, thermal, and mechanical equilibrium), then the state space associated with the equilibrium state is, by far, the largest &quot;volume&quot; of the state space.  If you &quot;move&quot; around the state space, then you will most probably find yourself in the region of it that represents equilibrium simply because its the biggest fraction of the state-space.</p></div>		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="dbarhh4">
		<a class="author" href="https://www.reddit.com/user/NeuroBill" target="_blank">NeuroBill</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>I just wanted to chime in, because there is lots of discussion about entropy, and a lot of it has gotten kinda high level and possibly over some peoples head.</p>
<p>In this whole discussion you can find people talking about entropy, information, and probability. But how are they all related?</p>
<p>Well lets say I had a coin, and I flipped it, and you wanted to know the outcome. If the coin was fair, and I told you that it came up heads, that I was transferring information to you. However, if the coin wasn't fair, and if you knew the coin always came up heads, then me telling you the coin came up heads provides you with no information. Indeed, the amount of information I pass to you is maximized when the probability of the coin coming up heads is 0.5 with each answer providing 1 bit of information. This may seem and odd example, so let's give a slightly more real world example. What is the letter missing from this sentence?:</p>
<pre><code>I l*ve cats</code></pre>
<p>It's an o, and nearly every native English speaker could figure that out. Why? Because probabilistically, with that combination of letters, nearly every time, that missing character would be an o. i.e it is like an unfair coin. Hence me telling you the outcome (that the character is an o) doesn't provide you with any information.</p>
<p>So what does that have to do with thermodynaic entropy? Honestly, I'm not 100% sure, but bear with me. I DO know that the equations have very similar forms.</p>
<p>For information (i.e. Shannon Entropy) the equation is:</p>
<pre><code>H = -Σ P(x) * Log2(P(x))</code></pre>
<p>Where H is Shannon Entropy, P(x) is the probability of outcome X, where you sum over all the possible outcomes.</p>
<p>Gibbs Entropy, on the other hand is:</p>
<pre><code>S = -kb * Σ P(i) * ln(P(i))</code></pre>
<p>Where kb is the Boltzman constant, and P(i) is the probability of a give microstate i, where you sum over all the possible microstates.</p>
<p>Just like how information is maximized when the coin is fair, entropy is maximized when all possible microstates are equally likely.  Just like how you have the least idea about the outcome of a coin when the coin is fair, you have the least idea about the properties of a particle when entropy is maximized.</p></div>		<div class="replies-placeholder"></div>
	</div>
</div>		</div>
		<div class="more-less">
			<a class="collapse" href="javascript:void(0)">collapse</a>
			<a class="more-answers" href="javascript:void(0)">4 more answers...</a>
			<a class="less-answers lower" href="javascript:void(0)">less answers...</a>
			&nbsp;
		</div>
	</div>
	<a class="show" href="/posts/5inv0m" onclick="return false"><span>show</span></a>
</li>
		</ul>
	</div>

	<script>
		var config = {"stream":{"initial":10,"catchup":5},"api":{"url":"api.veryinteresting.io"}};
	</script>
	<script src="/js/project.js"></script>
	<script src="/js/post.js"></script>
</body>
</html>