<div class="answers">
	<div class="answer" data-handle="e5pyjn7">
		<a class="author" href="https://www.reddit.com/user/CptCap" target="_blank">CptCap</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>When a real camera capture a frame, it capture 1/48th of a second worth of information. Moving objects will appear blurry, so a single frame will still give some sense of movement.</p>
<p>Video games (for examples) don't do that. They simulate the world frame per frame. The world doesn't exists in between frames, there is no real movement: objects are basically teleported during the simulation.</p>
<p>This means that when the game renders a frame it can only display the world as it is at this exact point in time, all information about movement is lost, it thus need to display at a higher framerate to appear as smooth as a real camera.</p>
<p>Motion blur tries to circumvent this (by computing the difference between the current and last state). While it helps a bit, it is an approximation (like anything else in real-time CG) and isn't as good as true, physically correct, motion blur.</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<a class="less-answers upper" href="javascript:void(0)">less answers...</a>
	<div class="answer" data-handle="e5q79dz">
		<a class="author" href="https://www.reddit.com/user/EMBNumbers" target="_blank">EMBNumbers</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>The REAL answer has nothing to do with motion blur. It is that human beings are very sensitive to latency: the delay between an action and the result of the action.</p>
<p>If you put a mouse cursor in a 24 frames/second movie and moved the mouse, you would suddenly find the low refresh rate almost unusable. Movies at 24 fps and TV at 30fps work because there is no feedback that could be used to detect latency. </p>
<p>Stated another way: Humans are terrible at absolute time sense but awesome at relative time sense. Without any time basis to compare, 24fps is adequate to make the illusion of smooth motion. As soon at there is a time basis for comparison like pressing button or moving a mouse, the illusion is spoiled at 24fps. In human trials, most humans do not notice latencies less than 1/60th of a second but do notice 1/50th of a second. This is why 60fps is considered the optimal/minimal frame rate.</p>
<p>Further note: If you game using an LCD monitor/TV and brag that your game achieves hundreds of FPS on high graphics settings, all you are doing is bragging about wasting electricity. Most (almost all) LCDs are configured in hardware so that pixels physically cannot change color more than 60 times per second. If your game is producing 120fps, half of all the frames are being discarded. You never see them.</p>
<p>If your game is running at 45 fps, it is an averaged value meaning half the frames took 1/30th second and the other half took 1/60th of a second. Variable frame rates are often considered worse that slow frame rates.</p>
<p>Due to the fact that your display updates 60 times per second, if your game doesn't finish a frame within 1/60th of a second, you must wait until the next 1/60th boundary to see the new frame. This means your true fps drops from 60 to 30. There is no frame rate in between unless the frame rates are averaged. If you don't get 30fps, you drop to 20fps, then 15, then 12, the 10, then 6, then 5, 4, 3, 2, 1.</p>
<p>From the USA Federal Aviation Administration:
<em>&quot;b. SAE ARPs provide recommended lag times for display of the format and primary flight data, and minimum rates for data updates, to meet symbol motion. A 50-60 Hz refresh rate is typically enough to remove traces of visible flicker on the display. Frequencies above 55 Hz for stroke symbology or non-interlaced raster and 30/60 Hz for interlace raster are generally satisfactory.&quot;</em> - <a href="https://www.faa.gov/documentLibrary/media/Advisory_Circular/AC%2023.1311-1C.pdf" target="_blank">https://www.faa.gov/documentLibrary/media/Advisory_Circular/AC%2023.1311-1C.pdf</a></p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="e5qcnku">
		<a class="author" href="https://www.reddit.com/user/ishotthedeputy" target="_blank">ishotthedeputy</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>While the motion blur/relative motion ideas make sense, i think they are minor reasons. </p>
<p>I think we are ok with it for the same reason we are ok with watching Black and White movies or with watching cartoons- your mind accepts these visual &quot;discrepancies&quot; as if they belong to the story world. </p>
<p>I think this is the main reason you haven't seen the filmmaking industry upgrade to higher frame rates as the gaming industry did. The look of 24fps has become so associated with what people think of as a &quot;movie&quot;.</p></div>		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="e5qct2a">
		<a class="author" href="https://www.reddit.com/user/TheManlyBanana" target="_blank">TheManlyBanana</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>It's also worth noting that film projectors can easily vary their frame rate to whatever is being played. 24, 30, 48 and 60fps where each is required. AFAIK, TVs and Monitors are either NTSC or PAL dependent on location, and mine at least don't change between 25hz multiples and 30hz</p></div>		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="e5qdu4m">
		<a class="author" href="https://www.reddit.com/user/spidy88" target="_blank">spidy88</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>I actually do not like watching movies in the cinemas these days because I notice the low framerate a lot and the stuttering is ruining the experience for me. 3D makes it even worse.</p>
<p>I really LOVED the Hobbit movies that were shown in 48 fps, and I do not understand why they do not increase the framerate even beyond that.</p>
<p>&#x200B;</p>
<p>I watched VGHS (Video Game High School) in 4K 60fps and this is the best movie/series experience I ever had. If you haven't watched it yet go and see for yourself just how smooth of a movie experience that is!</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
</div>