<div class="answers">
	<div class="answer" data-handle="dfgepx7">
		<a class="author" href="https://www.reddit.com/user/evensevenone" target="_blank">evensevenone</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>To give an example of just one small part of ATLAS (which is only one of several detectors): The Pixel Detector consists of 1744 modules arranged in cylindrical layers around one of the collision points. Each of those modules contains 47,000 pixels (so 80 million total). The beams are manipulated so that they collide with each other 40 million times a second, so all 80 million pixels must be read 40 million times a second. </p>
<p>Most of these readings are actually analyzed and thrown away in hardware, then there is a large computing cluster that does more analysis and discards even more data, but even after all that they store upwards of 100 megabytes per second while the beam is active.</p>
<p>This is all needed because we are looking for very rare particles, and we want to observe them many times in order for our measurements to gain sufficient statistical certainty. Having lots of pixels in the detector means their track can be observed more closely, which gives information as to their mass and charge.</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<a class="less-answers upper" href="javascript:void(0)">less answers...</a>
	<div class="answer" data-handle="dfg9yig">
		<a class="author" href="https://www.reddit.com/user/SantasDead" target="_blank">SantasDead</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>I'm not sure about the Hadron, but I used to work in X-ray crystallography which is kind of similar in terms of data collection. In our systems we would mount a single crystal which held an unknown protein. The sample would be mounted on something that rotated 360 degrees while an x-ray beam was shot at it and a detector collected the scatter of xrays that were emitted from the crystal. Each image was around 16Mb. Imagine the amount of data collected when the collection is set up to take one image every half of a degree for 360 degrees. They do this every few days for an entire year...that is a shit ton of data that takes massive clustered servers to process.</p>
<p>I'd imagine that since the Hadron has mere nanoseconds to collect the data they have a massive number of very high resolution detectors &quot;watching&quot; the reaction.</p></div>		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="dfg9co1">
		<a class="author" href="https://www.reddit.com/user/hawkdron496" target="_blank">hawkdron496</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>The LHC uses a lot of data because there is a lot of data to process. While the beam is active, billions of collisions per second are happening in the tunnel. Each of those collisions needs to be studied and processed. Each collision creates a myriad of particles that interact in other ways that need to be tracked and processed. They could use less data, but that would lead to less precise measurements. </p></div>		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="dfgkp1w">
		<a class="author" href="https://www.reddit.com/user/Sir_Aleister" target="_blank">Sir_Aleister</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>This is a piece of the ATLAS detector : <a href="http://www.atlasexperiment.org/photos/atlas_photos/selected-photos/full-detector/0511013_01-A4-at-144-dpi.jpg" target="_blank">http://www.atlasexperiment.org/photos/atlas_photos/selected-photos/full-detector/0511013_01-A4-at-144-dpi.jpg</a>
This is full of sensors  to track wehre the particle goes. lots of sensor --&gt; lots of data.
In fact most of the data are trown away in real time (you don't care about part which haven't been touched by particles for example). But particule physics is about finding rare phenomenon in a big set of phenomena we understand so a big part of the work is about producing a lot of &quot;events&quot; (beam side) being able to detect them (detector side) and to process them (software side).<br />
If you wonder why there are so many authors in these LHC experiment it's because you need all these people working together to be able to have enough good data to do some science with it. </p></div>		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="dfgf12m">
		<a class="author" href="https://www.reddit.com/user/DemigodHope" target="_blank">DemigodHope</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>If you are looking for a special process with a low crosssection (which basically means it's just rare to happen), you want to have a high amount of processes to start with (high luminosity). So you'll end up with lots of data.<br />
This is not only to actually record the special process that you are looking for (eg. Higgs-&gt;2x Photons), but also to record enough to separate it from the underlying signal. No detector is perfect and hence some recorded process, particles, energys etc. will be miss identified. So any process that will produce any number of photons could be miss identified as a Higgs decay as in my example (Photons could 'overlap' (pile up) with each other or other particles resulting in the detector to answer with a signal looking like a high energy photon just like the one you are looking for). There are endless possible process that will contribute as underlying signal to the signal you are looking for, so you need some way to make sure it's the real deal. </p>
<p>The approach is to work with a high confidence level aka 'x sigma' (6 for particle identification). Basically a way to tell you how sure you are that your signal is no random signal due to bad luck. </p></div>		<div class="replies-placeholder"></div>
	</div>
</div>