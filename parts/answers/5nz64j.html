<div class="answers">
	<div class="answer" data-handle="dcfmw6i">
		<a class="author" href="https://www.reddit.com/user/ericGraves" target="_blank">ericGraves</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>They can both be defined as functions of a probability distribution, and the definitions differ by a constant multiplier. For the information theoretic definition, this multiplier is set to arbitrarily make the entropy of a coin flip 1.</p>
<p>Of interest, there is discussion in the academic community over whether they should be interpreted as equivalent notions or not. The major prevailing opinion though is that there is enough there to justify overlapping study. </p>
<p>The major difference between the two is the types of problems they apply to. For instance, it is common for the processes used in information theory to not satisfy the second law, that is entropy does not always increase. While in statistical mechanics, this law is supreme. </p>
<p>The best discussion is probably <a href="http://bayes.wustl.edu/etj/articles/theory.1.pdf" target="_blank"><em>Information theory and statistical mechanics</em> by E.T. Jaynes (PDF)</a>. While if you want a more recent and thorough discussion, there is the book <a href="http://webee.technion.ac.il/people/merhav/papers/gcnotes.pdf" target="_blank"><em>Statistical physics and information theory</em> by Navi Mehrav (PDF)</a>. </p>
<p>edit- Changed <code>Staistical'' to</code>Statistical.'' Also changed a period to a comma. </p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<a class="less-answers upper" href="javascript:void(0)">less answers...</a>
	<div class="answer" data-handle="dcfyjla">
		<a class="author" href="https://www.reddit.com/user/aspera1631" target="_blank">aspera1631</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p><a href="https://www.amazon.com/Farewell-Entropy-Arieh-Ben-Naim/dp/9812707077" target="_blank">Here</a> is a good book on this subject.</p>
<p>The argument therein is that entropy in both cases quantifies &quot;missing information.&quot; Given a set of prior information, entropy tells us how much more information is necessary to specify the exact state of a system. </p>
<p>In thermodynamics, the prior information is the thermodynamic variables (temperature, pressure, etc), and the entropy tells us how much more information is necessary to specify the microstate.</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="dcg4zwg">
		<a class="author" href="https://www.reddit.com/user/The_Filthy_Spaniard" target="_blank">The_Filthy_Spaniard</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>The most concise way of describing entropy is a measure of the possible arrangements of a system.</p>
<p>In a low entropy state, like a solid, or a short password, there are fewer possible arrangements of atoms (being rigidly confined in a lattice) or letters than in a higher entropy state, like a gas (where molecules can move freely) or a longer password. </p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="dcg9hds">
		<a class="author" href="https://www.reddit.com/user/mstksg" target="_blank">mstksg</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>The basic idea revolves around the concept of the <strong>microstate</strong> and the <strong>macrostate</strong>.  The microstate is the configuration of the components of the system, and the macrostate is a &quot;label&quot; we use to categorize the microstates.</p>
<p>For example, in a game of yahtzee, the individual configuration of each dice might be the microstate, and the macrostate might be &quot;Number of sixes rolled&quot;.</p>
<p>We define the entropy of a macrostate to be proportional to the number of microstates that would be 'counted' as a part of that macrostate.  For example, &quot;five sixes&quot; would be an extremely low-entropy macrostate because there is only one way you could roll five dices to get five sixes.  &quot;four sixes&quot; is also a pretty low-entropy macrostate, but bigger than &quot;fix sixes&quot;.</p>
<p>You can extend this pretty seamlessly to the physical concept of entropy.  The microstate of a cloud of gas might be the position and velocities of every molecule in that cloud.  The temperature of that cloud would be a macrostate.  There are many different configurations of moleculues that will yield 300K.  And, again, that's all the entropy is -- a measure of the logarithm of &quot;how many&quot; molecular configurations would yield your macrostate (300K).  As it turns out, there are <em>more</em> ways to configure the moleclues to get 300K than there are to get, say, 30K.  There are even less ways you can configure the molecules in that cloud of gas to get 3K.</p>
<p>Information theory entropy and thermodynamic entropy are literally the exact same concept -- a number proportional to the possible configurations that would yield the macrostate you're asking about.</p></div>		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="dcg3wzg">
		<a class="author" href="https://www.reddit.com/user/realhamster" target="_blank">realhamster</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>In both cases entropy can be seen as how much information it would take to describe the state of a system. The more chaotic a system is, the more information you need to completely describe its state.</p>
<p>When matter is at 0 kelvin, its atoms aren't 'jiggling' because it lacks heat and therefore you need no information to describe it, as you already know how all its atoms are behaving. You could say its entropy is 0.  In a gas at a high temperature its atoms are jiggling and colliding onto each other, so you would need a lot of information to describe the velocity and energy of each one, so it's entropy would be high.</p>
<p>In information theory, if you have a set of messages you want to transmit, entropy tells you how much information you need to describe them, IE.: How much you can compress them. If there is a lot of repeated patterns between them you can make use of this and compress the messages. The more complex the messages are, the more bits (information) you need to describe them, so the more entropy they have.</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
</div>