<div class="answers">
	<div class="answer" data-handle="e15mudc">
		<a class="author" href="https://www.reddit.com/user/vulcan_scientist" target="_blank">vulcan_scientist</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>I am of course biased and some computer science is likely going to creep into this answer. But humor me anyway. :) </p>
<p>At least in a quantum mechanical universe, information cannot be created or destroyed. This is sometimes referred to as the &quot;time reversibility of Schrodinger's equations&quot;.
Mathematically, this is a simple consequence of the fact that in quantum mechanics, the time evolution of a system is &quot;unitary&quot;. If you bear with me, I explain this with some lightweight math below. </p>
<p>Regardless, philosophically, your question is somewhat intriguing. Quantum mechanics (which makes astoundingly accurate predictions in experiments) somehow suggests that there's this underlying big machine that is somehow storing the state of the universe and continuously computing the next states from the previous one by applying the appropriate unitary transformations. </p>
<p>Where is this information? We don't know. One thing, however, appears clear: we don't think this is coded &quot;onto everything&quot; in any useful way. </p>
<p>What do I mean by this? It turns out that if you have a system, with, say n particles, then the state of this system in quantum mechanics is described by an object (vector/density matrix) of size 2^n. This number can get very big very quickly and even if we could write 1 bit of information on every quark in the observable universe we won't be able to describe the state of more than, say, 100 particles. </p>
<p>Yet, of course, time after time, the outcome of our experiments are consistent with this huge amount of information being <em>really</em> used. Used in a way that cannot provably be done using any smaller object/information.  </p>
<p>So that's the mystery. Not so much a mystery for making predictions or computing outcomes. Mystery in a somewhat philosophical sense, though.</p>
<h2>Time Reversibility of Quantum Mechanics</h2>
<p>If you know a little bit of linear algebra, this is really simple: in quantum mechanics, the state of a system at any point in time is described by a complex valued vector*. Any physical process describes how this description of the state changes over time. </p>
<p>Quantum mechanics asserts that this evolution can be described by multiplication by a unitary matrix U(t). That is, if at time 1, the state is described by a vector $v$, then at time 2, the state is described by U(1)v. </p>
<p>Unfolding this time evolution, if you start from a state v and apply a physical process described by U(t), then the state at time n is given by
U(n-1) U(n-2)...U(1) v**. </p>
<p>Now, the key properties of unitary evolution for us is i) products of unitary matrices is also a unitary matrix and ii) unitary matrices are &quot;invertible&quot;.
The first property means that at time $n$, we can describe the state of the system we are studying by Uv where U = U(n-1) U(n-2)...U(1).  </p>
<p>What this second property means is simply this: If you know that Uv = w, then there's no other vector v' such that Uv' = w. In other words, given w and the process U, you can uniquely determine the original state v. </p>
<p>Thus, the information about v is never lost, no matter how long after the initial state you look at the system (so long as you know about the process acting on the system).</p>
<p>Long story short, this explains why in a quantum mechanical universe, there's complete reversibility in the state of the system and thus no loss (or gain) in the information content. </p>
<p>*Strictly speaking this holds only for pure states and in general, states are described by &quot;density matrices&quot; which can be thought of as analogs of probability distributions over pure states.</p>
<p>**I am using &quot;discrete&quot; time evolution for simplicity. In general, this is a somewhat fancier integral. But the larger point is valid. </p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<a class="less-answers upper" href="javascript:void(0)">less answers...</a>
	<div class="answer" data-handle="e15v0zx">
		<a class="author" href="https://www.reddit.com/user/Pondernautics" target="_blank">Pondernautics</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>It's important to keep in mind that there is a difference between quantum information and thermodynamic information in physics. Quantum information, like total angular momentum (denoted by J), is regarded as conserved and cannot be destroyed. </p>
<p>Thermodynamic information CAN be created and destroyed. From the thermodynamic perspective, energy is the property that is conserved for all systems (and this fact emerges from the conservation of quantum information). But systems can have internal information (sometimes called exergy) that is a function of its absolute entropy relating its internal information to its external environment. For example, the DNA of a genome contains information. The quantity of this information can be calculated as a function of absolute entropy, and therefore can be considered to be &quot;thermodynamic information.&quot; This information is not conserved, which is obvious when an organism reaches thermodynamic equilibrium with it's environment (dies). It's like the information of a sand castle on a beach; it is not conserved.</p>
<p>Edit: I implied &quot;spin&quot; was conserved before, which could be misleading. Spin angular momentum (S) is not conserved on it's own. Total angular momentum (J) is conserved. See Noether's theorem.</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="e160zqd">
		<a class="author" href="https://www.reddit.com/user/FroggyWatcher" target="_blank">FroggyWatcher</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>Not sure if I'm still late to the party or not, but my current understanding of information (from a classical physics standpoint) is that it is the &quot;remnants&quot; of an event. An event occurs in a fixed position at a certain instant. Say you're at 300 000 000 km from a star about to evolve into supernova, nothing in between you and the exact spot where it occurs. Since light has been measured and theorized through Electromagnetic theory to have a speed of 300 000 km/s, then the light emitted by the event (evolution into supernova) will only reach you 1000 seconds after it happened (provided that you and the star are in the same reference frame, that is stationary relative to each other.) You may think of this as one way to define information. Information needs a means to spread. When dealing with relativistic systems, these are either carried by light (we're thinking cosmological scales here) or by virtual particles (in the realm of particle physics). Information in that sense is really just evidence of an occurrence with a keyframe of time and space. And you determine where information comes from by having knowledge of the position and momentum of the carrier.</p>
<p>Physics loves generalizability. In that sense, information may be a starting condition, a rate, or just an intrinsic property of matter. Imagine a shrapnel explosion in vacuum, without any gravity at all. If you look at one single piece of shrapnel, it will be travelling at a constant velocity. By knowing how fast it is travelling (a rate) and where it is (a starting condition), you are able to trace it back to where it started. Except, at this point, you still don't know where that is, you need one more piece of shrapnel travelling in a different direction, so that you cross their inertial paths and then you'll know the intersection of the two is the point where the explosion took place, and since you know their time of flight (by knowing position and momentum at a given point in time), you will know where the explosion occurred.</p>
<p>When it comes to entropy, someone already gave a good analogy. To tie it with mine, think of this same explosion happening in an isolated system (no energy or matter may escape it). Then any single time a constituent of the system collides with another, more information is created, because an event occurred. Now you cannot tell where the bomb went off based on two pieces of shrapnel that happened to collide. If they collided, they changed their momentum, and you'll need to obtain information of their momenta before the collision and after the collision in order to trace it back to the original place where the explosion occurred. If you manage to cool down a gas to absolute 0 (K), then you were able to completely stop the system, and you can think of the method you used to cool things down as a way of draining the information from the system, or that the system cannot carry any information at all because there is no motion at all.</p>
<p>I hope (at least parts of) this explanation made sense, but feel free to ask further!</p></div>		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="e165en6">
		<a class="author" href="https://www.reddit.com/user/Untinted" target="_blank">Untinted</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>It depends on what level you are asking the question.</p>
<p>Generalized to all levels, information can be boiled down to a certain entropic state of matter that is recognizable as having a certain meaning.</p>
<p>simple example: A mass 'A' moving at collision speed x relative to another mass 'B' has its velocity as derived information between time states, and if one of those masses is your car, and the other is a pedestrian, that's information becomes quite imperative, but do you have the energy and entropy to move the car within the time left?</p>
<p>Another example: In computers, you have a certain number of zeros and ones, and both are needed in an entropic limited system to give those zeros and ones meaning.  1 and 15 zeros in a row can mean a single number: 32768, or two numbers: 128 and 0 or 4 numbers: 8 and three 0, or even -0,</p>
<p>I'd recommend studying statistics (along with physics) if you want to get a good answer.  All of statistics is based on the premise of how you classify information in the first place, and the difficulty of actually getting a good answer.</p></div>		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="e15xb56">
		<a class="author" href="https://www.reddit.com/user/agorism1337" target="_blank">agorism1337</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>The amount of information needed to completely encode a closed system is always increasing. This information is called entropy.</p>
<p>Some people think that our universe is limited in its possible reactions.
If you start with data encoding a closed physical system, then there is always an algorithm you can use with a turing machine to calculate data encoding the end result.
This algorithm can be executed on n turing machines in time O(t ) with space O (n) where t is the number of plank time units for the reaction to occur, and n is the number of particles in the reaction.</p></div>		<div class="replies-placeholder"></div>
	</div>
</div>