<div class="answers">
	<div class="answer" data-handle="dj2aw0e">
		<a class="author" href="https://www.reddit.com/user/mfukar" target="_blank">mfukar</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>There are techniques for automated processing of crash reports.</p>
<p>Generally, the goal is to match failure report(s) to a (known) problem. [1] [2] [3] [4] Initial approaches revolved around matching the call stacks generated at the time of a crash. [1] [3] Bartz et al. [5] applied a machine learning similarity metric for grouping Windows failure reports. This is done using information from clients when the users describe the  symptoms of failures. The primary mechanism for measurements is an adaptation of the Levenshtein edit distance process, which is deemed to be one of the less costly string matching algorithms.
Lohman et al. [4] technique consisted of normalizing strings based on length before comparing them. They applied metrics commonly used in string matching algorithms, including edit distance, longest common subsequence and prefix match.</p>
<p>Kim et al [6] developed crash graphs to aggregate a set of crash dumps into a graph, which demonstrated to be able to more efficiently identify duplicate bug reports and predict if a given crash will be fixed. Artzi et al [7] developed techniques for creating unit tests for reproducing crash dumps. The approach consists of monitoring phase and test generation phase. The monitoring phase stored copies of the receiver and arguments for each method and the test generation phase restores the method and arguments.</p>
<p>Le &amp; Krutz [8] noted that the same fault can result in different call stacks and developed the technique of grouping crash reports by cross-checking manually and automatically grouped crash reports to derive grouping criteria. Dhaliwal et al [9] on a case study of Firefox observed that grouping crash reports by two or more bugs together increased the time-to-fix for the bugs, and proposed a grouping approach that produced one group per bug.</p>
<p>Automated crash report grouping is nowadays thought as a requirement for every crash reporting solution.</p>
<p>After crash reports are grouped, there are also automated approaches dedicated to forensic analysis [e.g. for <a href="https://blogs.msdn.microsoft.com/wsdevsol/2014/06/26/automating-analysis-of-windows-store-crash-reports/" target="_blank">Windows store apps</a>]. There are multiple patents on <a href="https://www.google.com/patents/US6763517" target="_blank">similar goals</a> (scroll down to &quot;Reference by&quot; section).</p>
<hr />
<p>[1] M. Brodie, S. Ma, L. Rachevsky, and J. Champlin, “Automated problem determination using call-stack matching.” J. Network Syst. Manage., 2005.</p>
<p>[2] N. Modani, R. Gupta, G. Lohman, T. Syeda-Mahmood, and L. Mignet, “Automatically identifying known software problems,” in Data Engineering Workshop, 2007 IEEE 23rd International Conference on, 2007. </p>
<p>[3] M. Brodie, S. Ma, G. M. Lohman, L. Mignet, N. Modani, M. Wilding, J. Champlin, and P. Sohn, “Quickly finding known software problems via automated symptom matching.” in ICAC’05, 2005</p>
<p>[4] G. Lohman, J. Champlin, and P. Sohn, “Quickly finding known software problems via automated symptom matching,” in Proceedings of the Second International Conference on Automatic Computing, 2005.</p>
<p>[5] K. Bartz, J. W. Stokes, J. C. Platt, R. Kivett, D. Grant, S. Calinoiu, and G. Loihle, “Finding similar failures using callstack similarity.”</p>
<p>[6]  S. Kim, T. Zimmermann, and N. Nagappan, “Crash graphs: An aggregated view of multiple crashes to improve crash triage,” in Dependable Systems Networks (DSN), IEEE/IFIP 41st International Conference on, 2011</p>
<p>[7] S. Artzi, S. Kim, and M. D. Ernst, “Recrash: Making software failures reproducible by preserving object states,” in Proceedings of the 22nd European conference on Object-Oriented Programming, ser. ECOOP ’08, 2008</p>
<p>[8] Wei Le, Daniel Krutz, &quot;How to Group Crashes Effectively: Comparing Manually and Automatically Grouped Crash Dumps&quot;, 2012</p>
<p>[9]  Tejinder Dhaliwal, Foutse Khomh, Ying Zou, &quot;Classifying field crash reports for fixing bugs: A case study of Mozilla Firefox&quot;,  Software Maintenance (ICSM), 2011</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<a class="less-answers upper" href="javascript:void(0)">less answers...</a>
	<div class="answer" data-handle="dj2hkyh">
		<a class="author" href="https://www.reddit.com/user/crecod" target="_blank">crecod</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>I work in a mid sized software company and my delivery team is responsible for our main product (80%+ of sales). We have tools that give us reports on the post back errors each morning and they are designed to help us decide which order to tackle the issues in (there are far too many to complete all of them).
We use metrics around the number of clients affected, or if one of our big important clients are affected (like every business, some clients are worth far more to the company than others so they get preferential treatment) or the total number of hits on a single issue regardless of client. We the spend a couple of hours on these before moving onto the new functionality we want to add.
We also have a section on our report for our support team who work with clients. Here there are things like issues we already resolved (ie. ask client to take upgrade) or where there might be an environmental issue (failed to write a file due to being disallowed permission to the directory or something - here, support can work with the clients IT to resolve). Basically anything that won't require a software change.
This may or may not be industry standard, but it is what we do to try and reduce the issues.
Hope this helps! </p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="dj2m6im">
		<a class="author" href="https://www.reddit.com/user/bhearsum" target="_blank">bhearsum</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>There's a lot written up about Socorro (Mozilla's crash reportinc system). Here's some links that might be of interest to you:</p>
<p><a href="https://wiki.mozilla.org/Socorro" target="_blank">https://wiki.mozilla.org/Socorro</a></p>
<p><a href="https://blog.mozilla.org/webdev/2010/05/19/socorro-mozilla-crash-reports/" target="_blank">https://blog.mozilla.org/webdev/2010/05/19/socorro-mozilla-crash-reports/</a></p>
<p><a href="https://github.com/mozilla-services/socorro" target="_blank">https://github.com/mozilla-services/socorro</a></p>
<p><a href="https://socorro.readthedocs.io/en/latest/" target="_blank">https://socorro.readthedocs.io/en/latest/</a></p>
<p><a href="https://crash-stats.mozilla.com/home/product/Firefox" target="_blank">https://crash-stats.mozilla.com/home/product/Firefox</a></p></div>		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="dj35nz7">
		<a class="author" href="https://www.reddit.com/user/andrew_rdt" target="_blank">andrew_rdt</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>The best way is looking for identical crash reports and grouping by count.  The ones that happen the most are usually easier to reproduce and will fix the most users so it's time well spent.  Most crash reports have some info related to what the app was doing at that time.</p>
<p>If there is not enough info to fix an issue the fix may just be to add more info for the crash report so you'll have better luck when it happens on the updated version.</p></div>		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="dj4dwxq">
		<a class="author" href="https://www.reddit.com/user/veryveryveryserious" target="_blank">veryveryveryserious</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>On our team we collect shit loads of all kinds of telemetry data from browser, including crashes. We also have teams of testers constantly finding bugs, filing them, and then there's a process to fix them.  The telemetry is indeed &quot;big data&quot;.. But for a lot of it we take shortcuts like taking samples and kind of guessing the real impact based on scaling factors.</p>
<p>We also collect tons of anecdotal feedback like comments and stuff that users submit but that is a lot harder to analyze</p></div>		<div class="replies-placeholder"></div>
	</div>
</div>