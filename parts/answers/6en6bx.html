<div class="answers">
	<div class="answer" data-handle="dibjffr">
		<a class="author" href="https://www.reddit.com/user/ericGraves" target="_blank">ericGraves</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>Kinda. They were probably discussing <a href="https://www.math.uni-bielefeld.de/ahlswede/homepage/public/199.pdf" target="_blank">identification codes</a>. And, yes in some way, you can represent 2^(2^n) different states using an n bit sequence. </p>
<p>But to understand how, you have to understand the operational definition of representation here. ID codes operate under the assumption that there is one source and a large number of readers, each reader wants to identify whether the message the source sends is some unique message (the original example was wives of sailors wanting to know if their husband had drowned). There are two operational requirements for ID codes, when the source does produce their message the probability that the correct reader positively identifies the message must go to one, and if the source does not produce their message the probability the reader miss-identifies it must go to zero. Replacing the last condition instead with the probability that there is a reader that miss-identifies goes to zero instead results in only being able to represent 2^(n), instead of 2^(2^n).</p>
<p>As one might expect then from the preceding, with an ID code there <em>will be errors.</em> Just the probability of it being any particular reader will go to zero. And this hints at the design, but lets walk through it using n bits. With n bits, there are 2^(n) different sequences, and to each reader the ID code will associate some fraction, q2^(n), of the total. When the source wants to send a message to a particular reader, the source produces a n bit sequence by randomly choosing from those associated with the particular reader. This scheme works because the amount of overlap between any two of these sets can be made relatively small.</p>
<p>More specifically, from a Lemma in the originally linked paper, it is possible to create at least N = exp(q 2^(n) - n ln2) different subsets (A1, A2, ... AN) from the set of n-bit sequences, such that these sets satisfy</p>
<ul>
<li>|AI| = q 2^(n)</li>
<li>|AI intersection AJ|/|AI| &lt; ln(2e)/ln(q^(-1)-1) .</li>
</ul>
<p>Setting q = n^(-1) results in at least 2^(2^n-2log n) different sets, and the intersection between any two sets is basically 1/ln(n). As n gets larger the percentage in any particular set intersection decreases, and as a result the probability of a randomly selected message being in that intersection also decreases. What you have at the end is that reader A will identify a message when sent to it, and that with high probability reader B will not miss-identify the message. The number of unique messages though, is double exponential with the block length.</p>
<p>Perhaps more remarkable is that given those two operational definitions it is possible to show that 2^(2^n) is also the maximum number of messages which may be identified. This leads what we call ID-capacity, which is defined as the log(log( # messages that can be identified)). Furthermore, If you were to add a channel the messages had to be identified over, the ID-capacity would equal the channel capacity (in other words you can send 2^(nC) messages or identify 2^(2^nC), where C is the capacity of the channel).</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<a class="less-answers upper" href="javascript:void(0)">less answers...</a>
	<div class="answer" data-handle="dicgrze">
		<a class="author" href="https://www.reddit.com/user/2358452" target="_blank">2358452</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>You can represent more than 2^n states using n bits if you allow for some error. For example, you could make groups of uniform size k, have an error rate of 1-1/k, but be able to count k 2^n states. Like /u/ericGraves showed, you can also make sure that those groups are randomly selected.</p>
<p>If you get clever with your groups there are interesting applications.</p>
<p>One such application is <a href="https://en.wikipedia.org/wiki/Approximate_counting_algorithm" target="_blank">approximate counting</a>, allowing for you to <em>count</em> to arbitrarily high values with limited precision. </p>
<p>For example, say you have only 1 bit but want to be able to count to 1 million. Then each for each event (say a person passes by) you could simply chose to count with probability 1/1000000, and do nothing with probability 999999/1000000. On average, it will require 1000000 events for your bit to flip to 1, hence if you see 1 it's likely you've gone over 1 mill. The two groups might represent '0'-&gt;[0-999999] and '1'-&gt;[1000000-inf] in this case.</p>
<p>There's one way to generalize this such that with high probability, the number you are counting has good <em>relative precision</em>. This way n bits allow you to count up to ~2^(2^n) with any fixed relative precision (e.g. you could tolerate at most a 10% error). This is achieved by making the group sizes proportional to the numbers: small numbers belong to small groups, large numbers large groups; and by using randomness to increment your counter*. </p>
<p>This technique is actually used in practice afaik.</p>
<hr />
<p>Another completely different way to think about having more than 2^n states using n bits is simply through compression. If some states are more likely than others, you can use <em>on average</em> less bits than n to identify each of 2^n states. This is done by associating shorter codewords with most likely states, and using the longer ones for less likely states. In this manner, you don't lose any precision, but may need more than n bits to identify a state if you're unlucky. The best average number of bits you can achieve with this scheme is given by the entropy of the state distribution.</p>
<p>Those two approaches are roughly lossy compression and lossless compression respectively.</p>
<hr />
<p>* Note there's a technical caveat: to generate the random numbers themselves (unless you're given them from an oracle), you need to generate a number of bits and count them. Usually this means you need log(log(x)) bits of memory to generate x, which is actually not a problem in the case above, since log(log(2^(2^n))) = n; but if you want to count higher (like in the case where you use 1 bit to count to 1 million), then that's a problem. Of course, you can realize you've fallen into another counting problem, which really allows you to use the same algorithm again (at another loss of precision). So you can count up to 2^2^2^2^1 = 65536 by using a 2^(2^1) = 4 bit counter, but you could also again use a probabilistic counter to count up to 2^(2^2^1)=16, which loses precision but requires counting only up to 2^(2^1)=8, plus a register bit. You see you can continue until you have to count up to 2 and there's no probabilistic improvement. So even tolerating for large uncertainties I believe the utmost k you can count up to using this kind of method uses log*(k) bits (which is the <a href="https://en.wikipedia.org/wiki/Iterated_logarithm" target="_blank">iterated logarithm</a> function). But this is an extremely slow function, that is, you can count up to absurdly high values: using only ~5 bits (multiplied by some constant) you could approximately count to 2^65536 ; much more than the number of atoms in the universe.</p></div>		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="dibq8li">
		<a class="author" href="https://www.reddit.com/user/stiffitydoodah" target="_blank">stiffitydoodah</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>This amounts to sort of a workaround, but it seems to be a semi-relevant aside: there are estimates (I'm thinking of a book from a few years ago by Eliasmith) that each &quot;spike&quot; of a neuron in the brain, which is a binary signal, encodes on average 2-3 bits of information. The trick there is that the information is encoded in the timings between spikes, so really there isn't a magical boost in the information rate, it's just that the information is tracked locally, while minimizing the communication overhead.</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="dibsret">
		<a class="author" href="https://www.reddit.com/user/prince_polka" target="_blank">prince_polka</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>It does not make sense that you would be able to represent more than 2^n states and I do not believe it is possible.<br />
However you can use those available states to do neat things.<br />
As a simple example you can compare integers to floats, the integers represent evenly distributed numbers, and floats exponentially increasing numbers.<br />
The float can only represent a tiny fraction of the larger integers exactly and &quot;uses the gaps&quot; to represent small decimal numbers and astronomically large numbers.<br />
Also with hash sums you can identify a file larger than the numbers of bits in the hash, there is principle a vast amount of hash collisions but the chance of an actual collision in practice happening for any specific file is vanishingly small.  </p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="diceac1">
		<a class="author" href="https://www.reddit.com/user/Lordbug2000" target="_blank">Lordbug2000</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>I'm not positive but I think you meant you can use 4 bits and store more that 4 states? If that is the case it works like this:</p>
<p>It's called binary which works as a base2 number system, our typical system is base10. Each position is either on (1) or off (0). And each position(n) is equal to 2^n.</p>
<p>Eight bits is equal to one Byte, not super relevant info but still important.</p>
<p>00000000 = 0
00000001 = 1
00000010 = 2
00000011 = 3</p>
<p>So as you can see, 2^0 = 1 but you can represent two states, 2^4 = 8 but you can represent 15 different states 0001, 0010, 0011, 0100, 0101, 0110, 0111... you get the idea. </p>
<p>Each time you increase n by 1, you increase your possible states by 2^n+1 because the first bit is 2^0</p>
<p>4 bits=15 states, 5 bits=31 (15+16), 6 bits=63 (31+32)</p>
<p>Disclaimer: It is late-ish, and I'm doing this all in my head, so if I've done my math wrong somewhere, or I've been super confusing, let me know and I'll try to fix it.</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
</div>