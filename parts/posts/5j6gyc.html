<li class="post" data-handle="5j6gyc">
	<div class="overview">
		<a class="source" href="https://www.reddit.com/r/askscience/comments/5j6gyc/what_gives_neural_networks_an_advantage_over/" target="_blank" title="Reddit thread where this comes from"><i class="fa fa-external-link" aria-hidden="true"></i></a>
		<h2>
			<span class="tags tag-Computing">Computing</span>
			<a href="/posts/5j6gyc" onclick="return false">What gives neural networks an advantage over other machine learning solutions?</a>
		</h2>
		<!--<span class="date">2016-12-22</span>-->
		<span class="is-new">NEW</span>
	</div>

		<div class="question"><span class="qa" title="Question">Q:</span><div class="markdown"><p>What gives neural networks an advantage over other machine learning solutions?</p></div></div>

	<div class="comment-section">
		<div class="answers-placeholder">
			<div class="answers">
	<div class="answer" data-handle="dbdtehi">
		<a class="author" href="https://www.reddit.com/user/scienceistoohard" target="_blank">scienceistoohard</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>A couple of different things.</p>
<ul>
<li>They seem to be able to approximate high-dimensional functions while still capturing the correlations that we care about.</li>
</ul>
<p>For example, you might want to look at images and figure out whether or not they contain dogs. You can do this by writing a function f(x1,x2,x3...xn) where the variables xi are the pixels in an image, which will return 1 if there's a dog, and 0 if there isn't. Even if you can find such a function, it's extremely difficult or even impossible to actually store it; if your images are grayscale, you have 256^n different values to store to fully evaluate the function. There are approximation methods from convex optimization or linear algebra, but they produce poor results for functions like this. Neural networks, on the other hand, seem to work well, despite being small enough to store and evaluate on computers.</p>
<ul>
<li>Neural network models can be systematically fitted for solving problems, and it seems to work pretty well.</li>
</ul>
<p>There are effective ways of fitting neural network models (i.e. &quot;training&quot; them) that seem to work pretty well, and they mostly just rely on the chain rule of calculus. If you have a lot of data, a lot of time, and a good method for doing your fitting, things seem to work well. With other methods, they are often either easy to fit and not very effective, or potentially very effective but difficult to fit. With the &quot;kernel trick&quot;, for example, you can use otherwise less effective methods to get really good results, but this is mostly just by giving yourself a new, hard problem to solve: you now have to find a good kernel, which is a problem that's difficult to solve systematically.</p>
<p><a href="http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/" target="_blank">Here are some nice visualizations</a> that show how neural network training works, which can help you to contrast neural network classifiers with linear classifiers.</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<a class="less-answers upper" href="javascript:void(0)">less answers...</a>
	<div class="answer" data-handle="dbe0iw4">
		<a class="author" href="https://www.reddit.com/user/thecomputerscientist" target="_blank">thecomputerscientist</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>Neural networks are very powerful in that they can learn arbitrarily complex functions. When training a neural network, you don't need to have any idea what kind of function you're trying to approximate to represent your data. You just let the Neural network do all that work for you during learning.</p>
<p>The caveat is that the end result after training is impossible to interpret or understand for humans. We just know that when we feed data through the neural network, it will give us the right classification, assuming training was done properly.</p>
<p>Other machine learning techniques are limited to only certain kinds of functions, or  you need to know what kind of function you want to approximate beforehand.</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="dbe62za">
		<a class="author" href="https://www.reddit.com/user/6thReplacementMonkey" target="_blank">6thReplacementMonkey</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>Any machine learning solution involves fitting a model to data. There are three problems to solve when doing that:</p>
<p>1) Getting enough of the &quot;right&quot; data of good quality</p>
<p>2) Selecting a model that is <em>capable</em> of fitting the data</p>
<p>3) Training the model to actually fit the data</p>
<p>Neural networks have big advantages over other solutions in the first and second part of that. They are capable of building their own &quot;features&quot; out of the available data, ignoring whatever needs to be ignored and combining data in whatever ways are necessary to discover patterns. For the second part, it has been shown that arbitrarily complex neural networks are capable of modeling <em>any</em> computable function. In other words, if it is possible to map a given set of inputs to a given output, there is a neural network somewhere that can do it to arbitrarily high precision. That means with a sufficiently large network, you don't have to select a model - it will figure out the details for you.</p>
<p>The third aspect is where neural networks don't necessarily perform as well as other options. Most of the network architectures used in practice are trainable via gradient descent, which means you can compare the outputs you predict to the &quot;real&quot; output, then make small adjustments to the parameters to make sure that the next test will be a little closer to the right answer. Gradient descent is fast, but it only works really well if the problem is <em>convex</em>, meaning that there is one global optimum, or one &quot;best&quot; answer with all other answers being steadily worse the farther away you get from the best answer. Neural networks tend to not be convex, which means they have local minima. This means gradient descent algorithms can get &quot;stuck&quot; in locally good solutions that still aren't globally good. There are ways of dealing with this problem, but other methods that have convex parameter spaces are easier to train.</p>
<p>Another thing that makes them less good than other solutions (in some applications) is that their very high adaptability means they can easily <em>over-fit</em> data - they can learn too literally, and not do a good job making predictions for data that is outside the training set. This is called &quot;low bias&quot; in the model - it means the model doesn't make any assumptions about the data before it tries to fit it. To combat that, it is important to not make the network <em>too</em> flexible - you want it to be just flexible enough that it makes suitable predictions. You also tend to need much larger data sets than for other methods, because for a given model complexity, more data will prevent over-fitting.</p>
<p>tl;dr: Neural nets don't need to make assumptions about the data in order to fit it. It turns out that for problems where it is hard to guess what data is important or how it is related to the outputs, and where you have a <em>lot</em> of data, neural networks work really well.</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="dbe0wg4">
		<a class="author" href="https://www.reddit.com/user/PdotT" target="_blank">PdotT</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>One problem that neural network methods can really help address is <em>feature selection</em> (sometimes 'feature engineering'). In many settings it can be hard to decide what model to use and how to represent your data before feeding it into your model. For example, some older machine vision methods relied on hand-crafted image filters designed to do intuitive things like locate edges or other local patterns. <a href="https://www.reddit.com/user/scienceistoohard" target="_blank">/u/scienceistoohard</a> has already hinted at the problem of choosing the right kernel when using the 'kernel trick'.</p>
<p>Neural networks can be used to 'learn' meaningful features that represent your data well without having to hand-build too much prior knowledge into the system. Essentially, the problem becomes one of optimizing the network weights and shape rather than manual feature + model selection using our knowledge about the domain. <a href="http://www.jmlr.org/papers/volume11/vincent10a/vincent10a.pdf" target="_blank">Stacked De-noising Autoencoders</a> are a good example of neural networks being used for unsupervised feature selection before feeding the results into an image classification model. The hierarchical nature of features in neural nets - especially convolutional nets - where earlier units detect simple local features like lines and edges, while deeper layers might detect more complex structure - is very appealing for vision tasks like this!</p>
<p>In a related way, neural network methods can sidestep challenges in <em>model selection</em>. Rather than worry about what type of classifier or regression model to use, we can construct a network that fits our input and output domains appropriately and let the optimizer do the work. Consequently, there is a large body of <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf" target="_blank">literature</a> on how to optimize neural nets well, and generally for simpler problems, neural networks are usually not going to be much better than other easier to implement techniques. Training a neural net necessarily involves a fair amount of tuning over the structure of the network (what non-linearities do i use? How wide/deep? Convolutions? etc..) that can be a whole headache in itself. It is also computationally expensive unless you can afford a bunch of shiny GPUs.</p>
<p>A good example of where learning features with neural nets has been really successful is in vector representations of language, known as <a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank">Distributional Semantics</a>. In simple terms, older methods hand built representations of words by going through text and counting which words appear often together. It turns out that these lists of <em>co-occurence</em> counts can encode quite a bit of useful info about a word's meaning, which can then be used to represent that word for other machine learning tasks like sentiment analysis, information extraction etc... A few years ago, some neat methods for learning these kinds of representations in a less supervised way using neural nets, such as the one linked above, came along and really blew these count vectors out the water.</p>
<p>The above are not the only things neural nets have going for them (others have talked more about universal approximation) but they're certainly a part of it.</p></div>		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="dbelrww">
		<a class="author" href="https://www.reddit.com/user/unreplicate" target="_blank">unreplicate</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>In the standard model, current learning problems can be seen as having given a function over feature space.  For example, the input is a vector space of pixel values for image of letters, some wedge of this input space might have the value the letter &quot;A&quot;. This function might be called &quot;supervisor&quot; function.</p>
<p>The learning machines (LMs) try to learn the supervisor function from finite sampling of the points. Usually, the LMs just try to emulate the level curves of the supervisor function--that is, find the regions within the supervisor function that have constant values (e.g.&quot;A&quot; values, &quot;B&quot; value, etc.) As the supervisor function becomes complex, the geometric sections of the feature space that gets assigned values like &quot;A&quot; becomes more complex. For example, all of the images that correspond to the letter &quot;A&quot; might have a simple pixel-vector configuration for TimesRoman but become very complex with hand written print.</p>
<p>Neural nets tries to solve this problem by constructing piece-wise functions. That is, define functions over little pieces of the feature space and join these pieces together. Roughly, the nodes of the neural nets are these piecewise functions and the layers are different ways of joining them. Thus, very complex supervisor functions can be exactly emulated. </p>
<p>Support vector machines take a different approach. They use simple functions (more or less linear functions). To deal with the complex supervisor functions, they take the approach of embedding the feature space in much higher dimensions. This can make the problem simpler. For example, if you were to try to delineate a section of earth from the point of view of 2-dimensional surface, it would be somewhat complex. But, from the view of the surface of a sphere in three dimensions, we can just slice things off. The important &quot;trick&quot; is that this embedding can be done implicitly using kernel functions.</p>
<p>The problem with neural nets is that because it is piece-wise, the optimization is difficult and because you have the freedom to construct more or less arbitrarily complex functions, you can really over-fit the problem. Both of these problems are much better solved today because of better computers and the vast amount of electronic information available. People used to train optical character recognition on thousands of examples. Now, you can train on hundreds of millions. More training data you have, more you can avoid over-fitting. In the meanwhile, using kernel functions is a neat idea and there are much less computational load problems with SVM, but in practice there just isn't the kind of generic flexibility as with neural nets.</p>
<p>PS. I dont' think &quot;auto-encoding&quot; itself ia unique to neural nets.</p></div>		<div class="replies-placeholder"></div>
	</div>
</div>		</div>
		<div class="more-less">
			<a class="collapse" href="javascript:void(0)">collapse</a>
			<a class="more-answers" href="javascript:void(0)">4 more answers...</a>
			<a class="less-answers lower" href="javascript:void(0)">less answers...</a>
			&nbsp;
		</div>
	</div>
	<a class="show" href="/posts/5j6gyc" onclick="return false"><span>show</span></a>
</li>
