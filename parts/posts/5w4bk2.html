<li class="post" data-handle="5w4bk2">
	<div class="overview">
		<a class="source" href="https://www.reddit.com/r/askscience/comments/5w4bk2/how_can_we_be_sure_of_the_precision_and_accuracy/" target="_blank" title="Reddit thread where this comes from"><i class="fa fa-external-link" aria-hidden="true"></i></a>
		<h2>
			<span class="tags tag-Engineering">Engineering</span>
			<a href="/posts/5w4bk2" onclick="return false">How can we be sure of the precision and accuracy of modern measurement tools?</a>
		</h2>
		<!--<span class="date">2017-02-28</span>-->
		<span class="is-new">NEW</span>
	</div>

		<div class="question"><span class="qa" title="Question">Q:</span><div class="markdown"><p>Suppose I have defined a 'redditmeter' [rm] in some acceptable way (that is - I can always know that this 'thing' that I measure is indeed rm units in size). After a few months, a new way was invented to measure 0.5rm, so on so forth - we get to the smallest scales.</p>
<p>I logically conclude that this process is a very crude way of what happened in the way we humans measure things like length, weight  etc.</p>
<p>But how can we be sure that the scales we measure today are actually accurate? if we can measure 0.5rm with 99% accuracy, then measuring 0.25rm might have even less accuracy, going all the way to 1*10^-[integer] rm.</p>
<p>How can we know that our measurement tools are actually acceptably precise?</p>
<p>Or to put it in another words - How do we check our most modern and precise measurement tools?</p>
<p><em>Edit</em></p>
<p>Thank you for your current attempts of answering, but my question wasn't how can we be sure that a kilogram is a kilogram.
To clarify furthermore - How can we be sure that the most modern measurement device actually measures with a good enough precision and not with it's measurement fault being 50% of accuracy (50% of times or 50% of given value).</p></div></div>

	<div class="comment-section">
		<div class="answers-placeholder">
			<div class="answers">
	<div class="answer" data-handle="de76jlz">
		<a class="author" href="https://www.reddit.com/user/_amas_" target="_blank">_amas_</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>Currently, most units of measurements are derived from fundamental physical properties. For example, the meter is defined as the distance that light travels in a vacuum in 1/299792458 seconds. There is no ambiguity or inaccuracy in this definition, because it's defined relative to an unchanging constant of the universe. </p>
<p>If you look up various other units, you'll see that they are defined in this same sort of way. This was done to avoid any confusion as to the accuracy or measurement issues with picking a set of units. </p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<a class="less-answers upper" href="javascript:void(0)">less answers...</a>
	<div class="answer" data-handle="de7aamz">
		<a class="author" href="https://www.reddit.com/user/BackSack" target="_blank">BackSack</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>Not sure if this is what you're asking but I'll try to chime in.
On the industry or production side, measurement assurance can be achieved by verifying a tool meets the requirements to successfully measure the measurand, typically using what is called a &quot;<a href="https://en.m.wikipedia.org/wiki/ANOVA_gauge_R%26R" target="_blank">Gauge R and R</a>&quot;.
There are of course other methods. For instance, ASME and ASTM provide guidance and documentation on measuring tools and setups for a particular measurand so you can make sure your tool accuracy doesn't significantly contribute to the error.</p>
<p>In the calibration side of things, there is an industry set up to verify that tools are traceable back to the SI units through an appropriate governing body (NIST in the US, NFL in UK, lots of others...). There is strict documentation on verifying the tools accuracy, in the past a loophole in ANSI z540 allowed for a 4:1 Test Accuracy Ratio to be used meaning the calibration standard needed to be at least 4 times more accurate than the unit under test. This has given way with the hesitant adoption of ANSI z540.3 which now uses a 2% Probability of False Accept, which requires the lab to use the Measurement Uncertainty instead of Accuracy.</p>
<p>At any rate, there are strict guidelines in place to verify the tool is performing within its manufactured specifications.
With a properly calibrated tool, a proper measurement system, and possibly control charts and check samples, you can assure that your instrument does not contribute to the error in a significant way.</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="de77zdf">
		<a class="author" href="https://www.reddit.com/user/The_Old_Wise_One" target="_blank">The_Old_Wise_One</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>Typically, it is through repeated measurements of the same object/event. Taking your example, I may measure an object and find that it is 0.75 redditmeters. If I am uncertain in my measuring tool's accuracy, I can measure the object multiple times–maybe I get 0.74, 0.745, and 0.78 redditmeters for my new measurements. I can then use statistics to compute a confidence interval around my measurements. In the above example, the 95% confidence interval would be: 0.75375 ± 0.018. </p>
<p>The interval is calculated using the mean and standard deviation from the 4 measurements. You may want to look into <em>measurement error</em> for more information.</p>
<p>Edit: See the <strong>Distribution Curves</strong> section <a href="http://webs.mn.catholic.edu.au/physics/emery/measurement.htm" target="_blank">here</a> for more information.</p></div>		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="de7c7fz">
		<a class="author" href="https://www.reddit.com/user/MpVpRb" target="_blank">MpVpRb</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>In practice, it works like this</p>
<p>A primary standard is created, based on physical constants. The primary standard is usually located in a lab, and is expensive and complex to use. It is used to calibrate secondary standards, which are distributed to users</p>
<p>Secondary standards are used to calibrate working standards, which are used to calibrate the measurement tools used in the shop</p>
<p>Depending on requirements, periodic re-calibration is often required. The calibration process is controlled by paperwork and oversight by inspectors</p>
<p>I used to work in metrology</p>
<p><a href="https://en.wikipedia.org/wiki/Metrology" target="_blank">https://en.wikipedia.org/wiki/Metrology</a></p></div>		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="de7slhj">
		<a class="author" href="https://www.reddit.com/user/NotTooDeep" target="_blank">NotTooDeep</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>To your revised question: We can never be sure an instrument of any kind is measuring with a good enough precision until we have defined what good enough precision means.</p>
<p>I once had a client walk into our machine shop and ask for an aluminum part to be custom made within a millionth of an inch of the specified dimension. We agreed because we knew he couldn't measure that close. The part would change dimension more than that from the heat of his hand.</p>
<p>The simplest means of accurate measurement of something really small, like a few millionths of an inch, is an optical comparator. It basically projects an image of the small feature onto a large screen. We know the precise amplification. We can simply compare the shadow on the screen to a cardboard template made to the same proportions, and tell with excellent accuracy if the part feature is 2 millionths high or 5 millionths high. We can also tell if the angles on the part are within spec.</p>
<p>And there you have the most important phrase:  &quot;within spec&quot;. Measurement only has meaning relative to an expectation. </p>
<p>The front tube of the Hubble Telescope has a series of graphite rings of varying inside diameter. They are spaced a few inches apart. The function like a pinhole camera, causing the light entering the tube from different angles from the axis of the tube to be absorbed, never reaching the lens of the telescope. These all reduce noise in the images captured by Hubble. </p>
<p>So the accuracy of those images depends on doing the best we can with all the operational parameters we are faced with, and with the technologies at hand. </p>
<p>BTW, those graphite rings, or baffles, were machined along the inside diameter to a taper, such that they became translucent near the inside diameter. Does anyone here know why? We on the shop floor who did that grinding of the taper never learned why.</p></div>		<div class="replies-placeholder"></div>
	</div>
</div>		</div>
		<div class="more-less">
			<a class="collapse" href="javascript:void(0)">collapse</a>
			<a class="more-answers" href="javascript:void(0)">4 more answers...</a>
			<a class="less-answers lower" href="javascript:void(0)">less answers...</a>
			&nbsp;
		</div>
	</div>
	<a class="show" href="/posts/5w4bk2" onclick="return false"><span>show</span></a>
</li>
