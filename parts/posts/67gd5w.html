<li class="post" data-handle="67gd5w">
	<div class="overview">
		<a class="source" href="https://www.reddit.com/r/explainlikeimfive/comments/67gd5w/eli5_how_do_cameras_or_their_lenses_autofocus/" target="_blank" title="Reddit thread where this comes from"><i class="fa fa-external-link" aria-hidden="true"></i></a>
		<h2>
			<span class="tags tag-Technology">Technology</span>
			<a href="/posts/67gd5w" onclick="return false">How do cameras, or their lenses, auto-focus?</a>
		</h2>
		<!--<span class="date">2017-04-28</span>-->
		<span class="is-new">NEW</span>
	</div>

		<div class="question"><span class="qa" title="Question">Q:</span><div class="markdown"><p>At first I thought that maybe the concept was a little similar to our eyes in that it tries to get as much of the incoming light on one point, the middle of the sensor, and so adjusts the focus ring until it finds a position with the largest amount of light on this spot. However, this doesn't explain (to me, anyway - maybe I'm just being thick) how a lens will be able to refocus on objects based on distance.</p></div></div>

	<div class="comment-section">
		<div class="answers-placeholder">
			<div class="answers">
	<div class="answer" data-handle="dgq68jr">
		<a class="author" href="https://www.reddit.com/user/ConfusedTapeworm" target="_blank">ConfusedTapeworm</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>2 ways:</p>
<ul>
<li>
<p>You digitally analyze the frame to figure out the least blurry lens configuration. Least blurry means sharpest color transitions between neighboring pixels. Only works in certain types of digital cameras. </p>
</li>
<li>You measure the distance between the camera and the target, and figure out an appropriate lens configuration.  Little IR sensors can be used to measure the distance.</li>
</ul></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<a class="less-answers upper" href="javascript:void(0)">less answers...</a>
	<div class="answer" data-handle="dgq71pq">
		<a class="author" href="https://www.reddit.com/user/Target880" target="_blank">Target880</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>There is multiple ways to do it.</p>
<p>A way to do it  is to measure contrast. It you take the difference between two adjacent pixels on the sensor the maximum difference will occur when the image is in focus. It is obvious when you think about it because a out of focus image is blued and the difference between pixel will be smaller then in a in focus image</p>
<p>Laser Auto Focus was used on cell phones. If that is used the a LED or laser is used to illuminate the subject with a short puls. The device measure the time it takes for the light to travel to calculate the distance </p>
<p>Phase detection is also used where the incoming light is split into two images and compared. Look at the image on the <a href="https://en.wikipedia.org/wiki/Autofocus#Phase_detections" target="_blank">Wiki</a> of a explanation since it is hard to do in text but simpler to understand from the image</p></div>		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="dgq9jb2">
		<a class="author" href="https://www.reddit.com/user/aaaaaaha" target="_blank">aaaaaaha</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>The lenses don't do any auto-focus processing. It's all in a the camera for which there are two (main) methods. I will try to eli5 as much as possible. </p>
<ol>
<li>contrast detect (most compact and mirrorless cameras.) the camera hunts back and forth until it detects dramatic transitions between two colors in a defined area. </li>
<li>phase detect. (DSLRs) There is an autofocus sensor separate from the imaging sensor in a camera. This sensor detects only portions of an image in multiple places. When out of focus these portions are considered &quot;out of phase.&quot; How &quot;out of phase&quot; these portions are how much the camera should turn the lens to focus. Users of old film cameras used <a href="http://blog.epicedits.com/wp-content/uploads/manual-focus-indicator-1500.jpg" target="_blank">a split prism in the viewfinder</a>  <strong><em>edit</em></strong> - since phase detect is difficult to explain in words it's basically a modernized version of the split prism, and the mechanism has been moved out of view. </li>
</ol>
<p>2a. In recent years a hybrid of contrast detect has been developed that puts phase detect capabilities directly on the sensor, providing this capability to non-dlsr's </p>
<p>Bonus: <a href="https://www.lensrentals.com/blog/2010/07/how-autofocus-often-works/" target="_blank">the eli20 version</a> </p></div>		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="dgq9p6n">
		<a class="author" href="https://www.reddit.com/user/F0sh" target="_blank">F0sh</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>Rangefinding and Contrast-detection autofocus have already been covered, but phase-detection is more important than rangefinding, and historically was more important than contrast-detect. It's still used on every SLR camera and is generally better.</p>
<p>First of all you have to know what makes an image blurry when it's out of focus. This is because light from a single object - let's say the tip of your subject's nose - can travel different paths before getting to the film or image sensor. In particular it could travel straight into the middle of the lens, or more towards one of the edges. If these rays of light aren't all brought into the same point on the sensor then they get smeared over a big area, and you get a blurry splodge instead of a nice nose.</p>
<p>Contrast-detection works by looking at the splodge and shifting the bits of the lens around until it's less splodge-y and more nose-y. But this is unreliable if your nose happens to be indistinct and splodge-y already - what then? Well we can do this more intelligently. Remember the different paths light can take, through different parts of the lens? We can separate out the light going through different bits of the lens and divert it towards special auto-focus sensors. Now, the images formed on these sensors will be much less blurry, even when the image is out of focus, because each image is formed from something more like a pinhole camera. Instead, the images formed will be different from one another - shifted with respect to each other. (This makes a lot of sense if you think about it - the blurred main image is formed from infinitely many slightly-moved images all on top of one another)</p>
<p>So now you can shift the lens's focus mechanism and the camera will see these two little images line up with one another - and then you're in focus. Another advantage is that you can tell which direction you have to move the images in to line them up, and this how to adjust the lens; with contrast-detection you only know that it's blurry and have to guess which direction will make it less blurry.</p></div>		<div class="replies-controls">
			<a class="show-replies" href="javascript:void(0)">show replies...</a>
			<a class="hide-replies" href="javascript:void(0)">hide replies...</a>
		</div>
		<div class="replies-placeholder"></div>
	</div>
	<div class="answer" data-handle="dgq8ccy">
		<a class="author" href="https://www.reddit.com/user/punkinfacebooklegpie" target="_blank">punkinfacebooklegpie</a>
		<span class="qa" title="Answer">A:</span><div class="markdown"><p>Active autofocus involves measuring the actual distance to the subject, usually by infrared or ultrasonic rangefinders.</p>
<p>Passive autofocus is more common. It involves finding the greatest contrast between pixels. This is easier than it sounds. Imagine a dark subject on a light background. To focus on the subject, you place the autofocus sensor on it's outer edge. The sensor is a small line of pixels such that, when oriented correctly, the dark subject is on one half and the light background is on the other. When the subject is out of focus, the dark and light pixels will be blurred together into a middle value. Focus is performed by moving the lens toward or away from the imaging device. As you move the lens toward the correct focal point, the subject's edge sharpens and the dark and white values become distinct. Eventually the lens moves past the point of focus and the values blur together again. The autofocus computer looks for the point of maximum difference between pixels on each end of the autofocus sensor. It basically steps the lens forward while calculating &quot;average value here minus average value there&quot; and stops when the calculation goes from increasing to decreasing or vice versa. This is the point of focus. As such, the autofocus does not focus on a single point. It requires a small region of contrasting values.</p>
<p>SLR cameras use small dedicated cross-shaped sensors much like the one described above, while fixed lens cameras or your phone use the imaging sensor itself. The principle is the same in either case.</p></div>		<div class="replies-placeholder"></div>
	</div>
</div>		</div>
		<div class="more-less">
			<a class="collapse" href="javascript:void(0)">collapse</a>
			<a class="more-answers" href="javascript:void(0)">4 more answers...</a>
			<a class="less-answers lower" href="javascript:void(0)">less answers...</a>
			&nbsp;
		</div>
	</div>
	<a class="show" href="/posts/67gd5w" onclick="return false"><span>show</span></a>
</li>
