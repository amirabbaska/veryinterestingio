	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Redditor_Reddington" target="_blank">Redditor_Reddington</a>
			<div class="markdown"><p>IIRC, someone once proposed to define a division by zero as &quot;nullity&quot;. I thought it was a ridiculous idea for exactly the same reasons as you're describing here. You can call it whatever you want, that doesn't make it mathematically sound.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/sonomodata" target="_blank">sonomodata</a>
			<div class="markdown"><p>This is the best answer. If there was a use to define a division by 0 in some useful way, we would have done so.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Kljunar" target="_blank">Kljunar</a>
			<div class="markdown"><p>I don't think this is a very good mindset to have about math in general. </p>
<p>Before we made imaginary numbers we didn't have a use for them, but we found a use for them out of their creation. </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Randomize6789322" target="_blank">Randomize6789322</a>
			<div class="markdown"><p>Yeah “useful” there was misguided. If we could find a way to define it that was both <em>consistent</em> with existing theory and <em>free from contradiction</em> we would have done so.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/haukzi" target="_blank">haukzi</a>
			<div class="markdown"><p>That's not really true. We found their use and used them before we rigorously defined complex numbers. See the history of Cardano</p>
<p><a href="https://www.cut-the-knot.org/arithmetic/algebra/HistoricalRemarks.shtml" target="_blank">https://www.cut-the-knot.org/arithmetic/algebra/HistoricalRemarks.shtml</a></p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/sonomodata" target="_blank">sonomodata</a>
			<div class="markdown"><p>I don't mean useful for real world applications. I mean useful in the mathematical system being used. &quot;One plus one&quot; only has meaning because it's defined rigorously in some mathematical axiom or theorems built upon it.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/14pitome" target="_blank">14pitome</a>
			<div class="markdown"><p>I say i have a more compact answere: i works, whe Just don't know what i is. Definig Something= 1/0 Just doesn't Work.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/MrLeville" target="_blank">MrLeville</a>
			<div class="markdown"><p>The use of a b that has to verify at least &quot;b+x=b&quot;and &quot;b*x=b&quot;, for whatever x different from b, seems doubtful indeed.</p></div>		</li>
					</ul>
	