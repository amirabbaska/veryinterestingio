	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/PrivateFrank" target="_blank">PrivateFrank</a>
			<div class="markdown"><p>Ok. I think what I misunderstood was how significance is estimated from each experiment vs all the experiments done.</p>
<p>I thought that if you do 1000 experiments, you would be reporting (in the press) the likelihood of a result given all those experiments, rather than a subset.</p>
<p>This is probably what I get coming from a psychology/neuroscience background rather than HEP!</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/morphism" target="_blank">morphism</a>
			<div class="markdown"><blockquote>
<p>I thought that if you do 1000 experiments, you would be reporting (in the press) the likelihood of a result given all those experiments, rather than a subset.</p>
</blockquote>
<p>RobusEtCeleritas is referring to 1000 experiments where each one of them looks at an entirely different hypothesis. If you have 1000 research teams working in different fields, and the threshold for discovering a result is 3Ïƒ, then ~3 groups will report something even if all the 1000 different null hypotheses are true.</p>
<p>The jargon for experiments that test the same hypothesis again and again is &quot;more data&quot;.</p></div>		</li>
					</ul>
		</ul>
	