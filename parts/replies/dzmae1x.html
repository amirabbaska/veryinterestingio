	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/12vericg" target="_blank">12vericg</a>
			<div class="markdown"><p>How quickly could a modern top-tier consumer PC render Toy Story 1? </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/johnnySix" target="_blank">johnnySix</a>
			<div class="markdown"><p>At the same quality?  Faster than it takes to watch it. </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/tzaeru" target="_blank">tzaeru</a>
			<div class="markdown"><p>I'm not so sure. Toy Story 1 was partially ray traced, with scenes containing upwards to hundreds of lights in total. The animation itself is also pretty expensive. Woody alone had 723 motion controls.</p>
<p>Each frame took from 45 to up to 30 hours to render... <em>On a render farm composed of 117 computers</em>. In total, the movie ended up requiring 800 000 machine hours to render.</p>
<p>AMD Ryzen 7 1800X, released in 2017, can do around 300 000 million instructions per second. IBM-Motorola PowerPC 603e from 1995 would do around 188. That's about 1600 times more instructions per second.</p>
<p>So instead of 800 000 machine hours, you'd need around 500 machine hours to render Toy Story 1. Though it's worth nothing that our rendering techniques have improved and we can offload much of it to GPUs nowadays. Probably around 50 machine hours would be more realistic? Still fairly bit more than the duration of the movie.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/tzaeru" target="_blank">tzaeru</a>
			<div class="markdown"><p>Hard to say. Based on my guesswork <a href="https://www.reddit.com/r/explainlikeimfive/comments/8mboqk/eli5_what_exactly_have_allowed_each_console/dznnfc6/" target="_blank">here</a>, I'd say anywhere from 5 to 500 hours.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Kshnik" target="_blank">Kshnik</a>
			<div class="markdown"><p>Great explanation and great link to your other great explanation.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/skepticalrick" target="_blank">skepticalrick</a>
			<div class="markdown"><p>That's a fantastic answer. Could a supercomputer run a game with insanely realistic graphics then?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/rustlerustlefern" target="_blank">rustlerustlefern</a>
			<div class="markdown"><p>Not necessarily. Hardware is usually designed to do different kinds of calculations, while some hardware could render 3D objects in real time, other more &quot;powerful&quot; and expensive hardware would struggle because it's not designed for it. For example, with CPUs there are some that are designed to run really fast, with few cores (think brains) working to produce a 3D environment in real time. Whereas other workloads would benefit more from a larger amount of cores that are working to complete more separate tasks at once. These often have space/thermal/power/cost limitations and they have to run at slower speeds for stability purposes.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/GegenscheinZ" target="_blank">GegenscheinZ</a>
			<div class="markdown"><p>Some can. Most are designed to do an insanely large number of calculations in a short time, and by short, I mean within a day or so instead of longer than a human lifetime, but not necessarily real-time</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/yaosio" target="_blank">yaosio</a>
			<div class="markdown"><p>Yes, if it's made for that purpose. Nvidia showed real time ray tracing recently using a DGX server. It ran on 4 unreleased GPUs and used a new filtering method allowing the use of fewer rays.</p>
<p>The DGX 2 can make all GPUs in it appear as one GPU, although its not clear if that's only for AI applications or anything that runs on a GPU. This makes it easier for researchers to develop applications as they don't need to worry about getting their AI running across multiple GPUs.</p>
<p>The supercomputers we know today are actually a large number of computers that work together on one task. They would not be able to run modern games even if somebody tried, the latency would be too high between computers. They would be stuck on just one computer out of the entire supercomputer.</p></div>		</li>
					</ul>
		</ul>
	