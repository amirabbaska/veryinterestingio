	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Natanael_L" target="_blank">Natanael_L</a>
			<div class="markdown"><p>And in computing, entropy was described as a measurement of surprise by Shannon. </p>
<p><a href="https://plus.maths.org/content/information-surprise" target="_blank">https://plus.maths.org/content/information-surprise</a></p>
<p>The calculated entropy of some new piece of data depends on how likely that outcome was considered to be. Fewer possible / likely options means less surprise which means less entropy. </p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/itsatumbleweed" target="_blank">itsatumbleweed</a>
			<div class="markdown"><p>Thanks for this. I'm a combinatorialist who knows a bit of the information theory but I had no clue about the physics stuff and have always wondered about it. This was enlightening!</p></div>		</li>
					</ul>
	