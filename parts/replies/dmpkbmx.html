	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/MementoMoriR1" target="_blank">MementoMoriR1</a>
			<div class="markdown"><p>At the risk of sounding like a dick, p-values are merely a short cut to tell you if you're on to something. You should always look for effect sizes. Large sample sizes are almost guaranteed to give statistical significance but extremely small effect sizes (see: Spurious Correlation). I'm hopefully that more disciplines will move toward Bayesian statistics and confidence intervals.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/plugubius" target="_blank">plugubius</a>
			<div class="markdown"><p>One further note: 5% really is 5% here. Normally, when someone tells you they're 95% sure that they are on to something, you take that as virtually certain. But many more than 20 articles have been published where p &lt; 0.05, so you can be sure that many, many of those are reporting &quot;surprising&quot; findings that are false. And since we only publish statistically significant test results, those false results make up an even greater share of what gets published.</p>
<p>In some fields where common sense is pretty accurate, you could say that the <em>only</em> thing that gets published are the 5% of results that are statistical flukes rather than real discoveries. But you cannot prove that with accepted statistical methods, so everyone assumes this describes <em>someone else's</em> field.</p>
<p>So /u/MementoMoriR1 is half right. A focus on p-values leads to absurd results. That does not mean that Bayes had an acceptable solution, either, but the Bayesian critique is still pretty powerful.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Kroutoner" target="_blank">Kroutoner</a>
			<div class="markdown"><p>Confidence intervals and bayesian statistics aren't necessarily any better. Quite ironically, Fisher, who first introduced the foundations of much of frequentist hypothesis testing, disliked Bayesian statistics because the amount of subjectivity it introduced into doing science. Confidence intervals are essentially just hypothesis tests &quot;turned inside-out.&quot; </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Buckhum" target="_blank">Buckhum</a>
			<div class="markdown"><p>To be fair the reviewers would be more open to accepting priors that came from a well conducted experiment or meta-analyses and these would be far less subjective than priors taken from semi-related phenomena. Besides, I have a feeling that once more Bayesian papers are being published, we'll have a rule of thumb for the precision that will be used for priors based on the credibility of their sources. </p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Baloroth" target="_blank">Baloroth</a>
			<div class="markdown"><blockquote>
<p>A p-value is the probability that the results achieved in the study were due to random chance. </p>
</blockquote>
<p>That's not <em>quite</em> true, though subtly so. The p-value says that, given an infinite set of studies, and you choose one at random, the probability that the results of that study deviates that far (or more) from the null hypothesis by chance (and not due to a real effect) is less than the p-value.</p>
<p>As an example, lets say we want to study of whether a coin has a 50/50 chance of landing on heads. Lets say the coin does, in fact, have a 50/50 chance of landing on heads, but if we did a million studies of the fairness of the coin (however we did it, say flipping it a hundred times), 1/20 of them would find, with p value &lt;.05, that the coin was <em>not</em> fair (i.e. that it had slightly more likelihood of landing on one side of the other). Our null hypothesis, in this case, is that &quot;the coin has a 50/50 chance of landing on heads/tails&quot;, and the alternative hypothesis is &quot;the coin has a greater chance to land on one side than the other.&quot; The truth may be the null hypothesis, but we can't know that: all we can do is construct some test that gives us a <em>reasonable</em> belief that it is, or is not, true. Specifically, if we set p&lt;.05 as our &quot;reasonable&quot; value, 19/20 of the studies would say the coin is fair. </p>
<p>None of this tells you if a <em>specific</em> study found the &quot;correct&quot; answer, or if it was one of those unlucky 1/20 studies that just happened (by chance) to randomly produce results that disagree with our set p value. If we reduce the p-value to, say, 0.005, only 1/200 studies would (by chance) produce such a result, if the null hypothesis were true, but any particular study still <em>could</em> (and when you have literally thousands of studies in hundreds of topics, some of them will). In other words, the p-value does <em>not</em> tell you the probability of an individual study being correct or not, but only tells you that <em>most</em> studies with such a p-value will be correct. It's actually impossible to say whether the study is correct or not: we can never really know, only have a reasonable confidence. </p>
<p>This entire field of statistics is actually a bit of a hot topic in science, with wild debate between frequentists (who are &quot;fans&quot; of p-values) and Bayesians (who despise them), but it's a rather subtle debate that you could write books about (and in fact people have and do).</p></div>		</li>
					</ul>
	