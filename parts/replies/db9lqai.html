	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/GruePineapple" target="_blank">GruePineapple</a>
			<div class="markdown"><p>So the common way of describing it as being a measure of disorder is not an accurate way of describing what entropy actually is?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/rantonels" target="_blank">rantonels</a>
			<div class="markdown"><p>No, because there is a certain ambiguity (at the very least) in the words &quot;order&quot; and &quot;disorder&quot;. Sadly entropy is a delicate concept which does not allow for an easy non-technical description and &quot;disorder&quot; is an approximation.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/achtungpolizei" target="_blank">achtungpolizei</a>
			<div class="markdown"><p>I think Feynman once said something along of (para-phrasing): To answer a question someone asked you in a satisfying manner, you'd have to explain the matter in words and concepts the asking person is familiar with.</p>
<p>Now since this is /r/askscience we should expect people not familiar with the technicalities ask for satisfying explanations. For me this means giving answers that might be true in some context but might not reveal the whole knowledge of humanity on said subject. Because then the asking person might aswell just study the whole subject that his question relates to...</p>
<p>edit: what I'm saying is that there needs to be a trade-off in order for this subreddit to have a justification of its existence</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/RobusEtCeleritas" target="_blank">RobusEtCeleritas</a>
			<div class="markdown"><p>Equating entropy and disorder is not the best description. It leads to some misconceptions, like the thought that entropy is a qualitative concept rather than a quantity.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/ericGraves" target="_blank">ericGraves</a>
			<div class="markdown"><p>Only if you want to think of disorder as ``average number of bits needed to describe the state.'' If you take that as your definition of disorder, <del>than</del> then entropy is the disorder times the Boltzmann constant. </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Phooey138" target="_blank">Phooey138</a>
			<div class="markdown"><p>I just finished a thermo course, and was disappointed they didn't cover any of this. Maybe you can help me see what you mean by ``average number of bits needed to describe the state.''</p>
<p>Suppose I say a gas in in  left half of a box, or that it's evenly distributed between the two sides. If we want to describe every microstate corresponding to each, is the idea that we would need one extra bit per particle to get exactly the same resolution in the second case? (Whatever representation we have for a particle we can keep, and just add one bit to indicate shifting the position some fixed distance to the right). I can imagine that the difference between the two remains one bit even if the resolution goes to infinity, but thats where it seems really foggy. We never really talked about information in class. This is the same ratio you get for phase space (here (V2/V1)^N = 2^N, or N bits), is this sort of how it works?</p>
<p>It feels like thermo could have been a totally different class, and I wanted to be taking the other one. Maybe I should just take more thermo / stat mech?</p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/KevinUxbridge" target="_blank">KevinUxbridge</a>
			<div class="markdown"><p>So ... <em>entropy</em> is <em>not</em> an objective feature of a system ... but rather the measure of one's subjective ignorance?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/achtungpolizei" target="_blank">achtungpolizei</a>
			<div class="markdown"><p>&quot;your knowledge about it is minimum&quot; is not making a point that humans are just ignorant and unable to measure things correctly.</p>
<p>It means that there is just less information to deduct from the given system than if it were not in thermal equilibrium. And that's meant in a physical manner.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/KevinUxbridge" target="_blank">KevinUxbridge</a>
			<div class="markdown"><blockquote>
<p>And that's meant in a physical manner.</p>
</blockquote>
<p>Could you clarify that part please?</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/rantonels" target="_blank">rantonels</a>
			<div class="markdown"><p>Entropy is a function of state, where the state is the <em>macro</em>state, which is by definition the information you have about the system. (This is in general a very useful definition of state, especially in quantum mechanics). So yes, you could imagine different people which have access to different informations about the system and they will assign different values for the entropy, which in this general context would be better called information. (Each of them will verify a 2nd law with his own entropy, to make an example).</p>
<p>The standard <em>thermodynamic</em> entropy is implicitly meant to be defined by someone which knows only the standard other thermodynamic quantities (volume, pressure, etc) and is only defined in the states in which the latter quantities are well-defined.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Appaulingly" target="_blank">Appaulingly</a>
			<div class="markdown"><p>The Gibbs paradox, or more specifically the mixing paradox, addresses this understanding of subjective thermodynamics. If we allow two compartments of the same gas to mix the entropy of the system doesn't increase. But that's because we've defined the gas particles as indistinguishable: &quot;... the same gas...&quot;. The energy associated with any change in entropy, TÎ”S, is just the work we need to put into the system to 'un-mix' it or, put another way, bring it back it's original state. If we don't know or can't notice that the gas particles are different then from our point of view we don't need to put any work into the system to bring it back to it's original state.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/RenVit318" target="_blank">RenVit318</a>
			<div class="markdown"><p>These possible states of the system are then the positions in which the particles in the system could be? </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/rantonels" target="_blank">rantonels</a>
			<div class="markdown"><p>That's the configuration, the state is the position and momenta (velocities essentially) of all the particles.</p></div>		</li>
					</ul>
		</ul>
	