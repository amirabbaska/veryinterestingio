	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/IEEESpectrum" target="_blank">IEEESpectrum</a>
			<div class="markdown"><p>What differentiates genomic data from typical data?  In large part economic incentives: the amount of genomic data is growing fast enough it's worthwhile to create special-purpose schemes for storing it.  Lots of &quot;typical data&quot; has all sorts of interesting patterns (say, C++ code), but there isn't that much of it relative to other kinds of data.  A similar example is video: it accounts for such a large fraction of internet traffic that it's worthwhile to create custom schemes and squeeze every bit out that you can, rather than just feed it into a universal tool like gzip/winzip.  There are also differences in access patterns: genomic data might be accessed rarely (say only a couple times after its collection), so it might be worthwhile to compromise on decompression speed in favor of efficiency (kind of like giving gzip the -9 flag).</p>
<p>Ergodicity is an interesting thing to ponder here: on the scale of a single DNA sequencing file (FASTQ file), the quality scores discussed in our Spectrum article are pretty well modeled by a Markov chain of small order.  On the scale of many DNA sequencing files things get interesting: imagine all things that have ever lived as nodes on a giant graph going back billions of years to the origin of life (the &quot;tree of life&quot;, better as the &quot;directed acyclic graph of life&quot;) - then we can think of this graph as the underlying novelty-generating process, and DNA sequencing gives us a noisy view of some of the nodes that are alive today.  So we can imagine a giant repository for all DNA that has ever been sequenced, and its compressed size would be proportional to the underlying evolution rate of life (or just humans, if we restrict to human DNA).</p>
<p>Slepian-Wolf and Wyner-Ziv coding refer to compression of data when some side information is available at the decoder. In the context of genomes, the data to be compressed is the individual’s genome while the side information is the database of already sequenced genomes. Information theory suggests that even though the side information is available only at the decoder, we can still compress as well as the scenario where the side information is available at both the encoder and the decoder.  Traditionally reference-based compression refers to the scenario where the reference available at the end-user (encoder). But going forward, due to security and privacy concerns, it is likely that the encoder will not have access to the genome database. We are currently working on a scheme for Wyner-Ziv style compression, which shows promising results.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/mule_roany_mare" target="_blank">mule_roany_mare</a>
			<div class="markdown"><p>I’m surprised genomes aren’t highly compressible.</p>
<p>It’s a small number of characters, I assume there are huge chunks of repeating sequences, and there are sequences that are common to most people too.</p>
<p>I guess I don’t understand compression because all this sounds pretty ideal.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/ericGraves" target="_blank">ericGraves</a>
			<div class="markdown"><p>Genomes are highly compressible.</p>
<p>My first question is more that there already exist compression schemes which are asymptotically optimal (in terms of expected length of output) for all finite ergodic sources. Furthermore these compression schemes do not need to be aware of the source input distribution. So then, why do we need yet another scheme specifically for genome data? </p>
<p>From their linked article they are using the side information that the genome of any two random individuals will have a large statistical correlation. This correlation can be taken advantage of by using a slepian wolf style coding scheme. Once again I believe there are universal implementations of these algorithms as well, although I am more unsure if they are efficient in doing so. </p></div>		</li>
					</ul>
		</ul>
	