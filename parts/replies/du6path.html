	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/ptn_" target="_blank">ptn_</a>
			<div class="markdown"><p>what does 'entropy' refer to in this context?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/seruko" target="_blank">seruko</a>
			<div class="markdown"><p>non-deterministic change.<br />
When you're deep asleep or in a comma the brain is pretty much just running a sin wave. The medulla oblongata is just pumping the heart and moving the diaphragm in an out. Totally deterministic, very &quot;low entropy&quot;.  </p>
<p>But when you're awake and thinking all kinds of stimulus is happening auditory inputs/visual inputs/tactile inputs/vagus input/olfactory inputs/etc layered over with processing and post processing, and filtering mediated by memories, associations, and emotional reactions, along with the cacophony of different cogent actors all trying to rise to the level of conscious &quot;actions&quot; via 100 billion neurons synced over three main regions, broken up and coordinated across two qualitatively and physically distinct hemispheres. This system is not deterministic, or this system is &quot;high entropy.&quot;  </p>
<p>That's what they mean.  </p>
<p>edit: the above may not be clear call the first paragraph case 1 and the second paragraph case 2.<br />
In case 1 you could mathematically model the system with something on the mathematical complexity of f=sin. In the second you'd need to something about as complex as every computer running bitcoin in series just to model an example, and you still wouldn't get there because you'd need latency under 5ms between every processor to simulate consciousness.<br />
The difference in complexity is roughly equivalent to the difference in entropy.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/blandastronaut" target="_blank">blandastronaut</a>
			<div class="markdown"><p>Very interesting explanation! Thanks!</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Dunder_Chingis" target="_blank">Dunder_Chingis</a>
			<div class="markdown"><p>That's a weird way to use entropy. I always take entropy to refer to irreversible breakdown of anything into a undesirable or impractical state. </p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/thisrealhuman" target="_blank">thisrealhuman</a>
			<div class="markdown"><p>I'm replying to save for later, but my output from processing this is...
the human condition is to maximize personal internalized complexity to use entropy as a filter for finding consciousness in the shadow of chaos, to be used as a filter for finding humanity in the chaos of shadows? What does coffee do to Toxoplasma, I wonder?</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/WonkyTelescope" target="_blank">WonkyTelescope</a>
			<div class="markdown"><p>Both of the other responses are wrong.</p>
<p>Entropy is a count of states. It is the answer to the question &quot;how many ways can you arrange this system?&quot;</p>
<p>A system containing a single featureless particle that must be placed in one of two boxes has an entropy of ln(2) where ln is the natural logarithm.</p>
<p>A system consisting of only a deck of 52 cards can be arranged in 52! ways (52 factorial is ~10^65 ) so it has an entropy of ln(10^65 ).</p>
<p>A bucket of indistinguishable water molecules has huge entropy. That same bucket frozen has less entropy because the molecules have less freedom to find new arrangements.</p>
<p>A brain that is in a coma has little access to other arrangements. A brain that is seizing has access to too many useless states that don't actually produce useful physical activity. This is what the article is referring to.</p>
<p>Language also works this way. Low entropy language can only have a few states. So if we only used ABC we couldn't come up with many useful arrangements, if we used every letter in every arrangement we'd have mostly nonsense. It is only in the middle ground that we have useful language. The article postulates this is true for the brain (which seems obvious.)</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/GACGCCGTGATCGAC" target="_blank">GACGCCGTGATCGAC</a>
			<div class="markdown"><blockquote>
<p>The article postulates this is true for the brain (which seems obvious.)</p>
</blockquote>
<p>That is a fantastic explanation of entropy (applicable to any field using  entropy), but I want to point something out.  The fact that this <em>seems obvious</em> implies that the basic tenets proposed appear to be true.  Which means that entropy might be a good metric for intelligence.  It is entirely possible that the authors of the study found this to be false once tested.  </p>
<p>My point here is that many <em>abstract</em> ideas appear to be true or obvious once A) the argument is illuminated and B) the argument undergoes falsification by experimentation.  But empirically attempting to falsify these sound arguments routinely is extremely important, despite how obvious they might appear.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/ptn_" target="_blank">ptn_</a>
			<div class="markdown"><p>i know! i did physics in undergrad </p>
<p>i just didn't (some replies have made this make more sense to me) know what entropy meant in context of neuroscience/brain signals</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/pantaloonsofJUSTICE" target="_blank">pantaloonsofJUSTICE</a>
			<div class="markdown"><p>Your definition is immediately contradicting. If entropy is the number of ways a system can be arranged then your example with the particle and the box has the answer 2, not ln(2), which is not an integer, and so is not even coherent as a &quot;count&quot;.</p>
<p>If you mean to convey some idea about the information in a system, or something to do with efficiently expressing permutations/combinations, then I think you should respecify your definition.</p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/kittenTakeover" target="_blank">kittenTakeover</a>
			<div class="markdown"><blockquote>
<p>The authors of this paper suggest that by increasing BEN, caffeine increases complexity - i.e. before the caffeine the brain is below the optimal level of entropy.</p>
</blockquote>
<p>I don't see how the first sentence leads to the second. I thought you said there was an optimum amount of complexity. The fact that caffeine increases this does not indicate if you're moving towards the optimum or away from it. </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Moewron" target="_blank">Moewron</a>
			<div class="markdown"><p>Like any other drug, there's a dose-response curve. The effect one experiences from a drug goes from zero dose/zero effect to ideal dose/maximum positive effect, and then beyond. Think about how you respond to caffiene... unless you're Philip J. Fry, after a certain amount of coffee the negatives start to take over.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/kittenTakeover" target="_blank">kittenTakeover</a>
			<div class="markdown"><p>Yes, but often the optimum amount (maximum positive effect) on your dose-response curve is zero dose. Must-be-thursday said that before caffeine people are below the optimum level of entropy. How is that known? </p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/SamL214" target="_blank">SamL214</a>
			<div class="markdown"><p>Not to totally hijack this <em>TLC</em> , but this seems to loosely or more strongly tie into the psychology related to the <a href="https://en.wikipedia.org/wiki/Yerkes%E2%80%93Dodson_law" target="_blank">Yerkes-Dodson Law</a>. Well it ties in to more, but if we wanted to focus for a minute on disorders such as ADHD and General Anxiety Disorder, or Depression, we can see some use to the study. All  of these behavioral and mental disorders have motivational loss for varying reasons, but when treating them you can over activate or over depress the brain. What you want to manage is a good middle ground so that The brain is optimally aroused, thus <em>interested</em>. Without over stimulating the brain, which leads to anxiety. Too much anxiety or over activity in the brain inhibits a person from doing something. </p>
<p>Basically optimal but not maximal activity both in complexity and processes leads to beneficial performance. If it goes overboard inhibition due to anxiousness will present more often than optimal performance. Thus overall a person would be even less productive. </p></div>		</li>
					</ul>
	