	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/aquoad" target="_blank">aquoad</a>
			<div class="markdown"><p>Thank you, finally a detailed answer that doesn't gloss over the fact that there's a great deal of math that goes into deciding which bits are ones and which are zeroes. It's not just a bit-at-a-time threshold decision. </p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/SquidMcDoogle" target="_blank">SquidMcDoogle</a>
			<div class="markdown"><p>Thanks for sharing this!  Especially Github code examples.  Great read, much appreciated.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/servel333" target="_blank">servel333</a>
			<div class="markdown"><p>So, if our storage media was 100% reliable and we didn't need all the error correction, our storage would be 10x bigger?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/some_q" target="_blank">some_q</a>
			<div class="markdown"><p>No. A standard HDD has relatively little redundancy. For instance, a <a href="https://en.wikipedia.org/wiki/Hard_disk_drive#Error_rates_and_handling" target="_blank">1tb hard drive</a> only uses about 9% of its space for error correction.</p>
<p>However, cloud services will usually replicate your data to multiple hard drives in different data centers, so they may end up using 3x as much storage as they would if the technologies were perfectly reliable.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/ngutheil" target="_blank">ngutheil</a>
			<div class="markdown"><p>Would they be a lot faster since they don't have to error check?</p></div>		</li>
					</ul>
		</ul>
		</ul>
	