	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/is_this_4chon" target="_blank">is_this_4chon</a>
			<div class="markdown"><p>....like i'm a 5yr old. </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/IPlayEveryGame" target="_blank">IPlayEveryGame</a>
			<div class="markdown"><p>Imagine 4 pixels with 4 different colors close together in a grid. Now imagine you could (for whatever reason) see only one of those four pixels, because you don't have access to the information of the other pixels. This is the 1080p with 1080p source material situation. </p>
<p>Now imagine you can still just see one pixel, but you have information from the other 3 pixels and can create a kind of average between 4 of those pixels, which is displayed in the one pixel you can see. </p>
<p>Even if you can only see one pixel in the end, whatever color comes out of this average is a closer representation to what those 4 pixels look like together, than having no information about one of those pixels available at all. This is the 1080p with 4k source material situation. </p>
<p>Of course, being able to see all four pixels with 4 distinct colors would be the best option. Like 4k screen with 4k source material. </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Ignorred" target="_blank">Ignorred</a>
			<div class="markdown"><p>This is better.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Lixen" target="_blank">Lixen</a>
			<div class="markdown"><p>This doesn't satisfy as a response IMO.</p>
<p>It doesn't explain why the 1080p source material would not be an averaging of the same information as the 4K source to 1080p material.</p>
<p>Let's assume 2 cameras filming the same scene, one storing the material in 4K and one in 1080p.</p>
<ul>
<li>
<p>Camera 1 (4K): let's assume it averages information from 1mm^2 per pixel</p>
</li>
<li>Camera 2 (1080p): since it is filming the same scene, it averages information from 4mm^2 per pixel</li>
</ul>
<p>Downscaling 4 pixels to 1 from 4k to 1080p averages the information down to also 4mm^2 per pixel. Same end result. There is no gain in information by using this 4K as a source material. At least not when assuming a simple averaging of 4 to 1 pixels.</p>
<p>Edit: the actual answer is that regular 1080p recordings discard part of the data, which explains why 4K to 1080p conversion leads to more actual data per pixel than a direct 1080p recording.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/GigaRebyc" target="_blank">GigaRebyc</a>
			<div class="markdown"><p>The video in <a href="https://www.reddit.com/r/explainlikeimfive/comments/3zw478/eli5_how_can_a_4k_video_look_clearer_on_a_1080p/cyprmia" target="_blank">this comment</a> explained it very well, I thought. But given all the different answers we're seeing in this thread, I've no idea if it's true or not.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/gdq0" target="_blank">gdq0</a>
			<div class="markdown"><p>The truth is that it's both bitrate and 4:4:4 chroma subsampling.  More than likely a 2160p video (since it has a higher bitrate) will look better than a 1080p video.  If you encode/record a 1080p video with 4:4:4 it will likely be similar quality to a 2160p 4:2:0 video but smaller size.  If you use identical bitrates, the 1080p video should look better.</p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/CorgiButtSquish" target="_blank">CorgiButtSquish</a>
			<div class="markdown"><p>This is the right awnser. Games look better downsampled as well and there's no compression there</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Razord93" target="_blank">Razord93</a>
			<div class="markdown"><p>Downsampling in games only improve aliasing, since it already have 4:4:4/ rgb. 0-255 </p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/gdq0" target="_blank">gdq0</a>
			<div class="markdown"><p>Games being downsampled is a method of anti-aliasing called <del>oversampling</del> supersampling.  Doesn't really relate to 4:2:0 changing into 4:4:4.</p>
<p>EDIT: I couldn't remember super.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/IPlayEveryGame" target="_blank">IPlayEveryGame</a>
			<div class="markdown"><p>Supersampling is not oversampling. The nyquist frequency in question is not the frequency of pixels on the screen, but the frequency of the underlying signal (the world you're trying to render). Unfortunately, the frequency of that signal is infinity in the general case,  hence it's not possible to oversample it, and you can not perfectly antialias it. There will always be aliasing, you can just make it small enough that the eye can't see it.</p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/NodeMasterPro" target="_blank">NodeMasterPro</a>
			<div class="markdown"><p>This. Chroma subsampling throws away color information. 4K cancels out the loss, effectively giving you 100% color information at 1080p.</p>
<p>I still don't believe it's possible to see a difference between 4K vs 1080p (talking resolution only) at normal viewing distances, except the color information gained at 4K does result in a better overall picture.</p></div>		</li>
					</ul>
	