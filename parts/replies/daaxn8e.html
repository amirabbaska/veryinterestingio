	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/JerryKaplanOfficial" target="_blank">JerryKaplanOfficial</a>
			<div class="markdown"><p>Well it looks like some other folks have ben answering my questions. :)  I agree with Cranyx on this one ... the 'safety' concerns about runaway intelligence are based on watching too many movies, not on any meaningful scientific evidence. I suggest ignoring these inflammatory statements!</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/nairebis" target="_blank">nairebis</a>
			<div class="markdown"><p>With respect, this answer is provably ridiculous.</p>
<p>1) Electronics are approximately 1 million times faster at switching than chemical neurons.<br />
2) Human intelligence is based on neurons.<br />
3) Therefore, it's obviously possible to have a brain with <em>human-level intelligence</em> that is <em>one million times faster</em> than humans if you implement silicon neurons.</p>
<p>We can argue about practicality, but it's obviously possible. The implications of that are terrifying. AI doesn't have to be more intelligent than us, just faster. If our known upper intelligence bound is Einstein or Newton, an AI one million times faster can do one year of Einstein-level thinking every 31 seconds. A human adult lifetime of thinking (60 years) every 30 minutes.</p>
<p>Now imagine we really go crazy and mass produce the damn things. Thousands of Einstein brains one million times faster. Or how about a million of them?</p>
<p>This is provably possible, we just don't understand the human brain. Yet. But once we do, implementing neurons in silicon will be a straightforward step, and then it's all over.</p>
<p>You can argue that we're far away from that point, and that's obviously true. But the essence of the question is the future, and the future of AI is absolutely a huge problem.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/ericGraves" target="_blank">ericGraves</a>
			<div class="markdown"><p>So why is his answer provably ridiculous? All you said was &quot;it is possible.&quot; Which, yeah sure, it is possible. As of right now though, there is nothing to suggest we ever will figure out how to implement. </p>
<p>You are making a very strong assumption that we will eventually &quot;figure it out.&quot; The debating of the validity of that assumption would be asinine. You would point to humans always learning, and probably growth in the area of AI. These I would discount by pointing out that we have made considerable progress in mathematics, but problems like that collatz conjecture are still unsolved.</p>
<p>This is an expert in the field, considering your argument hinges on a single assumption, I believe you would need stronger evidence than what is provided.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/hgoel0974" target="_blank">hgoel0974</a>
			<div class="markdown"><p>The idea that one can somehow compare neurons to electronics is ludicrous at best. A neuron's activation involves lots of factors (ion gradients between membranes etc), and is inherently not binary, thus switching speed has very little meaning. Sure, it's terrifying to think about a machine that makes human's obsolete, but that's an existential problem relating to our instinctual belief that there's something inherently special about us. </p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/dblmjr_loser" target="_blank">dblmjr_loser</a>
			<div class="markdown"><p>It's not obviously possible to build an electronic brain. We have no idea how to accurately model a single neuron. </p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/NEED_A_JACKET" target="_blank">NEED_A_JACKET</a>
			<div class="markdown"><p>I think that attitude is literally going to cause the end of the world. If there were no films dramatizing it, it would probably be a much bigger concern. The fact that we can compare people's concerns to Terminator makes it very easy to dismiss them as being purely fictional. You're a sci-fi nut if you think an idea for a film could be reality.</p>
<p>We're not talking about skeleton robots that try to shoot us with guns, consider though, an AI with the logical (not necessarily emotional) intelligence of a human. It's attainable and will happen unless there's a huge disaster that stops us continuing to create AI. </p>
<p><strong>Ignoring AI potentially going rogue</strong> for now, which is a very reasonable possibility, imagine this human-level intelligent robot is in the hands of another government or terrorists or anyone wanting to cause some disruption. You could cause a hell of a lot of commotion if you allowed this AI to learn 100 years worth of hacking (imagine a human of average intelligence dedicated their life to learning hacking techniques). I hear this would take a very small amount of time due to the computing speed. This AI could now be used to literally hack practically anything that currently exists. Security experts say nothing is foolproof, and that's probably true for 99% of cases. Give someone (or an AI) 100 (or 10,000) years of experience and they would bypass most security systems. Sure, maybe it can't launch nukes, but it could do as much disruption as any hacking group, but millions of times over in a millionth of the time. </p>
<ul>
<li>
<p>If you think &quot;hacking&quot; AI is outside the reach of AI then you should take a look at automated tools already, and imagine if the team behind Deep Mind applied their work to it. I bet it's not long before they work on &quot;ethical hacking&quot; tools for security if they don't already.</p>
</li>
<li>If you don't think anyone would use this maliciously when it becomes widely available, that would be very naive. It would be as big of a threat as nuclear war, so if one government had this capability, everyone would be working towards it.</li>
</ul>
<p>You mentioned a lack of meaningful scientific evidence. I would say that's going to be the case for any upcoming problems that don't currently exist, but logically we can figure out that anything that can be used maliciously probably will be. Take a look at current &quot;hacking AI&quot; (this is just to stick with the above example). It exists and there's no reason to think it wont get significantly better as AI takes off. Is this not small scale evidence of the problem?</p>
<p>Also I strongly believe AI, even with the best of intentions, would go full skynet if it achieved even just human level intelligence (ignoring the superintelligence which would come shortly after). You'd need some extremely strong measures to prevent or to ensure that a smart AI wouldn't be dangerous (I think it would actually be impossible to ensure it without the use of an existing superintelligence), which <em>may</em> be fine if there was just one person or company creating one AI. But when it's so open that anyone with a computer or laptop can create it, no amount of regulation or rules is going to prevent every single possible threat from slipping through the net.</p>
<p>It would only take <em>one</em> AI that has the goal of learning, or the goal of existing, or the goal of reproducing, for it to have goals that don't align with ours. If gaining knowledge is the priority then it would do this at the cost of any confidentiality or security. Any average intelligence human could figure out that in order for them to gain knowledge they need access to as much information as they can get, which brings it back to hacking. Unless every single AI in existence is created with up-to-date laws for every country about what information it is and isn't allowed to access there would be a problem. If it doesn't distinguish whether it is accessing the local library, or confidential government project information, any AI with the intent of gaining knowledge would eventually take the path of &quot;hacking&quot; to access the harder-to-reach information.</p>
<p>Note: This is just one &quot;problem area&quot; relating to security/hacking. There are surely plenty more, but I think this would be the most immediate threat because it's entirely non-physical, but proven to be extremely disruptive.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Kuba_Khan" target="_blank">Kuba_Khan</a>
			<div class="markdown"><p>The fact you keep making comparisons between human intelligence and &quot;machine intelligence&quot; tells me that you aren't an expert within this field.</p>
<p>It's posts like these that make me hate pop-science. Machine learning isn't learning; it's just a convenient brand. Machines aren't smart, they rely entirely on humans to guide their objectives and &quot;learning&quot;. A more apt name would be applied statistics.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Osskyw2" target="_blank">Osskyw2</a>
			<div class="markdown"><blockquote>
<p>It's attainable and will happen </p>
</blockquote>
<p>Will it? Why would you develop it? What's the purpose of such a general AI? Why would you give it power and/or access?</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/randompermutation" target="_blank">randompermutation</a>
			<div class="markdown"><p>There is another angle like the 'skynet' question below. While AI itself doesnt pose a threat, there are systems which use AI to identify threats. Human finally decide on it but I wonder if humans make a mistake.  </p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/nickrenfo2" target="_blank">nickrenfo2</a>
			<div class="markdown"><p>The danger of AI will inevitably be presented by humans more than anything. I don't think we'll run into the whole &quot;skynet&quot; issue unless we're stupid enough to create an intelligence with nuclear launch codes, and the intelligence is designed to make decisions on when and where to fire. So basically, unless we get drunk enough to shoot ourselves in the foot. Or the head.</p>
<p>In reality, these intelligence programs only improve their ability to do what they were trained to do. Whether that's play a game of Go, or learn to read lips, or determine whether a given handwritten number is a 6 or an 8, the intelligence will only ever do that, and will only ever improve itself in that specific task. So I see the danger to humans from AI will only ever be presented by other humans. </p>
<p>Think guns - they don't shoot by themselves. A gun can sit on a table for a hundred years and not harm even a fly, but as soon as another human picks that gun up, you're at their mercy.  </p>
<p>An example of what I mean by that would be like the government (or anyone else, really) using AI trained in lip reading to basically relay everything I say to another party, thus invading my rights to privacy (in the case of government), or giving them untold bounds of information to target me with advertising (in the case of something like Google or Amazon or another third party).</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Triabolical_" target="_blank">Triabolical_</a>
			<div class="markdown"><p>Relevant &quot;Wait But Why&quot; Posts  <a href="http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html" target="_blank">1</a>     <a href="http://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html" target="_blank">2</a></p>
<p>TL;DR; I hate to try to summarize because you should read the whole thing, but the short story is that if we build an AI that can increase its own intelligence, it's not stopping at &quot;4th grader&quot; or &quot;adult human&quot; or even &quot;Einstein&quot;, it's going to keep going. </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/NotTooDeep" target="_blank">NotTooDeep</a>
			<div class="markdown"><p>Question: can you give AI a desire? </p>
<p>I get that figuring shit out is a cool and smart thing, but that didn't really cause us much grief in the last 10,000 years or so. </p>
<p>Our grief came from desiring what someone else had and trying to take it from them.</p>
<p>If AI can just grow its intelligence ad infinitum, why would it ever leave the closet in which it runs? Where would this desire or ambition come from? Has someone created a mathematical model that can represent the development of a desire? </p>
<p>It seems that for a calculator to develop feelings and desires, there would have to be a mathematical model for these characteristics.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/nickrenfo2" target="_blank">nickrenfo2</a>
			<div class="markdown"><p>Of course. But that doesn't make it dangerous. Just because it's able to learn doesn't mean it has access to launch codes. It's ability to learn and act is limited by the tools it has. If you give it a &quot;mouth&quot; and &quot;vocal chords&quot; it will be able to speak, take those things away and it can no longer even use words to hurt you. Give it access to the internet and the ability to learn how to break internet security, then you can bet your ass it might possibly cause some sort of global war. No matter how smart it is, it cannot see without eyes.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/shazammerbammer" target="_blank">shazammerbammer</a>
			<div class="markdown"><p>I'm really not clear what people think a 'smarter, more intelligent' AI would be. Is it just able to see that a tree is a tree <em>that</em> much better than a person can? Does it win at chess on the first move? Can it make a sandwich out of a shoelace? </p>
<p>Since we don't have an examples of anything smarter than ourselves, it would be hard to know. </p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/darwin2500" target="_blank">darwin2500</a>
			<div class="markdown"><p>The relevant thought experiment is the 'Paperclip Maximizer GAI'.</p>
<p>Lets say we invent real general artificial intelligence - ie, something that's like a human in terms of the ability to genuinely problem solve.  Let's say the CEO of Staples has a really simple, great business idea - put the GAI in a big warehouse with a bunch of raw materials, give it some tools to work with and the ability to alter it's own code so it can learn to work more efficiently, and tell it 'make as many paperclips as you can, as quickly as possible.'</p>
<p>If it's true that a GIA that is as smart as a human can change it's code to make itself smarter, and repeat this process iteratively...</p>
<p>And that it has enough tools and raw materials to make better tools and better brains for itself...</p>
<p>Then there's a very real chance that 5000 years later, the entire atomic mass of the solar system will have been entirely converted into paperclips, with an ever expanding cloud of paperclip-makers leaving the system at near-light speeds, intent on converting the rest of the mass of the universe ASAP.</p>
<p>The threat from AI is not that it will turn 'evil' like some type of movie villian.  That's dumb.</p>
<p>The threat is that it may become an arbitrarily powerful tool that is extremely easy for anyone to implement and entirely impossible for anyone to predict the full consequences of. </p>
<p>Another classic example: If you just tell the GAI 'make people happy', and it's metric for telling whether someone is happy is whether it's smiling or not, it may give everyone on the planet surgery so they are only able to smile... or it may tile the universe with microscopic drawings of smiley faces.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Jah_Ith_Ber" target="_blank">Jah_Ith_Ber</a>
			<div class="markdown"><p>Nobody is interested in creating AIs that learn to do [blank] really well. What people are trying to do is create an artificial human.</p></div>		</li>
					</ul>
		</ul>
	