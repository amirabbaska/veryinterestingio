	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/ArkGuardian" target="_blank">ArkGuardian</a>
			<div class="markdown"><p>/u/scienceistoohard is describing CNNs or convolutional neural networks here which are important in image recognition and I think one of the easiest ways for laymen to understand neural nets. You can also do much cooler things like Generative Adversarial Neural Networks. </p>
<p>In general, Machine Learning is split in 2 branches - supervised (with labels) and unsupervised(without labels). </p>
<p>*Supervised would be stuff like random forests, K-nearest neighbors, which are used to predict if someone has cancer based on all these qualities of other people who had cancer.</p>
<p>*Unsupervised is stuff like K-means, PCA which are used to identify hidden clusters of customer segmentation. Unsupervised is mainly used for dimensional reduction while supervised is used for prediction.</p>
<p>The problem with supervised is that we have to know what we want from our end result in the first place. Generative Adversarial Neural Networks let us work with something called wither semi-unsupervised or artificially supervised. This let's us use a first neural  network to basically take random inputs and figure out good things to &quot;suggest&quot; to the second neural network which trains normally. This let's us do predictive analytics starting off from a position in which we actually don't understand what we are looking for. </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/giziti" target="_blank">giziti</a>
			<div class="markdown"><blockquote>
<p>*Unsupervised is stuff like K-means, PCA which are used to identify hidden clusters of customer segmentation. Unsupervised is mainly used for dimensional reduction while supervised is used for prediction.</p>
</blockquote>
<p>While PCA is certainly useful for dimensional reduction, K-means (which is a particular type of clustering) is not really useful for dimensional reduction. The clustering problem is, in general, this: you have observations of some data generating process and you want to find groups of observations that are &quot;similar&quot; to each other, where similarity is defined by whatever metric you like. You may or may not know how many groups there are. It's supervised if you know the labels, unsupervised if you don't, semi-supervised if you know some of the group labels. K-means is a solution to the problem if you know the number of classes, use Euclidean distance, and have spherical homogeneous clusters.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Frozaken" target="_blank">Frozaken</a>
			<div class="markdown"><p>Wouldn't something like SVM's be able to do this aswell?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/scienceistoohard" target="_blank">scienceistoohard</a>
			<div class="markdown"><p>For classification they certainly can, they're just not as good at it. SVM's are linear classifiers, so they have all the same limitations as a linear classifier; if your data isn't at least approximately linearly separable, then you won't get good results.</p>
<p>You can make SVM's nonlinear by using kernels but, as I mentioned, that just replaces one hard problem with another hard problem. The trouble with kernels is that they are problem-dependent: for every new problem, you need a new kernel, and there isn't necessarily a systematic way of finding good kernels. The nice thing about neural networks is that you don't necessarily have to invent a whole new algorithm every time you have a new problem.</p>
<p>It's worth noting that neural networks are more versatile than just classification; you can use them for approximating any high-dimensional function, for any purpose (although it's not necessarily the best method for all applications). This is used in reinforcement learning, for example, where neural networks can be used to approximate the &quot;value function&quot; of an algorithm.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/CrushHazard" target="_blank">CrushHazard</a>
			<div class="markdown"><p>I'm using SVMs right now for a classification problem with good results. I wouldn't mind better results, but I'm having trouble figuring out how to apply a neural net to a classification problem. Is there a specific type that translates from SVMs well?</p>
<p>Also, how do NNs avoid overfitting?</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/danby" target="_blank">danby</a>
			<div class="markdown"><p>Great answer.</p>
<p>I would say though that in our group's experience where we have a well formed (classification) problem with highly reliable positive and negative training data we seldom see much difference between SVMs and NNs.</p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Exp_ixpix2xfxt" target="_blank">Exp_ixpix2xfxt</a>
			<div class="markdown"><p>As many advantages as NNs have, they lack in being understandable via their structure (except in the case of some that do implicit image processing) and are thus not so great at creating understanding of new phenomena. </p>
<p>Great explanation </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/CrushHazard" target="_blank">CrushHazard</a>
			<div class="markdown"><p>What about overfitting?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Exp_ixpix2xfxt" target="_blank">Exp_ixpix2xfxt</a>
			<div class="markdown"><p>That's not limited to NNs though</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/MuonManLaserJab" target="_blank">MuonManLaserJab</a>
			<div class="markdown"><p>Is it possible that we just don't yet know how to analyze them well?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/stathibus" target="_blank">stathibus</a>
			<div class="markdown"><p>A common criticism of NNs is that the programmers know that they work but the theorists aren't good at explaining why yet.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/scienceistoohard" target="_blank">scienceistoohard</a>
			<div class="markdown"><p>I think they're a lot less inexplicable than most people believe. Part of the problem, I think, is that most of the results for neural networks - and most of the things that people learn about them - are basically experimental results. Physics or chemistry would seem very mysterious too if you only learned about experiments, and never theory.  Granted, there isn't as much theory about neural networks as there ought to be, but that's slowly changing for the better.</p>
<p>For example, the fact alone that neural networks are basically just function compositions tells you why they tend to do better at approximation for certain things than using linear algebra basis functions: with a linear basis of size <em>n</em>, you can only get a polynomial of degree <em>n</em>, whereas with <em>n</em> composed polynomials of degree <em>k</em>, you can get a polynomial of degree <em>k</em> ^ <em>n</em>. It's not surprising, then, that you can more accurately approximate certain functions with compositions of functions than you can with sums of functions.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Exp_ixpix2xfxt" target="_blank">Exp_ixpix2xfxt</a>
			<div class="markdown"><p>Sure, but all of the niceties of PAC Learning are basically inapplicable. Comparatively, we have no theory that covers NNs as well as we do for other discriminative learning strategies. </p></div>		</li>
					</ul>
		</ul>
		</ul>
	