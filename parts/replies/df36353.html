	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/ImSpartacus811" target="_blank">ImSpartacus811</a>
			<div class="markdown"><p>So much this. </p>
<p>And the biggest thing to understand is that it's not about frame rate, but <em>frame time</em>. </p>
<p>A 24 fps movie has constant 42 ms frame times. Every frame is shown for about 42 ms, then the next is shown. </p>
<p>A video game has frame times that vary, often significantly. It's possible to have an average fps of, say, 30 fps, in a given one second (1000 ms) interval and still have one really long 100 ms frame next to a couple short 15ms frames. </p>
<p>You will notice the 100 ms long frame. It looks like a stutter. And it's actually harmful when you need to react to stuff quickly. </p>
<p>Tech Report did a famous article on this phenomena that changed how video games are benchmarked. It's a fantastic read. </p>
<p><a href="http://techreport.com/review/21516/inside-the-second-a-new-look-at-game-benchmarking" target="_blank">http://techreport.com/review/21516/inside-the-second-a-new-look-at-game-benchmarking</a></p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/gameboy17" target="_blank">gameboy17</a>
			<div class="markdown"><p>So instead of frames per second, we should look at seconds per frame?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/ImSpartacus811" target="_blank">ImSpartacus811</a>
			<div class="markdown"><p>I think it's more complicated than that. It's about moving away from a an overly simplistic single &quot;average&quot; figure (whether you calculate it as FPS or &quot;SPF&quot;) and recognizing that there is significant variance from that average. </p>
<p>The classic example from that Tech Report article is Crossfire (SLI was still bad, but CF was <em>really</em> bad): </p>
<ul>
<li>
<p>A <a href="http://techreport.com/r.x/inside-the-second/bc2-6970.gif" target="_blank">single 6970</a> was pretty consistent in its frame times. </p>
</li>
<li>But two of them in Crossfire? <a href="http://techreport.com/r.x/inside-the-second/bc2-6870cfx.gif" target="_blank">Terrible</a>. Almost constant jitter. </li>
</ul>
<p>The scary thing is that the Crossfire setup has a much higher average FPS. So if you're just looking at the average FPS, then Crossfire appears a ton better. But it's an absolutely horrific experience in practice due to awful jitter. </p>
<p>That Tech Report analysis caused Crossfire (and SLI) to get significant improvements to mitigate jitter. Multi-GPU setups still have more jitter than single-GPU setups, but it's not quite as awful. AMD actually hired the author of that article because his perspective was so valuable. </p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Fairchild22" target="_blank">Fairchild22</a>
			<div class="markdown"><p>Interesting!! It's almost as though the perceived fps in that situation is more like 10fps because the 100ms frame too our attention.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/ImSpartacus811" target="_blank">ImSpartacus811</a>
			<div class="markdown"><p>Absolutely. That is exactly the takeaway of why it's so flawed to even use the term &quot;fps&quot; with respect to gaming. </p>
<p>The variance of frame times is pretty large. For example in <a href="http://techreport.com/r.x/2017_03_08_Nvidia_s_GeForce_GTX_1080_Ti_graphics_card_reviewed/Watch_Dogs_2_Percentile.png" target="_blank">4K Watch Dogs 2</a> (<a href="http://techreport.com/review/31562/nvidia-geforce-gtx-1080-ti-graphics-card-reviewed/4" target="_blank">from this review</a>), a 1080 Ti will vary from roughly 17 ms (i.e. 59 fps equivalent)  to 35 ms (i.e. 29 fps equivalent) while the &quot;average fps&quot; is an overly simplistic 54 fps. </p>
<p>That's not digging on the 1080 Ti, it's just an example of the reality that all gaming has to deal with. </p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/yrrosimyarin" target="_blank">yrrosimyarin</a>
			<div class="markdown"><p>As someone who has a gamed on many low end systems, I think this is right.</p>
<p>It really depends on the type of game. A turn based game will feel fine at a smooth 24 fps. A shooter that requires constant user inputs will be useless.</p>
<p>It also feels very different to be CPU or GPU limited in a game. When you are GPU limited your inputs are processed on time, but the feedback of the results are delayed. When CPU bound, you can feel that both slow.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/zdakat" target="_blank">zdakat</a>
			<div class="markdown"><p>can confirm- used to play a popular mmo/arena game and thought I was doing fine(maybe just a little underskilled,but I'm no pro anyway...haha) with lots of stuff turned off. got a new computer and was blown away by the improvement in gameplay when the fps was higher</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/LowenNa" target="_blank">LowenNa</a>
			<div class="markdown"><p>Scabbing on to this because it is correct and the top post.</p>
<p>Note that I use Hz (hertz) and FPS (frames per second) interchangeably in this post.  And we are assuming that any frame rate discussed is consistent.</p>
<p>Go to <a href="https://www.testufo.com/#test=framerates&count=3&background=none&pps=960" target="_blank">https://www.testufo.com/#test=framerates&count=3&background=none&pps=960</a>  to see a real time demo of various refresh rates (please note that examples it give will be based on your display refresh rate.  It defaults to one demo at your refresh rate, one at half, and one at 1/4.)</p>
<p>Assuming that you have a 60hz display, you should have examples of 60 fps, 30, and 15.  You should be able to see a difference between all of them.  60 is pretty smooth. 30 is ok, but a little jittery.  15 is really jittery.  Assume that 24fps is slightly worse than 30.</p>
<p>As Henx125 said in his above post, video games are very dependant on player reaction time and accuracy.  Imagine 2 players with the same skill levels in a first person shooter game but one is playing at 30fps and the other at 60.  The 60fps player is an advantage over the 30 fps player because the movement is smoother, clearer, and faster.</p>
<p>One step farther, the higher the frames per second, the  more up to date the info on the screen is.  Henx125 also touched on this in his post, but let's look at some numbers.</p>
<p>At 24hz, the information on screen is updated once every 41ms.</p>
<p>At 30hz, the information on screen is updated once every 33ms.</p>
<p>At 60hz, the information on screen is updated once every 16ms.</p>
<p>Many PC gamers use 144hz monitors.  They would update every 7ms</p>
<p>As you can see, the higher the frame rate, the lower the latency.  The lower the latency, the better the players response time can be (and the more smooth and fluid the game will feel).  But you can also see some diminishing returns.  The difference in latency between 24hz or 30hz and 60 is huge.  The difference between 60 and 144 is not as huge.  For that reason, many people consider 60 FPS to be the base line for good smooth game play.</p>
<p>To bring it back to what other posters have said about 24fps movies having motion blur which makes it look smoother... You don't want blur in most video games.  You want clarity and smoothness.  If your First person shooter had enough motion blur to make 24fps look decent, you would have a very hard time discerning targets when aiming.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Ianniedz" target="_blank">Ianniedz</a>
			<div class="markdown"><p>Tibikl</p></div>		</li>
					</ul>
		</ul>
	