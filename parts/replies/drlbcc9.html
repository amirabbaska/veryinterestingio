	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/barnacledoor" target="_blank">barnacledoor</a>
			<div class="markdown"><p>I had heard stuff like this, but I wonder how this applies when you are doing activities that require more flexibility and range of motion.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/RockaiE" target="_blank">RockaiE</a>
			<div class="markdown"><p>I am curious as well. I don't have access to that full article, but it does not appear that the study differentiated activities and thier injury rates, but rather analyzed all excercise as a whole.</p>
<p>It's not too far fetched to think that &quot;Extreme&quot; sports may benefit in some way.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/CheapClimber" target="_blank">CheapClimber</a>
			<div class="markdown"><p>Can anyone who is more knowledgeable about studies explain to me why they would grab multiple studies and make the conclusion they did in this article? It seems like they just found some studies that agreed and some that didn't and concluded that stretching didn't have benefits. It doesn't seem very logical to do that as each study would have had different variables. Also is there a way to find all the studies they were comparing i can't seem to find them in the linked article.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/fooliam" target="_blank">fooliam</a>
			<div class="markdown"><p>It's called a &quot;meta-review&quot; or &quot;systematic review&quot;, depending on what exactly you're doing.  Studies are selected on a range of criteria, such as minimum n size, length of intervention, how the results were reported (ie was just a p level reported, or effect sizes and p level, things like that), when the study was published, study design (cross-sectional vs longitudinal vs intervention), etc.  As with all research, they vary in quality, and one of the biggest areas of variance is the specificity and thoroughness in selecting which data to include in the review.  A well-done review will be very specific in what criteria an article must meet to be included in the review.  </p>
<p>A systematic review will then go through all the papers and summarize them, basically saying &quot;SO all these papers say W, X, and Y, which is evidence to support conclusion Z.  Papers A and B support Conclusion C, but the evidence for Conclusion C isn't as good because of Reasons D and E.&quot;  It's the &quot;grown-up&quot; version of a report you do for your college classes.</p>
<p>A meta-review does sort of the same thing, but uses statistical methods to analyze the reported results in various papers to then quantify what the sum of evidence suggests is true.  It's just like doing a statistical analysis in any other study, but the data points become the results of each study, instead of data gathered in each study, if that makes sense.  </p>
<p>Both of these methods serve the purpose of summarizing existing research, which can be very important when there is A) a lot of data about a topic B) The results of all the data aren't immediately clear. </p>
<p>For instance, no one is realistically going to read all the hundreds or thousands of research articles that have been published on stretching and injury.  It takes too long, people have better things to do.  Except for the person doing a meta-review or systematic analysis.  They will come up with selection criteria for what they think makes a &quot;good&quot; study on that topic, find every single article they can on the topic, filter out those that don't meet the selection criteria, then summarize the results of those that do.  This allows someone to get the &quot;big-picture&quot; take away on a topic without spending the months/years necessary to track down and read all those papers themselves.</p>
<p>It's a standard, accepted, and useful methodology in pretty much every field.  </p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/RockaiE" target="_blank">RockaiE</a>
			<div class="markdown"><p>The assumption is that &quot;more data = better understanding of the results&quot;. If you read only one of the articles that said &quot;under these circumstances, stretching helps reduce injury&quot;, it is easy to overlook the importance of the &quot;under these circumstances&quot; part.  Within that statement can hold a lot of assumptions that may or may not affect the exercise we do outside of the controlled lab environment (assuming it was researched in a lab). Ideally, the different studies would assume different circumstances in order to catch as many variables as possible.  Since some studies obviously disagree with the others, a larger comprehensive study of these smaller studies may be done to find out why these smaller studies don't agree. </p>
<p>I could only find the abstract of the article, but assuming this larger study was rigorously scientific, I imagine the full article may have the answers to those specifics.</p>
<p>Edit:  You typically need to have a subscription to the journal to read the entire article. Some universities or research companies may have access that allows you to read it, but unfortunately I do not. Any proper article will have those specific studies cited.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/CheapClimber" target="_blank">CheapClimber</a>
			<div class="markdown"><p>This means we would have to assume that each study was done correctly?  I've seen a few studies that had obvious faults and so i have a hard time trusting a study without taking a look at it. And some of the studies showed positive effects and some showed negative effects so i would conclude something wasn't being factored and not that they cancel each other out so stretching has no effect.</p></div>		</li>
					</ul>
		</ul>
		</ul>
	