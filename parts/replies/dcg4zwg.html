	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/QtPlatypus" target="_blank">QtPlatypus</a>
			<div class="markdown"><p>Doesn't really answer the question though which asked how entropy in the sense of shallon compares to entropy in the sense physics uses it</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/brownck" target="_blank">brownck</a>
			<div class="markdown"><p>Ahh, but I think it does ( I think, at least). Here's my reasoning. The number of possible arrangements is correlated (or maybe it exactly is) given by the shannon information formula. For example, if you were to flip a coin 100 times, a fair coin gives us the maximal possible ways to arrange those 100 coin flips. Why? Consider the extremes. If the probability of heads was 0, there is only one arrangement of 100 coin flips - zero. Likewise, if the prob of tails was zero, the only arrangement of 100 coins is zero as well. But if the prob of heads is .5, the number of ways this can happen is maximized. There is, of course, more mathematical reasoning behind this involving the gibbs measure (ref?).  P.S. this is why the uniform distribution is typically used as a prior because it maximizes entropy (distribution with the most possible states). </p></div>		</li>
					</ul>
		</ul>
	