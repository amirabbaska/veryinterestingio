	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Keleris" target="_blank">Keleris</a>
			<div class="markdown"><p>Some people might be getting confused because they don’t understand null hypotheses. As you said to someone else, this does not have to be the null, it can be whatever you want. However, researchers usually use the p-value when rejecting the null, meaning that a value of 0.01 is good (because that means what was observed was not very compatible with the null hypothesis). That’s why you might want low p-values, even though it means something quite different than most people assume.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/HorseWizard" target="_blank">HorseWizard</a>
			<div class="markdown"><p>Thankyou for your reply. So that would mean that there is a 1% chance of there being a sampling error ?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Rather_Dashing" target="_blank">Rather_Dashing</a>
			<div class="markdown"><p>No, its doesn't tell you anything directly about sampling error. All it tells you is that under the null hypothesis (fair coin, no difference due to treatment, etc) that there is a 1% chance of you getting the data you got (or something more extreme).  It does not directly tell you anything about your alternate hypothesis: so it does not tell you that there is a 1% chance of there being sampling error, or 1% chance that your alternate hypothesis is wrong, or 99% chance that your results will replicate, or any of these other common misconceptions. It simply tells you the likelihood of getting your data if the null hypothesis is true.</p>
<p>If you want to read more I think this Nature News article does a decent job of describing the p value and some of the misconceptions around it:</p>
<p><a href="http://www.nature.com/news/scientific-method-statistical-errors-1.14700" target="_blank">http://www.nature.com/news/scientific-method-statistical-errors-1.14700</a></p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/7LeagueBoots" target="_blank">7LeagueBoots</a>
			<div class="markdown"><p>Reminds me of the Micceri 1989 paper, <a href="http://psycnet.apa.org/record/1989-14214-001" target="_blank">The Unicorn The Normal Curve and Other Improbable Creatures</a></p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/AreYouForSale" target="_blank">AreYouForSale</a>
			<div class="markdown"><p>It means that there is 1% chance of getting this data through chance alone.</p>
<p>It's a measure of how skewed your results are.</p>
<p>-Hypothesis: this coin has two tails.
(Null Hypothesis: no, it's just a usual coin)</p>
<p>-Proceeds to throw coin a bunch of times.</p>
<p>-p value tells you the probability of getting the same result if the coin had heads and tails (the Null Hypothesis).</p>
<p>There is no sampling error here, you did everything right. But no matter how much you throw, there will remain a small chance of the coin having two sides after all. Science! The only thing you really know is that you know nothing.</p>
<p>Note: this example is a bit awkward, as usually p values are used to measure correlation. Works the same exact way.</p>
<p>-Null Hypothesis: there is no correlation between the two coins.
(Hypothesis: these are magical entangled coins)</p>
<p>-Flip the two coins n times</p>
<p>-Calculate correlation</p>
<p>-p value gives you the probability of getting this level of correlation given that these are just ordinary coins</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/JingoKhanDetective" target="_blank">JingoKhanDetective</a>
			<div class="markdown"><p>THanks for this. Everything that confused me about stats just fell into place.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/ATAD8E80" target="_blank">ATAD8E80</a>
			<div class="markdown"><p>Minor point: doesn't &quot;sampling error&quot; just refer to (or at least include) deviation of the sample statistic (e.g., mean) from population parameter by virtue of randomness?</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/DrunkFishBreatheAir" target="_blank">DrunkFishBreatheAir</a>
			<div class="markdown"><p>It means that your results imply a low chance of your null hypothesis being true.</p>
<p>Sampling error isn't super relevant.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/phonz1851" target="_blank">phonz1851</a>
			<div class="markdown"><p>No! As a statistician this is not true! We assume the null hypothesis to be true, so we cannot find the probability of the null hypothesis. In order to find that probability you need to do a Bayesian analysis which is a fair bit more complicated and requires a different set of assumptions. In frequentist statistics you are using a likelihood which is the data given your parameters. Therefore you can only make statements about your data.</p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/imaginary_num6er" target="_blank">imaginary_num6er</a>
			<div class="markdown"><p>Wouldn't it depend on Type-I &amp; Type-II error and also Power though?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/RobusEtCeleritas" target="_blank">RobusEtCeleritas</a>
			<div class="markdown"><p>I’m not sure what you mean, “depend on them” how?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/imaginary_num6er" target="_blank">imaginary_num6er</a>
			<div class="markdown"><p>Well, if you have a p-value of 0.01 with a low sample size, you have a 1% chance of observing a similar result. However, the conclusion might be due to a false hit (Type-I error), which is the inverse probability of the p-value.</p>
<p>Type-II error and power becomes relevant when you get a p-value of 0.01 with a low sample size. The probability that your study was a false miss is reduced with more data points, but is separate from the p-value. You can get a low p-value by a study with n=3, but it's going to have such low power that no one is going to take you seriously. </p></div>		</li>
					</ul>
		</ul>
		</ul>
	