	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/infected_funghi" target="_blank">infected_funghi</a>
			<div class="markdown"><p>Fun fact: time even isnt the same in orbit  because of relativity theory. The very first satellites used to have problems syncing time with earth because they underestimated the effect of the difference of gravity on time/space. They drifted off by a few ns per day. So you have to adjust your clock even to the gravitational pull </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/lookxdontxtouch" target="_blank">lookxdontxtouch</a>
			<div class="markdown"><p>It's not gravity that changes the times I the clocks of satellites. Gravity is essentially the exact same at low earth orbit as it is on the ground. The reason the satellites clocks are slightly slower over time is because they are traveling so much faster than the computers on the ground.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/millijuna" target="_blank">millijuna</a>
			<div class="markdown"><p>Actually, both effects play a role. </p>
<p>The best example is with the GPS satellites. Special relativity (which causes time changes due to velocity) would cause the onboard clocks to tick about 7 microseconds slower per day. On the other hand, because they are far enough out of our gravity well, General relativity says they should gain about 45 microseconds per day.</p>
<p>Thus, the net relativistic effect on the clocks onboard the satellites is about 38 microseconds, and that is indeed what they see in practice.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/helpinghat" target="_blank">helpinghat</a>
			<div class="markdown"><blockquote>
<p>because they underestimated the effect of the difference of gravity on time/space</p>
</blockquote>
<p>They had to estimate? Were the exact physical equations not known at the time of the first satellites? </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/infected_funghi" target="_blank">infected_funghi</a>
			<div class="markdown"><p>Im no professional in the topic but some research:</p>
<p>Einstein supposed gravitational timedilatation in 1908. It was approved by redshifting experiment in 1960. Until gravitational timedilatation was more than just a hypothesis that may be true but never experienced there where already two satelites in orbit (Sputnik 1: 1957, Explorer 1: 1958).</p>
<p>&quot;estimating&quot; maybe was bad phrasing. Like u/MadDoctor5813 said: they didnt believe in it because it was still unproven.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/MadDoctor5813" target="_blank">MadDoctor5813</a>
			<div class="markdown"><p>The way I heard the story told is that the engineers didn't believe in relativistic effects before sending them up, and only changed it after they noticed the drift.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Brownie3245" target="_blank">Brownie3245</a>
			<div class="markdown"><p>I still can't wrap my head around how a digital clock would tick faster or slower depending on gravitational pull. Its basically a calculation, so it should be constant whereever it is.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/infected_funghi" target="_blank">infected_funghi</a>
			<div class="markdown"><p>Its nothing intuitive. People tend to imagine time as a constant independent &quot;thing&quot; but it actually is entangled with a reference of view. When you are on the space station, your watch ticks the same frequency like you experienced on earth because you're in the same reference view (for you its not going slower or faster). But if you come back to earth your watch will be asynced because from earths view it was ticking slower. Relativity is just weird and runs over our trivial idea of how nature works. (Feel free to correct me, im still no physicist!)</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/whyisthesky" target="_blank">whyisthesky</a>
			<div class="markdown"><p>Relative to itself it is constant, all observers feel time passing at the same rate for themselves, however we see the clock ticking at a different speed because relative to us its time, and thus the speed of the internal mechanism making it change, is constant</p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/westward_man" target="_blank">westward_man</a>
			<div class="markdown"><p>What are you talking about?</p>
<p>A signed 32-bit integer can represent 68 years in seconds, and and unsigned int can represent 136 years.</p>
<p>A signed 64-bit integer (long) can represent 2.92 x 10^11 years in seconds, and an unsigned long can represent 5.85 x 10^11.</p>
<p>Seeing as how the universe is estimated to be 1.382 x 10^10, I'm really curious as to what time scales you think overflow is going to occur at.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/MasterFubar" target="_blank">MasterFubar</a>
			<div class="markdown"><p>When doing astronomical calculations you need all the digits of accuracy you can get, so you try to avoid adding big constants to any number you use.</p>
<p>The standard format for 64 bits floating point is IEEE-754, which has 53 bits of mantissa. At first sight, this seems to be enough for 285 million years of <strong>counting</strong> seconds, but you want sub-second accuracy in your calculations.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/westward_man" target="_blank">westward_man</a>
			<div class="markdown"><p>Accuracy is a fair point, but now I'm confused, because you said they use days because the second was too small.</p>
<p>But now you're saying they need sub-second accuracy, which in my mind doesn't really mesh with using days, a non-SI unit, as a unit.</p>
<p>And I think even in computer calculations we can still rely upon significant figures and percent error to maintain accuracy.</p></div>		</li>
					</ul>
		</ul>
		</ul>
	