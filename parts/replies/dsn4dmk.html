	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Gohanthebarbarian" target="_blank">Gohanthebarbarian</a>
			<div class="markdown"><blockquote>
<p>Thus a lower bound of the information encoded in the activation of a neuron would contain about 1015*7000 possible stages, or roughly 348802 bit, or about 43kB.</p>
</blockquote>
<p>I find this confusing. 10 raised to the power of (15*7000) is an astronomical number much greater than 43kB. Even just 10 raised to the power of 15 is a huge number, that would be about 1000 terabytes.</p>
<p>Are there multiple stages per byte?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/jaredjeya" target="_blank">jaredjeya</a>
			<div class="markdown"><p>A bit is a single binary digit, and a byte is 8 bits.</p>
<p>For example, 10010100 = 148 is a 8-bit number because written in binary it requires 8 digits. It could be represented by a row of switches with the 1st, 4th and 6th turned on (which is why bits are a useful unit in computing). The largest possible 8 bit number is 255 = 11111111 = 2^(8)-1, so (including 0) that’s 2^8 possible numbers.</p>
<p>That means 10^(7•15000) is 8•43,000 = 344,000 digits long in binary (approximately) - the calculation would be log2(10)•7•15000 - and you’d need 344,000 switches to store that number in its entirety. </p>
<p>Given the number of atoms in the universe is ~2^266, meaning you’d need just 266 bits to give every single atom a unique identifying number, and that 145 bits will let you measure the width of the universe to the diameter of a proton, that’s pretty huge. </p>
<p>On the other hand, when you start combining things you can get an extraordinary number of possible combinations, and given letters are usually represented with 8-bit numbers (look up ASCII), 43kB is how much it takes to store a 43,000 character long book. Which sounds much less impressive.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/skydivingdutch" target="_blank">skydivingdutch</a>
			<div class="markdown"><p>It just means that that 43 kB of storage can represent that many states, ie 2^43*1024.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/MrWorshipMe" target="_blank">MrWorshipMe</a>
			<div class="markdown"><p>43 kB can represent 256^43x1024 = 2^8x43x1024, since there are 8 bits in a byte...</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/MrWorshipMe" target="_blank">MrWorshipMe</a>
			<div class="markdown"><p>Actually, 10^15 can be stored in 50 bits ( 2^50 is a bit larger than 10^15 ).</p>
<p>And 10^15<em>7000 just requires 7000</em>50 bits.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/StartingVortex" target="_blank">StartingVortex</a>
			<div class="markdown"><p>In machine neural networks, the weights can have  fairly low resolution with little loss of performance.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/MrWorshipMe" target="_blank">MrWorshipMe</a>
			<div class="markdown"><p>Sure, I'm not saying all of the biological complexity is needed for a functioning neural net, I was just trying to quantify how much information is in an average biological neuron.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/EqualResponsibility" target="_blank">EqualResponsibility</a>
			<div class="markdown"><p>In other words it’s a forward chaining expert system that can be represented by a unidirectional weighted graph.</p>
<p>In essence it’s the difference between miles (a unit) and miles per second (a measurement) Or perhaps Watts versus Watthours. </p>
<p>This is why I don’t believe AI will ever be a concern as a autonomous being. As silicon based computers only deal with 1s and 0s, they can only do what they were programmed to do.</p>
<p>Any AI would need to be based on a SOC design in which it could reconfigure it’s own silicon structure. Not simulate it via binary inputs. Physical chips can have potentiometers but they remain static. Our brains basically build their own machinations over and over. Rerouting the inputs and outputs.</p>
<p>I’m just blabbering gibberish. Feel free to ignore. </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/MrWorshipMe" target="_blank">MrWorshipMe</a>
			<div class="markdown"><p>There's no need for a hardware based method of plasticity.</p>
<p>These kind of graphs are easily adjusted and simulated in software. Current neural net architectures, while greatly simplified (representing each connection weight by only 4 or 2 bytes, and containing only about 10^8 weights as opposed to 10^13 weights in the human brain), are simulated on current computers, and can be trained within days household computers.</p>
<p>If Moor's law holds for some 16 years from now - we'd have the technology to simulate such a simplified graph with the same number of weights as are present in the human brain on a regular computer. Wait another two years to have it reach the minimum I've posited for resolution of a biological neuron (even though current neural nets do not require such resolution to perform well). </p>
<p>But extrapolating into the future is a tricky business :)</p></div>		</li>
					</ul>
		</ul>
	