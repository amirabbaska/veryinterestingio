	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/EdDwag" target="_blank">EdDwag</a>
			<div class="markdown"><p>For this reason, I've seen a cabinet of only GPUs used to analyze data used in large physics experiments, such as the newly famous g-2 experiment at Fermilab, I believe. I worked there as an intern for a summer. </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/mfb-" target="_blank">mfb-</a>
			<div class="markdown"><p>GPUs are often used in those cases, indeed. &quot;We have 1 billion events which should all be fed to the same analysis code.&quot;</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/jenbanim" target="_blank">jenbanim</a>
			<div class="markdown"><p>How'd you like your internship? G-2 seems like a pretty intense place to get started in the physics world.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/EdDwag" target="_blank">EdDwag</a>
			<div class="markdown"><p>My Fermilab internship was great, although I didn't actually work for g-2 (sorry for the miscommunication). I just toured the brand new g-2 facility at the time (2014). I actually worked at the D0 particle detector on the Tevatron accelerator. The team was trying to find statistically significant evidence of the Higgs particle (just like the detectors at the LHC did in 2012). It's much more difficult at the Tevatron due to its lower energies. I learned quite a bit, although I always wish I could go back and give it another go, because I know I would be 100 times more useful now than I was back then (had just ended my second year as a physics student at the time). Now having had much more similar research under my belt, I know I could actually really help the team out instead of just trying to learn things the whole time. But hey, I guess that's the difference between being a lowly intern and, say, a professional scientist haha.</p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/aNewH0pe" target="_blank">aNewH0pe</a>
			<div class="markdown"><p>&quot;There's nothing stopping you from running arbitrary code on a GPU, but performance will tank.&quot; </p>
<p>Only true, if your code doesn't need CPU exclusive features, like e.g. recursion. </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/poizan42" target="_blank">poizan42</a>
			<div class="markdown"><blockquote>
<p>Only true, if your code doesn't need CPU exclusive features, like e.g. recursion.</p>
</blockquote>
<p>You have memory, you can always build your own stack. Also see <a href="http://stackoverflow.com/questions/3644809/does-cuda-support-recursion" target="_blank">this</a></p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/aNewH0pe" target="_blank">aNewH0pe</a>
			<div class="markdown"><p>Wow, that's actually pretty cool, that they got this to work.</p>
<p>The more you know...</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/MadScienceDreams" target="_blank">MadScienceDreams</a>
			<div class="markdown"><p>At least 5 years ago when I was doing CUDA programming, conditionals, loops, and non-block-aligned-memory-access all were...problematic (not impossible, just slowed down your code by orders of magnitude).</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/xthecharacter" target="_blank">xthecharacter</a>
			<div class="markdown"><p>To add the the other person who responded to you, GPUs can compute nand so they are Turing complete, so they really can do everything -- it just might be terribly convoluted and inefficient.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/actuallyserious650" target="_blank">actuallyserious650</a>
			<div class="markdown"><p>It's my understanding that GPUs transistors are also activated with lower voltages than CPUs.  This makes them more prone to errors, but as a tradeoff they produce far less heat and can be packed much more tightly.</p></div>		</li>
					</ul>
	