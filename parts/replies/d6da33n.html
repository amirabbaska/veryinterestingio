	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/HugodeGroot" target="_blank">HugodeGroot</a>
			<div class="markdown"><p>The problem is that for all of its flaws the p-value offers a systematic and quantitative way to establish &quot;significance.&quot; Now of course, p-values are prone to abuse and have seemingly validated many studies that ended up being bunk. However, what is a better alternative? I agree that it may be better to think in terms of &quot;meaningful&quot; results, but how exactly do you establish what is meaningful? My gut feeling is that it should be a combination of statistical tests and insight specific to a field. If you are in expert in the field, whether a result appears to be meaningful falls under the umbrella of &quot;you know it when you see it.&quot; However, how do you put such standards on an objective and solid footing? </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/veritasium" target="_blank">veritasium</a>
			<div class="markdown"><p>By meaningful do you mean look for significant effect sizes rather that statistically significant results that have very little effect? The Journal Basic and Applied Psychology last year banned publication of any papers with p-values in them</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/HugodeGroot" target="_blank">HugodeGroot</a>
			<div class="markdown"><p>My ideal standard for a meaningful result is that it should: 1) be statistically significant, 2) show a major difference, and 3) have a good explanation. For example let's say a group is working on high performance solar cells. An ideal result would be if the group reports a new type of device that: shows significantly higher performance, it does so in a reproducible way for a large number of devices, and they can explain the result in terms of basic engineering or physical principles. Unfortunately, the literature is littered with the other extreme. Mountains of papers report just a few &quot;champion&quot; devices, with marginally better performance, often backed by little if any theoretical explanation. Sometimes researchers will throw in p values to show that those results are significant, but all too often this &quot;significance&quot; washes away when others try to reproduce these results. Similar issues hound most fields of science in one way or another. </p>
<p>In practice many of us use principles somewhat similar to what I outlined above when carrying out our own research or peer review. The problem is that it becomes a bit subjective and standards vary from person to person. I wish there was a more systematic way to encode such standards, but I'm not sure how you could do so in a way that is practical and general. </p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/[deleted]" target="_blank">[deleted]</a>
			<div class="markdown"><p>You can engineer a study to produce a p value. The construction of the experiment is the only meaningful thing-does it control properly? Or does it cherry pick? If it's badly constructed the p-value means nothing. And how much does the p-value skew the likelihood of getting published? It's the definition of a perverse incentive.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Xalteox" target="_blank">Xalteox</a>
			<div class="markdown"><p>Well, I personally want to chime in and say that even where P values are used, the scientific world seems to have too much dependence on the 0.05 value, even if it may not be the best method. The 0.05 threshold is certainly not a &quot;one size fits all&quot; approach, however is treated as one. I have a feeling that many journals do not look much further than the abstract and the data, including P values. This would require science as a whole to change the way it looks at study results, and maybe a system simply without P values would be the easiest way to do so.</p>
<p>I'm no scientist, just interested.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/atomfullerene" target="_blank">atomfullerene</a>
			<div class="markdown"><p>One common problem in biology is that results can be statistically significant without being biologically significant.  This tends to happen when your data comes out statistically significant, but the effect size is tiny.  Eg if fish show a significant preference for eating A over B in controlled lab conditions, but that preference means they eat A 1% more often than B, it likely means that in the wild other variables totally swamp this preference and it's not having an ecological impact.  </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Phoebekins" target="_blank">Phoebekins</a>
			<div class="markdown"><p>That's an issue in health/medical fields as well.  It's up to the clinician to decide if the effect of some new treatment is really worth change from the gold standard or adding additional steps.  An outcome may be &quot;statistically significant&quot; but not &quot;clinically significant.&quot;</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/superhelical" target="_blank">superhelical</a>
			<div class="markdown"><p>I think the p-value would be a lot more meaningful if the analysis is appropriately registered, blinded, and pre-established like /u/veritasium says in the video. It is much more powerful if you can remove it from the human foibles that lead it astray.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/luckyluke193" target="_blank">luckyluke193</a>
			<div class="markdown"><p>This is what they partially do at the big experiments at CERN. The entire data analysis pipeline is set up and tested with fake and simulated data, before it is fed real data.</p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/XkF21WNJ" target="_blank">XkF21WNJ</a>
			<div class="markdown"><p>I think the problem lies with the way the term &quot;significant&quot; (or &quot;statistically significant&quot;) is used, rather than the term &quot;significant&quot; itself. If Fisher, or whomever first used the term &quot;significant&quot;, had used the term &quot;meaningful&quot; instead we'd probably have the same problems.</p>
<p>The real problem seems to be that people started using the term &quot;significant&quot; whenever the result passed some arbitrary statistical test, even if that test was entirely inappropriate for that particular experiment.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/EquipLordBritish" target="_blank">EquipLordBritish</a>
			<div class="markdown"><blockquote>
<p>The real problem seems to be that people started using the term &quot;significant&quot; whenever the result passed some arbitrary statistical test, even if that test was entirely inappropriate for that particular experiment.</p>
</blockquote>
<p>I think you're totally right.  This is an issue of using the proper test for the experiment and definitely not something that a lot of reviewers check for; especially because they tend to focus on the concepts of the experiments rather than being a statistician and looking at the validity of the test.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/superhelical" target="_blank">superhelical</a>
			<div class="markdown"><p>Great point, I guess I'm arguing against the implied &quot;significance (at p &lt; 0.05) = insight&quot;, rather than the term itself.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/abecedarius" target="_blank">abecedarius</a>
			<div class="markdown"><p>That's fair, but if the convention were like &quot;this result is null-improbable (p=0.044)&quot;, the everyday meaning of the words would align a lot better. If we were robots influenced only by the technical meaning, it wouldn't matter. But <a href="https://en.wikipedia.org/wiki/Misunderstandings_of_p-values" target="_blank">https://en.wikipedia.org/wiki/Misunderstandings_of_p-values</a> suggests otherwise.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Hypothesis_Null" target="_blank">Hypothesis_Null</a>
			<div class="markdown"><p>It simply means something specific in the scientific realm vs more common parlance.</p>
<p>The downside is that people trying to push an agenda can fully claim &quot;significant&quot; results, and the general public will misinterpret that to mean &quot;significant in magnitude&quot; rather than &quot;significant in probabilistic certainty.&quot;</p>
<p>For instance, consuming extra salt will <em>significantly</em> increase your blood pressure.  Solid science proves this.</p>
<p>However, unless you're an uncommon person with significant salt sensitivity, consuming an excessive amount of salt will only raise your blood pressure by one or two points out of 120-150.  So it is an utterly meaningless amount - scientists are just <em>very</em> certain that those one or two extra points were indeed caused by the salt.</p>
<p>Yet people still repeat the meme: &quot;Salt is bad for you.&quot;</p>
<p>But changing to another word wouldn't really fix that issue.  People get lazy or hyperbolic using scientific terminology when talking with other people.  This isn't a fight you can win by changing words.  Just attack the bullshit wherever you see it.</p></div>		</li>
					</ul>
	