	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/DisagreeableMale" target="_blank">DisagreeableMale</a>
			<div class="markdown"><p>I was going to explain every detail of the internet for you, but it's taken me two attempts to accept that that's not what you want to know. So I'll just tell you.</p>
<p>Many websites that you go to have only one point of entry, and it's referred to as the &quot;web root.&quot; On many sites or services, this file can usually be identified as something like <strong>index.html</strong> or <strong>index.js</strong> (or anything referring to index).</p>
<p>To prove this, you can go to GitHub and click a random repository (code base) and just look at the file names. Many advanced sites, like social media sites, will use many different &quot;views&quot; to render different portions of an application on one document (e.g. having dynamic page content but only one page). This is by cleverly inserting content into an existing page without having to reload it again; this isn't important to dig into further.</p>
<p>But when you're looking at the repository, you'll notice that there are <strong>MANY</strong> files in those codebases that is not index.html or index.js and are not meant to be accessed directly.</p>
<p>All of those other files are the machinery that's moving content, styles, data, network functionality, and performance that brings the page to life (other than being static bullshit on your non-connected computer). So, for every entry point to a website, you're likely to have a fuckton of files that are considered &quot;deep&quot; by comparison. </p>
<p>Someone could say that these logic-based files on the back-end shouldn't be considered &quot;deep web&quot; because they're not locations, really. A user can't click a link and navigate to one of them, which is actually not true. Any file can be served to a domain name. It just so happens that the only one that 99.999999999% of the Internet want you to see is their front-end content, which is the point, because when you look at it through a browser, it looks pretty good.</p>
<p>You <em>could</em> look at a database file, but you're not going to like reading it. You <em>could</em> look at the logic behind signing up, logging in, making a post, etc. but you're not going to get shit from that unless you're a programmer trying to learn (e.g. ethical hacker) or someone trying to weasel through a logical loophole to something you shouldn't be doing (e.g. unethical hackers).</p>
<p>The only things determining whether a file on a site is a &quot;location&quot; is what's referred to as &quot;routing,&quot; which dictates that if someone goes to our domain <strong>reddit.com</strong>, which specifically decides that it wants to display <strong>index.html</strong> as its web root, we're going to show them the index.html file. If they look for a subresource from that root access point, such as a subreddit, we're going to make them indicate that by adding &quot;/r/&quot; and then the name of the subreddit afterword.</p>
<p>When they enter a string of characters after /r/ that makes sense to our database of subreddits (aka the subreddit exists), we will then reload our browser document with that database record of that unique subreddit and the subresources of it (aka the posts).</p>
<p>Reddit <em>could</em> make the subreddits database a location we could access, but that'd be silly and it'd look terrible. It'd basically be a big spreadsheet of subreddit names and whatever other related info they decide to log with it (likely subscriber counts and moderators included).</p>
<p>Anyway, my point here is that because of how the web technologies are reliant on each other and how bits and pieces of them are used to render a single website view, there's not really a clean way to determine how the Internet can be measured.</p>
<p>On a personal site, you may have a ratio of 1 viewable file to 10 unviewable files that provide services (styles, dynamic effects, stored data) to that viewable page. While a company may have a much higher ratio, because they will have viewable pages for employees only to view (which would be considered &quot;deep&quot;, since a rando can't see it).</p>
<p>We could measure the amount of registered domain names, sure, but not all sites have domain names and not all domain names have viewable or accessible files associated with them, so that won't work.</p>
<p>There is no way to measure the deep web, because it is intentionally unobservable. Only a developer or tech professional will have access to a site or services &quot;source code,&quot; so their tiny bubble can really only be measured by them, which does no one else any good.</p>
<p>As far as your second question about the porn. It's probably true that most Internet use by the mainstream public is to access porn, which is just another way that people feed their cravings, just like jumping out of bed at 3am because you gotta have some McDonald's fries (what? I've done it). Comparatively though, you can't jump out of bed and simply say &quot;I just gotta have some pussy&quot; and then hit the drive-thru real quick, so many people route all that animalistic energy to their dream machine (aka the Interwebz).</p>
<p>Sorry for the rant. I love tech and love talking about it.</p>
<p>Edit: Thank you for the gold, kind stranger. :)</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/mysecondaccount150" target="_blank">mysecondaccount150</a>
			<div class="markdown"><p>You're pretty agreeable for a disagreeable male.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/whootdat" target="_blank">whootdat</a>
			<div class="markdown"><p>Had to say, JavaScript is not a supported web root (to my knowledge) htm, html, and php are the usual. If you're running IIS you could have asp/aspx. So index.js would be pretty incorrect.</p>
<p>You also described MVC  (model/view/controller), kind of, but very poorly. It sounds like you are explaining it having been told about it 3rd hand. MVC is one way to do dynamic web pages. Places like Reddit and Twitter use them pretty heavily. You didn't even touch on how others can be dynamic, like how Google is google.com/search?q= and by changing what is on the end, you get completely different pages. (E.g. changing parameters instead of different files or views)</p>
<p>You also seem to be conflating files on a web server with &quot;web pages&quot; just because I put a text document or a image on a web server does not mean it is a web page, it is simple a file that is accessible to some person or people  (who may or may not have to be logged in). So saying a style sheet (css) is included, or various files attached to dynamic sites (partial html or php or whatever) is stretching things pretty far. Those files can't display an actual web page without some other components and information.    </p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Acysbib" target="_blank">Acysbib</a>
			<div class="markdown"><p>Very!</p>
<p>Last I saw about 65-75% of the internet is porn... That includes the wholesome... And not so wholesome</p>
<p>Edit: just read an article (still doing research) that stated 30% is porn.</p>
<p>2 billion web pages... Holy shit...</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/MJBrune" target="_blank">MJBrune</a>
			<div class="markdown"><p>Can you link theses things you are finding? 30% seems high to be honest. How are we measuring it's size?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/prthfr" target="_blank">prthfr</a>
			<div class="markdown"><p>Good point. Are we talking length or girth?</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Alis451" target="_blank">Alis451</a>
			<div class="markdown"><p>I would guess through traffic stats, possibly ISP, probably throughput, or bandwidth.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Acysbib" target="_blank">Acysbib</a>
			<div class="markdown"><p>Reddit hates me... No can links</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/PineappleResearchEnt" target="_blank">PineappleResearchEnt</a>
			<div class="markdown"><p>600 Million <a href="https://www.quora.com/How-much-is-600-million-dollars-in-Rupees" target="_blank">https://www.quora.com/How-much-is-600-million-dollars-in-Rupees</a></p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/dubs_decides" target="_blank">dubs_decides</a>
			<div class="markdown"><p>the &quot;dark net&quot; (tor, i2p, freenet, basically the sneaky anonymizing subnets) are actually a vanishingly small amount of public content compared to the surface web. Most tor forums, markets, imageboards etc are woefully inactive. Rumors of vast criminal networks or cult hubs or secret research caches and stuff like that are incredibly overstated. There's <em>some</em> drug trade, <em>some</em> illegal porn, <em>some</em> hacking/carding websites, most of it is just boring nonsense.</p></div>		</li>
					</ul>
	