	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/annitaq" target="_blank">annitaq</a>
			<div class="markdown"><p>Putting this into numbers, Jupiter receives 50 W/m^2 from the Sun. This might look tiny compared to Earth's 1360 W/m^(2), but as you said, our perception is not linear.</p>
<p>We could compare this to a room illuminated by a 100 W incandescent bulb. That's a powerful one. If you put it in a small 10 m^2 room it will look <em>disturbingly</em> bright... and it's not as bright as Jupiter!</p>
<p>Neptune receives just 1.5 W/m^(2). A 70 m^2 room illuminated by a 100 W bulb would receive more or less the same power and look moderately dark. But if we consider that an incandescent bulb produces very little visible light compared to infrared, and that sunlight has a much higher fraction in the visible spectrum, Neptune would actually look much brighter.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/foomprekov" target="_blank">foomprekov</a>
			<div class="markdown"><p>Do they have the same percentage of visible light? </p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Astromike23" target="_blank">Astromike23</a>
			<div class="markdown"><p>The total amount of light you'd see also really depends on how close your spacecraft is to the planet. As a basis for comparison, let's think about the dimmest of these, Neptune, and how bright the light would be compared to the Full Moon. </p>
<p>If the Moon were twice as close, it would take up 4 times as much area on the sky, and the moonlight would then be 4 times as bright. The same goes for Neptune, although with a few additional corrective factors. It's 30 times as far from the Sun as the Moon-Sun distance, which means Neptune is only receiving 1/900th the sunlight as the Moon per square meter. However, Neptune is also about 4 times as reflective (surprisingly, the Moon is actually very dark, about the same reflectivity as asphalt), so Neptune actually reflects about 1/225th the sunlight per square meter as the Moon.</p>
<p>On top of that, you also have to account for the fact that Neptune is quite a bit bigger than the Moon. Since it's radius is about 14x the size of the Moon, that means it takes up about 200x more area of the sky. When combined with the 1/225th per square meter of sunlight reflected, it turns out that if your spacecraft-Neptune distance were just about the same as the Earth-Moon distance, it would be just about as bright as the Full Moon (though it would take up a much larger portion of the sky). As you change your distance to Neptune, again that would increase or decrease that brightness as the distance squared.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/tiredofthrowing" target="_blank">tiredofthrowing</a>
			<div class="markdown"><p>Isn't there an equation that defines the minimim amount of change required to detect a change? For example let's say 10%. If there are 10 units of brightness from a light(i don't know the actual unit, lux maybe?), then if it changed by one unit, you'd be able to detect a change. However if there was a brighter light at 100 units, there
would have to be a change of 10 units in order for you to detect a change</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/alaughton" target="_blank">alaughton</a>
			<div class="markdown"><p>You're probably thinking of Weber's fraction. It's pretty rule-of-thumb, though. </p></div>		</li>
					</ul>
		</ul>
	