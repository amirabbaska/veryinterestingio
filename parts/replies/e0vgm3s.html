	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/MaxWelling" target="_blank">MaxWelling</a>
			<div class="markdown"><p>Currently the new tools are indeed just that: tools. But we are slowly seeing the emergence of algorithms, like AlphaGO that seem to develop an &quot;understanding&quot; of a limited domain that goes beyond the best humans. I agree that current AI is not providing many clues about human intelligence at this point. Human intelligence seems to develop a very deep understanding of the world around us from which it is easier to generalize to new tasks. We are far better in learning from few examples, and develop abstractions of the world from which we can make very effective predictions. And we do all that using much less energy. So it seems we still need a number of new ideas to get as good as human intelligence. </p>
<p>It's also interesting to ask yourself even we would build a general AGI that is equally intelligent as a human, would we have learned much about how human intelligence works? It might still be a black box that is as difficult to understand as a human brain. </p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/csreid" target="_blank">csreid</a>
			<div class="markdown"><blockquote>
<p>[Artificial Intelligence] is doing very little from a scientific point of view. It is achieving things from an engineering point of view. Which is OK -- nobody should be opposed to bigger bulldozers if they work. But we shouldn't be misled about the insight it is supposed to be providing into the nature of intelligence.</p>
</blockquote>
<p>You didn't ask me, but this is kind of an annoying take. The science behind ML isn't the science behind the nature of intelligence. ML is a science unto itself, where we study techniques and math for modeling problems. Tools are engineered from that science. None of that necessarily touches &quot;the nature of intelligence&quot; because that's more the domain of philosophy or biology.</p>
<p>It's entirely possible (and, imo, likely) that ML will <em>never</em> enlighten us about the nature of intelligence because it could very well turn out that the best way for <em>computers</em> to tackle learning problems is totally different from what natural selection landed on for tackling learning problems.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/tzjmetron" target="_blank">tzjmetron</a>
			<div class="markdown"><p>Well, that's a fair criticism from Chomsky because AI did start out on that premise, and nobody's bothered to change the perception since.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/anotherdonald" target="_blank">anotherdonald</a>
			<div class="markdown"><p>Well, that's why we traditionally, and that means since 50 years or more, distinguish strong from weak AI. The latter is imitating human intelligence to solve problems computationally, while the former is supposed to provide insight in what intelligence is.</p>
<p>Chomsky is not one to speak, though: his linguistic work is all about superficial description of language. It offers no insight into how language understanding or production actually works.</p></div>		</li>
					</ul>
		</ul>
		</ul>
	