	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Dr_Zandi" target="_blank">Dr_Zandi</a>
			<div class="markdown"><p>Don't forget that neurons do not only use electrical differentials, but also neurotransmitters. They're not simple binary devices.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/herbivorous-cyborg" target="_blank">herbivorous-cyborg</a>
			<div class="markdown"><p>Neurotransmitters are what cause electrical differentials, arent they?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/MadGeekling" target="_blank">MadGeekling</a>
			<div class="markdown"><p>Basically yeah.  Another cell (like another neuron) has a receptor for it and will respond differently depending on A) what the neurotransmitter is and B) what the receptor is.  So a neurotransmitter that results in an excitatory response in one type of cell can result in an inhibitory response in another.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/OwariNeko" target="_blank">OwariNeko</a>
			<div class="markdown"><p>Building on MadGeekling's answer, neurotransmitters bind to receptors on the 'receiving' neuron and activate or deactivate ion-channels so that ions can permeate through the membrane more or less easily, which results in a change in the concentration of positively and negatively charged ions on either side of the membrane - we get a different membrane potential.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/smedes" target="_blank">smedes</a>
			<div class="markdown"><p>Neurotransmitters are released from the axon terminal of the pre-synaptic cell and bind to receptors on the post-synaptic cell. Many of those receptors are ligand-gated ion channels, which means that when the proper neurotransmitter binds to them, the channel opens and ions rush in or out of the cell. The concentrations of various positive and negatively charged ions are different inside the cell and out, and that's why there is a voltage across the membrane.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/andreasbeer1981" target="_blank">andreasbeer1981</a>
			<div class="markdown"><p>They also transmit genes, as was recently discovered. So yeah, it's complex.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/encomlab" target="_blank">encomlab</a>
			<div class="markdown"><p>THANK YOU!!  I am constantly defending this position when discussing AI or Cybernetics: neurons are not binary switches - they are analog electro-chemical transducers with both potentials and threshold values. </p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/8spd" target="_blank">8spd</a>
			<div class="markdown"><p>It seems interesting to me to turn the question around, and ask &quot;how many bits of data do we need to represent the state of a neuron with reasonable accuracy?&quot; </p>
<p><strong>edit:</strong> I'm thinking that this will take into account both the nessisary precision of the various parameters, and the number of perameters. For example, the amount of electrical energy present is one parameter, and would presumably need to be represented by quite a few bits of data to meaningfully represent the state. 16bits? 32 bits? Whether or not it's firing might be another parameter, one that could be represented by a single bit. Am I on to something here?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Brudaks" target="_blank">Brudaks</a>
			<div class="markdown"><p>If we're talking about e.g. long-term memory storage, then in the &quot;state of a neuron&quot; we can <em>ignore</em> things like the amount of electrical energy present (or, really, all short-term state that's going to change within a single minute), the thing that seems to matter is the particular connectivity pattern - i.e. what synaptic connections the neuron has, what is their structure, what is their relative strength.  A single neuron may easily have direct connections to 1000-10000 other neurons, and their &quot;strength&quot; matters (changes in synaptic links is likely to be the basis of learning) - for each connection, a quadrillion of such connections in our brain, it'd take multiple bytes to describe <em>which</em> other neuron it's connected to (it can be literally on the other side of the brain or even the body), and at least a few bits for the connection &quot;parameters&quot; as it's believed (known?) that the branching tree-like structure of these connections also matters and might perform something like rudimentary calculations (&quot;logic gates&quot;) within that single neuron. </p>
<p>So we might be talking 10kb - 100kb to describe the connectivty pattern of a single neuron.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/8spd" target="_blank">8spd</a>
			<div class="markdown"><p>I wasn't actually sticking with the memory storage theme of the OP, and just wondering how difficult it would be to model a single neuron in a digital context.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/zebediah49" target="_blank">zebediah49</a>
			<div class="markdown"><p>So then the messy question is if we can further compress this representation -- is there a way of predicting the potential places it can connect to, so that addressing them is more efficient.</p>
<p>Or, to put this in terms that make it obvious that no answer will be forthcoming, &quot;how much of the details of a connectome are defined by nature vs. nurture?&quot;.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/balls4xx" target="_blank">balls4xx</a>
			<div class="markdown"><p>The relevant question for you is, at what resolution? If you want to see some full scale molecular simulations of at least segments of neurons, there is a program called MCell being developed at the Salk Institute to do just that. </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/8spd" target="_blank">8spd</a>
			<div class="markdown"><p>&quot;At what resolution?&quot;, is really my question too. At what resolution do we need to understand or model a neuron to have a meaningful understanding of it's activity or function? Is this something that has been established?</p></div>		</li>
					</ul>
		</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Yotsubato" target="_blank">Yotsubato</a>
			<div class="markdown"><p>Isnt the 3D arrangement of neurons and how they are connected the way information is stored? Its not based on wether the neuron is depolarized or not, thats more how information is accessed and sent out to the proper output. </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/SoPeriPersonal" target="_blank">SoPeriPersonal</a>
			<div class="markdown"><p>It seems to be a combination of patterns of activation, timing of activation, and patterns of timing of activation. In addition, certain brain areas seem to be specialized for storing different types of information better than others, but it's not clear if this is due to innate (i.e., genetically determined) factors or experiential ones. </p>
<p>Source: PhD in cognitive psychology, who has several friends and colleagues studying memory/neural specialization.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/MuonManLaserJab" target="_blank">MuonManLaserJab</a>
			<div class="markdown"><blockquote>
<p>In addition, certain brain areas seem to be specialized for storing different types of information better than others, but it's not clear if this is due to innate (i.e., genetically determined) factors or experiential ones. </p>
</blockquote>
<p>There is definitely specialization between brain regions even at birth. Certainly if you're talking about completely different parts like the cerebellum vs. the hippocampus, but also between regions of the cortex.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/balls4xx" target="_blank">balls4xx</a>
			<div class="markdown"><p>Good answer. </p>
<p>Certainly different brain areas are specialized for different sorts of information processing. I would say all areas are genetically determined to be what they are supposed to be, but require environmental input to develop properly. </p>
<p>A few examples, ocular dominance columns require input from both eyes during the critical period. Experiments have shown that sewing one eye shut during this period will disrupt their formation and even after opening the closed eye, the animal will never be able to make the same visual discriminations as one who had both eyes open during the critical period. </p>
<p>Likewise, the auditory system starts wiring itself in utero (in humans, no other animals have language) in response to the mothers speech. But severely neglected children who grow up never hearing speech will only develop extremely limited language ability if at all, though they have the same broca and wernickies areas as everyone else. </p>
<p>We don't really know all of the ways brains store information. I suspect much is in the complex 3D geometry of the neurons themselves, but one thing we're pretty confident of is at least some information is stored as synaptic strength. </p>
<p>Abstracting this idea to two dimensional artificial neural networks indeed underlies nearly all of modern machine learning. Most simply/abstractly ANNs are represented as series of matrix or tensor operations. Most learning algorithms are profoundly non-biological, i.e., backpropagation, which is fine since we're no where close to understanding all the learning rules implemented by biological neural networks. The more we learn the more we can apply to ANNs and the more powerful they become.  </p>
<p>Source: neuroscience PhD specializing in learning and memory. </p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/trashacount12345" target="_blank">trashacount12345</a>
			<div class="markdown"><p>Depends on the time scale. Information can be stored for a few seconds by firing rates in a balanced excitatory/inhibitory network. </p>
<p><a href="https://link.springer.com/article/10.1007/s10827-017-0662-8" target="_blank">https://link.springer.com/article/10.1007/s10827-017-0662-8</a></p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/pragmojo" target="_blank">pragmojo</a>
			<div class="markdown"><p>There's way more to it than that.  For instance, the sensitivity of different synapses is modulated over time based on activity, so the same neuronal structure may behave very differently depending on how the synapses have been tuned.</p></div>		</li>
					</ul>
		</ul>
	