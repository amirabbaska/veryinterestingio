	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/xrelaht" target="_blank">xrelaht</a>
			<div class="markdown"><p>5Ïƒ means they are stating their result with a 99.9999426697% <a href="http://en.wikipedia.org/wiki/Confidence_interval" target="_blank">confidence interval</a>. r is a variable in the model related to something called the tensor/scalar ratio, but I don't think I can explain it very well. It's right where it was predicted by theory.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Schpwuette" target="_blank">Schpwuette</a>
			<div class="markdown"><p>Sean Carrol's article (posted before the results were released, it's in the OP here too) seemed to imply that we were expecting a much lower r, thanks to previous data - <a href="http://www.preposterousuniverse.com/blog/wp-content/uploads/2014/03/planckinflation.png" target="_blank">something like 0 to 0.05</a>. He notes that those predictions were low sigma, though...</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/xrelaht" target="_blank">xrelaht</a>
			<div class="markdown"><p>I believe it. I went to my department's viewing of the press conference this morning, and one of the people there with more expertise than I have said this project was so far out there that they've been in danger of having their funding cut for years. The FAQ linked up top from Kek-BICEP even mentions the Planck r~0.11 result in the context of theirs.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/xrelaht" target="_blank">xrelaht</a>
			<div class="markdown"><p>I guess what I should have said is that r~0.2 puts it right where the BICEP people were betting it would be. BICEP <em>needed</em> r to be big for it to be detectable by them in any kind of finite time frame. Otherwise, <a href="http://lsst.org/lsst/" target="_blank">the LSST</a> or <a href="http://pole.uchicago.edu/" target="_blank">the SPT</a> would see it first and all their efforts would have been for nothing because BICEP is really a specialist instrument while those other ones have a much wider scope of potential projects.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Mac223" target="_blank">Mac223</a>
			<div class="markdown"><p>r = 0.2 is actually slightly larger than expected.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Astrodude87" target="_blank">Astrodude87</a>
			<div class="markdown"><p>r=.2 is explained by me <a href="http://www.reddit.com/r/askscience/comments/20n0zn/official_askscience_inflation_announcement/cg4wf16" target="_blank">here</a>. Sigma = 5 is a way of saying that if we performed ~5 million experiments that produced the same results as this work, only one of those results would be a false positive. So, in other words 5-sigma means &quot;and we believe this result to be 99.999% correct&quot;. The term comes from <a href="http://en.wikipedia.org/wiki/Normal_distribution#Standard_deviation_and_tolerance_intervals" target="_blank">statistical distributions</a>.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/joevector" target="_blank">joevector</a>
			<div class="markdown"><p>In order for such an explanation to make sense wouldn't we need to assume a pre-existing distribution of the probabilities of r values? Sorry if this is completely off base, my stat background isn't too good.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/gilgoomesh" target="_blank">gilgoomesh</a>
			<div class="markdown"><p>We don't need to pre-assume a distribution of r-values. But we do need to know what r would be if the theory was wrong.</p>
<p>For sigma variance, we ask:</p>
<ul>
<li>What would the result be given the null hypothesis?</li>
<li>What would the result be in our affirmative hypothesis?</li>
<li>What is the percentage chance that the points we sampled are actually showing a null hypothesis and not the affirmative?</li>
</ul>
<p>In this case, random spectral slopes (r=0) would be the null hypothesis and r=0.2 is the affirmative. So, what's the chance that the underlying distribution is actually r=0 despite the apparent indication of r=0.2 in the locations that happened to be sampled?</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/efrique" target="_blank">efrique</a>
			<div class="markdown"><p>(Statistician here, not a physicist, so don't rely on my for any of the physics)</p>
<p>If there are enough samples involved in the estimate (this applies to a wide variety of kinds of estimate, for example, it applies to averages of individual observations, but also to many other kinds of estimate), then the distribution of the parameter estimate will be really close to normal, except in the really extreme tail. The idea of going out 5 sigma implies a pretty strong reliance on normality of the estimator for it to hold well that far out in the tail, but in reality it probably doesn't matter all that much even if it isn't quite normal that far out (it would change the corresponding p-value, but it will still be a very,very small number). </p>
<p>[With enough observations, the calculation probably holds reasonably well out that far.]</p>
<p>With p-values that small, really, the main concerns relate to bias (that is, in a sense, other assumptions that the calculation relies on failing to hold, rather than it being not-sufficiently normal). Almost anything that mucks up the probability calculation will be because of bias in some form (e.g. some issue with equipment would go to bias). Needless to say, you can bet the team have worked pretty hard to rule all those kinds of alternative explanations out ... and now people will (rightly) raise every objection they can think of. </p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Astrodude87" target="_blank">Astrodude87</a>
			<div class="markdown"><p>The probabilities deal with uncertainty in this answer, not in the possible r values allowed. The 5 sigma is stating that there is a 5 (~0.0001%) sigma probability that r could be zero and they would observe the value of 0.2, given the uncertainties and systematic errors.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/skeen9" target="_blank">skeen9</a>
			<div class="markdown"><p>Thanks</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/McDarling" target="_blank">McDarling</a>
			<div class="markdown"><p>It is not that sigma=5, but the measurement was performed to 5 times sigma. </p></div>		</li>
					</ul>
	