	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Fala1" target="_blank">Fala1</a>
			<div class="markdown"><p>I would add to this that lowering the alpha value (which is the cut-off for the p value; the point where you accept your hypothesis as true) from 0.05 to 0.005 not just only makes false positives less common (which is a good thing of course) but it also makes false negatives more common, where you conclude that your treatment had no significant effect, even though in reality it did have an effect.    </p>
<p>So choosing your alpha value is also a trade-off between getting false positives and false negatives. </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/FlippenPigs" target="_blank">FlippenPigs</a>
			<div class="markdown"><p>Exactly. And to maintain the false negative rate you would need to substantially increase your N, which can be impractical for some studies.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/luxurylength" target="_blank">luxurylength</a>
			<div class="markdown"><p>Yup. A masters thesis may take ten years of field work to gather a large enough sample size. </p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/darwin2500" target="_blank">darwin2500</a>
			<div class="markdown"><p>And in practical terms, it means that many small effects, or effects with a low signal-to-noise ratio, may be pragmatically impossible to 'discover'.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/RainhaLouca" target="_blank">RainhaLouca</a>
			<div class="markdown"><blockquote>
<p>Going back to the drug example, if we insist on a p of 0.05, that means that out of every 20 drug trials where we showed the drug is better than placebo, we will be wrong one time and the study didn't really show them to be different. If we ask for a p-value of 0.005, then we'll only be wrong 1 in 200 drug trials.</p>
</blockquote>
<p>Actually this is incorrect. If we insist on a p of 0.05, it only means that that in each 20 trials of drugs <em>that are no better than placebos</em>, we would have roughly 1 trial where we wrongly say there is a difference. The p-value says nothing about the probability of being wrong when there is a difference between drug or placebo or about the probability of being wrong in general. This is actually at the root of the &quot;p-value scandals&quot; that we've been having for a while. As you wrote, people think that a low p means that the study has a low probability of being wrong, but it means nothing of the sort, and the probability that the study is wrong can actually be very, very high.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/tadrinth" target="_blank">tadrinth</a>
			<div class="markdown"><p>Good catch, thanks!  I updated my post accordingly.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/dalaio" target="_blank">dalaio</a>
			<div class="markdown"><blockquote>
<p>The bigger the difference between the two, the smaller the p-value, because it's less likely the drug and placebo are the same if they cure different numbers of people.</p>
</blockquote>
<p>I think it's important to note that the magnitude of the difference isn't the only thing that determines the p-value (especially given that people confound significance and effect size all the time). A tiny difference can have very small associated p-value - in the case of a standard t-test, for example, we could have small difference in means between treatment groups, but very low variance about the mean in each group.  </p>
<p>P-values shouldn't be interpreted in a vacuum. The effect size is important, because while your treatment may be producing a true effect, if that effect is very small it may not matter.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/atomfullerene" target="_blank">atomfullerene</a>
			<div class="markdown"><p>As my prof used to say &quot;is it biologically significant as well as statistically significant?&quot;</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/LoyalSol" target="_blank">LoyalSol</a>
			<div class="markdown"><p>Yup this is the trap a lot of people fall into is trusting the p-value a little too much.  The p-value only indicates that your study's results is unlikely to be due to &quot;bad luck&quot;</p>
<p>However there are still dozens of ways experimentally to get a good p-value and still end up with wrong results and even if the results are correct your interpretation of the results may not be.  Or as you just said, you could end up with a situation where the results aren't meaningful in the grand scheme of things. </p></div>		</li>
					</ul>
		</ul>
		</ul>
	