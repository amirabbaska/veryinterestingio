	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/gseyffert" target="_blank">gseyffert</a>
			<div class="markdown"><p>The example in the top comment assumed a processor with ~100 GFLOPS of computing power. A single Titan X has 6600 GFLOPS of single precision compute power, so you can assume that, theoretically, it would be able to do it nearly 66x faster. Double precision compute performance is a good deal less, but not necessary for this application I don't think.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/[deleted]" target="_blank">[deleted]</a>
			<div class="markdown"><p>Depending on the kind of hashing used, AMD might be a better choice. Remember when Newegg was selling MSRP $550 R9 290Xs for $900 during the height of the mining craze? Bitcoin and Litecoin and whatnot use some kind of hashing - I am not sure what to be honest - that AMD GPUs seem to be better at. Fiji, releasing in June, is supposed to have 4096 sahder cores, or 1280 more than the 290X or exactly twice the flagship card before that, the 7970 (rebranded as the 280X).</p>
<p>Anyway. Yeah. CPUs are really bad at floating-point. If you get an eight-socket Haswell-EX system with flagship CPUs, you have 8x18 cores capable of 32 single-precision FLOP per cycle (FLOPC?). At 2.5GHz, the system nets 11.5TFLOPS, or two Titan Xes. It costs close to $50 000 however, while a Titan X can be slapped into a sub-$100 embedded system (Intel Atom or AMD AM1) for much less. Maybe you spring for a larger motherboard that can accomodate two Titan Xes. Cool. That's a whopping &lt;$2500 for the whole thing.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Dirty_Socks" target="_blank">Dirty_Socks</a>
			<div class="markdown"><p>FYI, litecoin (along with dogecoin and other scryptcoins) use an algorithm that is specifically memory intensive, rather than processor intensive. Which means that a graphics card is not actually hugely great at mining it, because it can't be parallelized very well. This was done to help the problem that Bitcoin now faces, of only being able to make money mining with the absolute top of the line equipment, and even then for only a month or two. </p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/gseyffert" target="_blank">gseyffert</a>
			<div class="markdown"><p>Oh no absolutely, AMD is generally always a better compute choice since they don't intentionally cripple double precision float performance. That's why the were popular with crypto miners.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/CupricWolf" target="_blank">CupricWolf</a>
			<div class="markdown"><p>FLOPS aren't the only measure of performance. It seems to me that it isn't even the most relevant either because I'd think the enigma machine isn't creation floats.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/gseyffert" target="_blank">gseyffert</a>
			<div class="markdown"><p>Yeah I mean there'd be a lot more to it than simple FLOP performance, but I was more piggybacking on the example already given to show give an idea of how much (potentially) faster a GPU could be. In reality it probably wouldn't be anywhere near a 66x speedup, likely due to memory bottlenecks, but who knows. It's a purely hypothetical situation, and there are way too many factors to consider that are beyond the scope of my knowledge, especially when it comes to the intricacies of the Enigma algorithm.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/heap42" target="_blank">heap42</a>
			<div class="markdown"><p>the thing is, its just a constant, i dont know which coment you are refering to but it could be that he was talking about complexity and Bit O notation wich neglects all constants </p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Ori_553" target="_blank">Ori_553</a>
			<div class="markdown"><blockquote>
<p>ut using graphics cards to run parallel processing? I messed around using my old 8800gfx video card cracking wifi wpa codes using rainbow tables and was shocked at how many combinations per second I could g</p>
</blockquote>
<p>Using parallelism with a CUDA-enabled GPU would make it definitely faster, considering brute-force can be easily serialized </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Crazyachmed" target="_blank">Crazyachmed</a>
			<div class="markdown"><p>Uhm, serialized? ;)</p></div>		</li>
					</ul>
		</ul>
	