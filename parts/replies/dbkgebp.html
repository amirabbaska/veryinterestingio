	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Panda_Muffins" target="_blank">Panda_Muffins</a>
			<div class="markdown"><p>Great comment. In these cases, you really want to know the effect size as well. I just wrote up a manuscript which consisted of two data sets, each with hundreds of thousands of data points. After running the stats, I showed that one set scored better than the other on a specific test (p value super super small). But in reality, the difference is completely negligible and you'd never be able to perceive it in real life (effect size super super small too). Always a good thing to keep in mind. </p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/moxius" target="_blank">moxius</a>
			<div class="markdown"><p>I don't understand why this makes the p value misleading? It's not supposed to be an indicator of effect size, but an indicator of how likely it is that the detected effect was non-random.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/lambertb" target="_blank">lambertb</a>
			<div class="markdown"><p>I suppose if you understand precisely what the P value means, it's not misleading. But I would argue that many if not most people do incorrectly assume that the P value is a measure of effect size. In fact, I would argue most people do not know what effect size is.</p></div>		</li>
					</ul>
		</ul>
	