	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/FrontColonelShirt" target="_blank">FrontColonelShirt</a>
			<div class="markdown"><p>What about electromigration? Is that less of a factor for GPUs than CPUs because their functional units are so much simpler?</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/uberbob102000" target="_blank">uberbob102000</a>
			<div class="markdown"><p>This isn't a massive deal in CPUs either, at least during the life they'd be useful in. Even CPUs that run hot/higher voltage will last well past their useful life. </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/FrontColonelShirt" target="_blank">FrontColonelShirt</a>
			<div class="markdown"><p>Interesting. So I currently have a Pentium II 300MHz machine that I built in 1996 or 1997 acting as a router for a part of my LAN. If instead of giving it a normal workload and instead had run it at 100% for these past 20 years doing e.g. distributed computing or (ha) mining bitcoins, it'd most likely still be going strong?</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/III-V" target="_blank">III-V</a>
			<div class="markdown"><p>No, complexity in and of itself doesn't have anything to do with it. It's pretty much entirely dictated by the manufacturing process. You may see a difference between products fabricated with Samsung and TSMC, for example, or between a 14nm node and a 10nm node.</p>
<p>You really don't see much data or studies being done on longevity however, so I can't really paint a good picture for you.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/III-V" target="_blank">III-V</a>
			<div class="markdown"><blockquote>
<p>GPUs are built to run at the temperatures they do.</p>
</blockquote>
<p>No they aren't. Some of the components on the circuit board are selected for higher temperature operation (e.g. MOSFETs), but the GPU itself (the silicon) is just going to use whatever the process design rules are -- and operating temperature is pretty low down on the list of priorities. Density, current drive are a couple of the big ones.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/pepe_le_shoe" target="_blank">pepe_le_shoe</a>
			<div class="markdown"><p>Perhaps a better way of wording it is that they'll shut themselves off long before they hit temperatures that they can't sustainably operate at. Where sustainably means for the entirety of the warranty period for some desired percentage of units as decided by the manufacturer.</p>
<p>Modern gpus will also down clock themselves as they get hotter in an attempt to prevent any more increase in temperature.</p></div>		</li>
					</ul>
		</ul>
	