	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/ObserverPro" target="_blank">ObserverPro</a>
			<div class="markdown"><p>It is extremely trippy. It's the closest thing I've ever seen that looks like an actual acid trip. Thank you for the explanation. I couldn't help but to laugh thinking about it looking for dogs in photos with no dogs. I do the same thing, because I am an actual 5 year old.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/CydeWeys" target="_blank">CydeWeys</a>
			<div class="markdown"><p>Some minor corrections:</p>
<blockquote>
<p>the image recognition software has thousands of reference images of known things, which it compares to an image it is trying to recognise.</p>
</blockquote>
<p>It doesn't work like that.  There are thousands of reference images that are used to <em>train</em> the model, but once you're actually running the model itself, it's not using reference images (and indeed doesn't store or have access to any).  A similar analogy is if I ask you, a person, to determine if an audio file that I'm playing is a song.  You have a mental model of what features make something song-like, e.g. if it has rhythmically repeating beats, and that's how you make the determination.  You <em>aren't</em> singing thousands of songs that you know to yourself in your head and comparing them against the audio that I'm playing.  Neural networks don't do this either.</p>
<blockquote>
<p>So if you provide it with the image of a dog and tell it to recognize the image, it will compare the image to it's references, find out that there are similarities in the image to images of dogs, and it will tell you &quot;there's a dog in that image!&quot;</p>
</blockquote>
<p>Again, it's not comparing it to references, it's running its model that it's built up from being trained on references.  The model itself may well be completely nonsensical to us, in the same way that we don't have an in-depth understanding of how a human brain identifies animal features either.  All we know is there's this complicated network of neurons that feed back into each other and respond in specific ways when given certain types of features as input.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Kman1898" target="_blank">Kman1898</a>
			<div class="markdown"><p>Listen to the radio clip in the link below. Jayatri Das will use audio to simulate exactly what you're talking about relative to the way we process information </p>
<blockquote>
<p>She starts with a clip that's been digitally altered to sound like jibberish. On first listen, to my ears, it was entirely meaningless. Next, Das plays the original, unaltered clip: a woman's voice saying, &quot;The Constitution Center is at the next stop.&quot; Then we hear the jibberish clip again, and woven inside what had sounded like nonsense, we hear &quot;The Constitution Center is at the next stop.&quot; </p>
<p>The point is: When our brains know what to expect to hear, they do, even if, in reality, it is impossible. Not one person could decipher that clip without knowing what they were hearing, but with the prompt, it's impossible not to hear the message in the jibberish. </p>
<p>This is a wonderful audio illusion. </p>
</blockquote>
<p><a href="http://www.theatlantic.com/technology/archive/2014/06/sounds-you-cant-unhear/373036/" target="_blank">http://www.theatlantic.com/technology/archive/2014/06/sounds-you-cant-unhear/373036/</a></p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/DemetriMartin" target="_blank">DemetriMartin</a>
			<div class="markdown"><p>What's weirder is I knew what the words were going to be based on your comment and it helped me decipher a few syllables, but I still couldn't hear the whole sentence. Once the regular voice was played everything clicked and I couldn't unhear it.</p>
<p>Cool stuff.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/pumper6000" target="_blank">pumper6000</a>
			<div class="markdown"><p>Hello. I have another real life example for this phenomenon.</p>
<p>English is not my native language, but i like to watch english movies, hence subtitles.
But i try my best to not to look at them, because i don't want to end up 'reading' the movie.</p>
<p>A lot of times, the character's talking speed exceeds my brain's capacity, and as a result i cannot understand that sentence. </p>
<p>So, when i read the subtiltes, the dialogue is fed to my brain in a clearer way. </p>
<p>Next time i watch the same scene again, i completely understand the dialogue.</p>
<p>Our brain runs on 'watch and learn' principle, hence this. </p>
<p>once you know you that red light is for 'caution', your brain will become more cautious when it sees the light again.
it's all linked.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/CredibilityProblem" target="_blank">CredibilityProblem</a>
			<div class="markdown"><p>You kind of ruined that by including the excerpt that tells you what you're supposed to hear.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/superkamiokande" target="_blank">superkamiokande</a>
			<div class="markdown"><blockquote>
<p>You have a mental model of what features make something song-like, e.g. if it has rhythmically repeating beats, and that's how you make the determination. You aren't singing thousands of songs that you know to yourself in your head and comparing them against the audio that I'm playing.</p>
</blockquote>
<p>This is actually something of an open question in cognitive science. <a href="https://en.wikipedia.org/wiki/Exemplar_theory" target="_blank">Exemplar Theory</a> actually maintains that you are actively comparing against an actual stored member that best typifies the category. So in the music example, you would have some memory of a song that serves as an exemplar, and comparing what you're hearing to that actual stored memory helps you decide if what you're hearing is a song or not.</p>
<p>This theory is not uncommon in linguistics, where it is one possible model to account for knowledge of speech sounds.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/rychan" target="_blank">rychan</a>
			<div class="markdown"><p>Yes, that's an open question about how our brains work, but to be clear it's not an open question about how deep convolutional networks work. They don't directly remember the training images.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Lost4468" target="_blank">Lost4468</a>
			<div class="markdown"><p>What about classifying something into a genre of music?</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Khaim" target="_blank">Khaim</a>
			<div class="markdown"><blockquote>
<p>Exemplar Theory actually maintains that you are actively comparing against an actual stored member that best typifies the category.</p>
</blockquote>
<p>In some sense that is exactly how neural networks operate. The top-level neuron encodes one particular instance of the category, which is basically what the AI thinks is the ideal member. (Or something like that; I'm simplifying a little.)</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/DaRiskyFloid" target="_blank">DaRiskyFloid</a>
			<div class="markdown"><p>Im glad you corrected this. It is important to understand that this form of AI does not just use a simple cookbook like algorithm to achieve its result. Neuronal networks are a simulation of a simplified brain. It is trained very much like a real brain would be trained. There is a psychological theory (I dont remember the name), that states, that dreaming is a random signal, generated by the brain in order to keep the signal processing system busy when there is no real information to process. This is important because the system needs to stay active for some reason.
If you consider this, feeding noise into a neuronal network is indeed quite similar to dreaming.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Hazzman" target="_blank">Hazzman</a>
			<div class="markdown"><p>Yeah as impressive and fun as this image recog stuff is I feel like the name is confusing people and a bit of a misnomer. </p>
<p>Googles AI is not dreaming/ inventing new things/ or doing anything particularly sentient. </p>
<p>Its like taking a picture of a house and saying &quot;Find the face&quot; so it finds the face by highlighting areas that look like the face. Then you take that image and ask it again, to &quot;Find the face&quot; and it recognizes the face even easier and manipulates the image in the same way, again, making it even more face like. Do that a few hundred times and you start to see recognizable faces all over the now completely skewed image. </p>
<p>This is absolutely not to say this isn't fun and impressive - image/pattern recognition has classically been a challenge for AI so seeing the advances they've made is really cool, but it is pretty annoying when news outlets present it as some sort of sentient machine dreaming about shit and producing images - this is absolutely not the case.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Lost4468" target="_blank">Lost4468</a>
			<div class="markdown"><blockquote>
<p>Googles AI is not dreaming/ inventing new things/ or doing anything particularly sentient. </p>
</blockquote>
<p>I disagree that it's not inventing new things, it's creating pictures from random noise and is capable of creating new objects that aren't in the images it learned, it's essentially creating new objects with the properties of 1 or more other objects it has learned about. This is basically the same way humans tend to create things.</p>
<blockquote>
<p>Its like taking a picture of a house and saying &quot;Find the face&quot; so it finds the face by highlighting areas that look like the face. Then you take that image and ask it again, to &quot;Find the face&quot; and it recognizes the face even easier and manipulates the image in the same way, again, making it even more face like. Do that a few hundred times and you start to see recognizable faces all over the now completely skewed image. </p>
</blockquote>
<p>This is what humans do as well, look at something and try to find faces in it, then just keep looking and you'll start seeing faces where there are none.</p>
<blockquote>
<p>some sort of sentient machine dreaming about shit and producing images - this is absolutely not the case.</p>
</blockquote>
<p>It's not sentient but it absolutely is hallucinating and producing images out of past experiences.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/wbsgrepit" target="_blank">wbsgrepit</a>
			<div class="markdown"><p>It is even more amazing when you realize that the shapes and images that we recognize are not actually referenced.   The DNN has been trained on reference images,  but these images and the shapes generated are happening from the outcome of this -- the DNN has conceptualized &quot;rules&quot; for these types of images and is producing the new images/shapes from these rules/learning. </p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/null_work" target="_blank">null_work</a>
			<div class="markdown"><blockquote>
<p>Googles AI is not dreaming/ inventing new things/ or doing anything particularly sentient.</p>
</blockquote>
<p>Though we run into the possiblity that dreaming/inventing new things/doing things particularly sentient is really just an accident of how our brains process things. Which is to say, we can't actually say we do anything more meaningfully different than what these programs are doing.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/somewhat_oxygenated" target="_blank">somewhat_oxygenated</a>
			<div class="markdown"><p>This whole discussion makes me wonder what would happen if you did a Turing test with the images generated by the program and some paintings. Would a human be reliably able to pick the paintings made by humans?</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/RagingOrangutan" target="_blank">RagingOrangutan</a>
			<div class="markdown"><p>No? I thought dreaming in humans was caused because of random electrical firings in the brain. The brain then tries to interpret this random information however it can.</p>
<p>Isn't that sorta what's happening here? The images are getting matched to stuff that the neural network already knows about.</p>
<p>In a sense, in both cases pattern matching is being applied to noise, and crazy stuff results.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/TomHardyAsBronson" target="_blank">TomHardyAsBronson</a>
			<div class="markdown"><blockquote>
<p>Googles AI is not dreaming/ inventing new things/ or doing anything particularly sentient. </p>
</blockquote>
<p>I would be interested to see if and to what extent the program's distorted &quot;dreamed&quot; images statistically  match it's reference photos. I'm sure it has millions of reference photos so it's going to be statistically similar to at least one of them, but that could be an interesting way to see how much &quot;creation&quot; is going into the image. </p></div>		</li>
					</ul>
		</ul>
	