	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/beavertime" target="_blank">beavertime</a>
			<div class="markdown"><p>Your post has some good information, but it doesn't actually answer the OP which long predates Apple using in house designs for their CPU. The answer to that is in various other posts, having to do with garbage collection versus reference counting.</p>
<p>However you do have some revisionist history in your post. Firstly, PowerPC was <em>partly owned by Apple</em> (along with Motorola and IBM. As a ironic aside, ARM was cofounded by Apple, originally to design the processor for the much maligned Newton). Apple wasn't dependent on anyone, but the simple reality is that Intel left PowerPC behind -- Intel was being financed by the vast majority of the market, with a massive R&amp;D budget, while PowerPC was scraping by with a tiny market. It couldn't compete. Apple brought processor design in house for the iOS devices when they had so many billions of profits they could eat all the R&amp;D necessary (and no longer had to pool, effectively, with Samsung). They have done remarkable work, and have fantastic single core efficiency, but it's disingenuous to say it's two generations ahead when so many other chips (e.g. Exynos) are edging it. As mobile exploded, and money started pouring into mobile designs, those long derelict ARM cores got dramatically more competitive. </p>
<p>Intel, it is notable, is in a tough situation where their biggest worry is competing with themselves -- their cash cow of x86 is in the office and data center, so they've always crippled their mobile offerings: The former makes them hundreds of dollars per chip, while the latter is dollars per chip at best. This is the same reason why nvidia pulled back on some <em>fantastic</em> mobile chips -- market leading chips -- because they make a shitload more money selling the X1 to Nintendo than to a mobile maker.</p>
<p>Secondly, Apple most certainly wasn't remotely first with Big.little, and it's even iffy to say that they were first with 64 bits. ARMv8 was introduced as a reference design two years early (ergo, any ARM maker could start fabbing out chips if there was a market). Android as a project wasn't prioritizing 64-bit, so makers simply didn't move to hardware that the OS couldn't support.</p>
<p>Your ending bit on Intel versus ARM is just ridiculous, and reads like an article from the 1980s. It is wrong on every level in a modern context. The labels CISC and RISC don't even make any sense any more.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/dont_forget_canada" target="_blank">dont_forget_canada</a>
			<div class="markdown"><blockquote>
<p>PowerPC was partly owned by Apple</p>
</blockquote>
<p>Yes but IBM was the one actively developing the architecture at the time and were going too slow for apple's tastes. The promised 3Ghz G5's never happened and IBM couldn't get the POWER series running cool enough to even consider continuing in the powerbook. This was a big deal at the time and IBM certainly <strong>did</strong> screw up Apple's timeline.</p>
<blockquote>
<p>while PowerPC was scraping by with a tiny market. It couldn't compete. </p>
</blockquote>
<p>This last part simply isn't true. The PA6T was incredibly promising and was even developed outside of AIM.</p>
<blockquote>
<p>Apple brought processor design in house for the iOS devices when they had so many billions of profits they could eat all the R&amp;D necessary.</p>
</blockquote>
<p>Apple was talking to P.A. Semi several years before buying them and the consensus was that Apple would ditch AIM and stay with PPC going with the PWRficient series. This <strong>would have</strong> supported multiple cores which arguably ran cooler than competitive intel chips. Instead, Apple realized early on that their future was in the iPhone and not the Mac. They bought the company, axed R&amp;D into PWRficient and moved it to ARM.</p>
<blockquote>
<p>Android as a project wasn't prioritizing 64-bit, so many makers simply didn't move to hardware that the OS couldn't support.</p>
</blockquote>
<p>doesn't that further show how, when you control all the modules encompassing a product, you can coordinate the sw and hw together and make a big transition like from 32bit -&gt; 64bit easier and faster than your competition?</p>
<blockquote>
<p>Your ending bit on Intel versus ARM is just ridiculous, and reads like an article from the 1980s. It is wrong on every level.</p>
</blockquote>
<p>You really don't think Intel is worried at all that Microsoft has Windows 10 on ARM <strong>in addition to a transparent rosetta like runtime transpiler</strong>? We're not talking about the NT kernel simply having support for ARM, this clearly goes far beyond that. You don't think they're worried that Apple is about to cancel all future contracts with Intel for the Mac altogether? Intel has enough trouble keeping the thermals in their desktop class chips in line (go look at the thermal spikes people report with the 7700k for example). You really think in the portable direction Apple (and the industry) is headed that Intel has a future without another massive re-design?</p>
<blockquote>
<p>The labels CISC and RISC don't even make any sense any more.</p>
</blockquote>
<p>How can you say that? Your Intel CPU is running a RISC like core and translating x86 instructions own to uops that execute on that core. Those x86 instructions were created pre-P6 microarchitecture for true CISC chips. The legacy x86 instruction set intended for true CISC chips was kept for compatibility even after all these years, but now software has caught up and Intel is left holding the bag for something nobody wants anymore.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/beavertime" target="_blank">beavertime</a>
			<div class="markdown"><blockquote>
<p>This last part simply isn't true.</p>
</blockquote>
<p>It's completely true. PowerPC was totally eclipsed by Intel. PWRficient  was singularly targeted at power efficiency, and had little market because that just wasn't enough of a draw.</p>
<blockquote>
<p>doesn't that further show how</p>
</blockquote>
<p>Yes, it absolutely does. I don't disagree with that at all. Apple has remarkable control, and as a developer who has targeted both Android and Apple, I <em>vastly</em> prefer Apple devices. I'm glad there's competition though.</p>
<blockquote>
<p>You really don't think Intel is worried at all that Microsoft has Windows 10 on ARM in addition to a transparent rosetta like runtime transpiler?</p>
</blockquote>
<p>Transcoding will always be somewhat second tier, and I doubt Intel is all that worried. Intel is likely worried about Apple, but compared to their data center cash cow Apple is small, small, tiny potatoes. Again, everything Intel does has to be balanced against competing with themselves.</p>
<p>ARM has been purported to be ready to take over the data center for decades now. When it actually gets to data center scale, though, the benefits and efficiency just dissolve.</p>
<blockquote>
<p>Your Intel CPU is running a RISC like core and translating x86 instructions own to uops that execute on that core.</p>
</blockquote>
<p>Instruction sets are largely interchangeable. On both ARM and Intel chips that instruction set is converted to microcode that can contain multiple microoperations, perfectly optimized for the processor. That whole debate hasn't been relevant for years. </p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/K3wp" target="_blank">K3wp</a>
			<div class="markdown"><blockquote>
<p>You really don't think Intel is worried at all that Microsoft has Windows 10 on ARM in addition to a transparent rosetta like runtime transpiler?</p>
</blockquote>
<p>Absolutely not, because the ARM architecture is a teeny, tiny little toy for babies compared to a modern i7.  Consider my first Core i7 920 to the Snapdragon in a modern Android phone:</p>
<p><a href="http://cpuboss.com/cpus/Qualcomm-Snapdragon-800-vs-Intel-Core-i7-920" target="_blank">http://cpuboss.com/cpus/Qualcomm-Snapdragon-800-vs-Intel-Core-i7-920</a></p>
<p>Now look at the GeekBench scores.  The 10 year old Intel design is still 3X+ faster than the ARM design.</p>
<p>Now compare that to modern i7:</p>
<p><a href="http://cpuboss.com/cpus/Qualcomm-Snapdragon-800-vs-Intel-Core-i7-6700K" target="_blank">http://cpuboss.com/cpus/Qualcomm-Snapdragon-800-vs-Intel-Core-i7-6700K</a></p>
<p>It <em>destroys</em> it.  It's 6x+ times faster than the ARM design.  ARM has far fewer execution units, so it simply can't compete.  And never will, without a complete redesign that would kill it as a mobile processor.  See, that's what you are missing.  It's only successful in the mobile space because it uses so little power.  And it uses little power because it has little execution pipelines.  RISC/CISC has nothing to do with it.</p>
<blockquote>
<p>Those x86 instructions were created pre-P6 microarchitecture for true CISC chips.</p>
</blockquote>
<p>A.  RISC instructions are a subset of CISC instructions.  Hence the whole &quot;reduced&quot; thing.</p>
<p>B.  All modern AMD/Intel parts are x86-64 designs, which is an effectively modern hybrid architecture that blends the best (and worst!) of both CISC and RISC architectures.</p>
<p>C.  The internal of the i7 is a RISC core with a <em>transparent rosetta like runtime transpiler</em> that breaks down CISC instructions into RISC-like micro-ops.</p>
<p>So, basically, Intel already built a better RISC core than ARM did.  And then built a hardware transpiler on top of it to allow it to run legacy code with no performance penalty!  </p>
<p>It gets worse for Intel's competitors when you realize they can build an i7 for the mobile computing market and can effectively emulate a low-power competitor simply by clocking down and disabling features.  And then you can plug it in at your buddies place and game with him!</p>
<p>Indeed, Intel has given up on the smartphone market.  Because of low margins.  They will continue to build PC parts forever.</p>
<p>Anyways, I work at a STEM Uni.  The kids show up these days with PCs, smartphones, consoles, tablets, etc.  They didn't replace one with the other.  </p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/mostlikelynotarobot" target="_blank">mostlikelynotarobot</a>
			<div class="markdown"><p>Nvidia chips were only good at what Nvidia had always been good at: graphics. Otherwise, they were power hungry, and flawed. The big.LITTLE doesn't even work in the X1.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/flaizeur" target="_blank">flaizeur</a>
			<div class="markdown"><blockquote>
<p>having to do with garbage collection versus reference counting.</p>
</blockquote>
<p><strong>Moon instructs a student</strong></p>
<p>One day a student came to Moon and said: “I understand how to make a better garbage collector. We must keep a reference count of the pointers to each cons.”</p>
<p>Moon patiently told the student the following story:</p>
<p>“One day a student came to Moon and said: ‘I understand how to make a better garbage collector...</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/symmetry81" target="_blank">symmetry81</a>
			<div class="markdown"><p>A small correction, Android actually had simultaneous use of big and little cores first with the Exynos 5 Octa back in 2013 and global task scheduling has been standard since about 2014.  Whereas Apple's first globally scheduled bit.LITTLE SOC was the A11 released in 2017.  Otherwise a very interesting post!</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/floatboth" target="_blank">floatboth</a>
			<div class="markdown"><blockquote>
<p>that team from P.A. Semi has designed Apples ARM CPUs for the iPhone ever since</p>
</blockquote>
<p>It took them until 2012 to ship an actual custom CPU though, <a href="https://www.anandtech.com/show/6292/iphone-5-a6-not-a15-custom-core" target="_blank">with the A6</a>. They've been using ARM Cortex cores before.</p>
<blockquote>
<p>first to allow the use of &quot;big&quot; and &quot;little&quot; cores simultaneously</p>
</blockquote>
<p>naaaah. Samsung shipped a big.LITTLE Exynos in like 2013.</p>
<blockquote>
<p>In five years from now we likely wont even recognize what MacOS and Windows are anymore</p>
</blockquote>
<p>Software is extremely hard to kill once it gets even slightly popular. There are still mainframes running COBOL programs out there in the world, mostly in airports and old banks and such.</p>
<blockquote>
<p>they physically can't compete speed/heat/size with ARM now</p>
</blockquote>
<p>ARM is ahead on size, but really behind on speed. Where are the ARM chips with workstation-grade performance? Cavium makes 48 core ThunderX's but their single core performance is significantly behind x86. Apple indeed has better single core performance than most other ARM CPUs but it's still not close to desktops.</p>
<p>Sure mobile devices are getting more popular for web browsing, but the high performance market will NOT go away.</p>
<p>Side note, Intel is indeed starting to lose. To good old AMD, that is. Zen is an incredible success story already. Imagine what it will be when they get to the 7nm process! Intel is still struggling to get reasonable yields on their 10nm. AMD / Global Foundries will kick their ass <em>hard</em>.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/CreideikiVAX" target="_blank">CreideikiVAX</a>
			<div class="markdown"><blockquote>
<p>There are still mainframes running COBOL programs out there in the world, mostly in airports and old banks and such.</p>
</blockquote>
<p>Modern z/Architecture mainframes are pretty nice, and there is, of course, modern software being developed on it.</p>
<p>It also just so happens that IBM are the fucking undisputed Kings of backwards compatibility. Because the COBOL program written back in 1964 on the then so-brand-new-the-serial-number-is-in-the-single-digits System/360 Model 40 can still run, unmodified, on z/OS today.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/ALWAYS_YELLEN" target="_blank">ALWAYS_YELLEN</a>
			<div class="markdown"><p>I really need to get my hands on a mainframe</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/gimpwiz" target="_blank">gimpwiz</a>
			<div class="markdown"><blockquote>
<p>naaaah. Samsung shipped a big.LITTLE Exynos in like 2013.</p>
</blockquote>
<p>Did it allow simultaneous use of both sets of cores, as the other person emphasized? I can't remember.</p>
<blockquote>
<p>Where are the ARM chips with workstation-grade performance? </p>
</blockquote>
<p>Yeah, that's the big question when these conversations go towards arch switches. It makes little sense to switch only part of the intel lineup; so how do they switch the big stuff?</p>
<p>Truth is that intel failed in the mobile space, but they jealously defend the workstation-and-up space, where absolute power levels are also far less of a concern. There's TDP (or &quot;SDP&quot;) for total power, performance/watt, and total performance levels, and workstations care much more about #3 and #2 than #1; as long as it fits inside a healthy envelope, it's okay.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/dont_forget_canada" target="_blank">dont_forget_canada</a>
			<div class="markdown"><p>As far as consumers go, very few are interested in high end workstations. You or I might be the exception, but the majority of people probably already own and use machines with processors less powerful than the A10X.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/AceJohnny" target="_blank">AceJohnny</a>
			<div class="markdown"><blockquote>
<p>Imagine what it will be when they get to the 7nm process! Intel is still struggling to get reasonable yields on their 10nm. AMD / Global Foundries will kick their ass hard.</p>
</blockquote>
<p>Source on that? I admit I haven't been following the field, but my understanding is that Intel has been pretty good at maintaining their tech lead at the fab.</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/roselan" target="_blank">roselan</a>
			<div class="markdown"><p>Their clock is broken. The next process technology was due 2 years ago, and we will be lucky to see it this year. They literally hit a wall with euv and 10nm.</p></div>		</li>
					</ul>
		</ul>
		</ul>
	