	<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/Kampfschnitzel0" target="_blank">Kampfschnitzel0</a>
			<div class="markdown"><p>So going back to the car analogy, the more uncertain variables i have the higher the entropy is? As in  more unpredictable values (e.g. miles per tire) = higher entropy</p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/NeuroPsychotic" target="_blank">NeuroPsychotic</a>
			<div class="markdown"><p>The first thing you should consider is not just the number of random variables, but whether you have a high or low informative random variable. The former can take on a particular value with a probability much higher than all the others; the latter has similar probability for all the values it can take. This stands at the core of unpredictability calculation in information theory. Unfortunately our car example shows its inadequateness here: simplifying, you don't get a new value of miles per tire every time you switch your car on. </p></div>		</li>
					<ul class="replies">
		<li class="reply">
			<a class="author" href="https://www.reddit.com/user/CourageousCabbage" target="_blank">CourageousCabbage</a>
			<div class="markdown"><p>If you compare to a fuel pump occasionally pumping diesel inexplicably instead of fuel it makes sense.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/CourageousCabbage" target="_blank">CourageousCabbage</a>
			<div class="markdown"><p>More like whether your fuel pump is actually going to pump fuel or whether your spark plugs will actually spark (when the gap is correct). In a low entropy system we know the fuel pump will pump fuel and not soda, and we know the spark plugs will spark when set at a certain gap.</p></div>		</li>
					</ul>
			<li class="reply">
			<a class="author" href="https://www.reddit.com/user/iugameprof" target="_blank">iugameprof</a>
			<div class="markdown"><p>This concept of &quot;just enough&quot; entropy seems to correspond to the creation of Class IV cellular automata (those that create flexible meta-stable structures with neither too much periodicity or chaos). Fascinating, if not unexpected, to see that show up in something as complex as the brain.</p></div>		</li>
						<li class="reply">
			<a class="author" href="https://www.reddit.com/user/todzilla2012" target="_blank">todzilla2012</a>
			<div class="markdown"><p>Helped me, thanks</p></div>		</li>
					</ul>
	